{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/aoyuli/.cache/huggingface/datasets/json/default-90eae69d8c584b52/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a905ed5301354df694ff8b784f217806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 用你的本地 SQuAD 数据集文件路径替换这里的路径\n",
    "local_bioasq_train_file = 'data/BioASQ/train/BioASQ-train-list.json'\n",
    "local_bioasq_val_file = 'data/BioASQ/val/BioASQ-val-list.json'\n",
    "local_bioasq_test_file = 'data/BioASQ/test/BioASQ-test-list.json'\n",
    "\n",
    "# 加载本地 SQuAD 数据集\n",
    "bioasq_dataset = load_dataset('json', data_files={'train': local_bioasq_train_file, 'val': local_bioasq_val_file, 'test': local_bioasq_test_file}, field='data')\n",
    "\n",
    "# 访问数据集的子集，例如 'train' 和 'validation'\n",
    "train_data = bioasq_dataset['train']\n",
    "val_data = bioasq_dataset['val']\n",
    "test_data = bioasq_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def flatten_dataset(dataset):\n",
    "    flattened_examples = []\n",
    "\n",
    "    for example in dataset:\n",
    "        for paragraph in example['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                flattened_example = {\n",
    "                    'question': qa['question'],\n",
    "                    'context': context,\n",
    "                    'answers': {\n",
    "                        'text': [qa['answers'][0]['text']],\n",
    "                        'answer_start': [qa['answers'][0]['answer_start']],\n",
    "                    },\n",
    "                    'title': 'BioASQ',\n",
    "                    'id': qa['id']\n",
    "                }\n",
    "                flattened_examples.append(flattened_example)\n",
    "\n",
    "    return flattened_examples\n",
    "\n",
    "# train_examples = flatten_dataset(train_data)[:320]\n",
    "# val_examples = flatten_dataset(val_data)[:40]\n",
    "# test_examples = flatten_dataset(test_data)[:40]\n",
    "\n",
    "train_examples = flatten_dataset(train_data)\n",
    "val_examples = flatten_dataset(val_data)\n",
    "test_examples = flatten_dataset(test_data)\n",
    "\n",
    "train_data = Dataset.from_dict({key: [example[key] for example in train_examples] for key in train_examples[0].keys()})\n",
    "val_data = Dataset.from_dict({key: [example[key] for example in val_examples] for key in val_examples[0].keys()})\n",
    "test_data = Dataset.from_dict({key: [example[key] for example in test_examples] for key in val_examples[0].keys()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets = train_data.train_test_split(test_size=0.2)\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'validation': val_data,\n",
    "    'test': test_data\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'context', 'answers', 'title', 'id'],\n",
       "        num_rows: 6878\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'context', 'answers', 'title', 'id'],\n",
       "        num_rows: 859\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'context', 'answers', 'title', 'id'],\n",
       "        num_rows: 861\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Rivaroxaban versus warfarin in Japanese patients with nonvalvular atrial fibrillation for the secondary prevention of stroke: a subgroup analysis of J-ROCKET AF. BACKGROUND: The overall analysis of the rivaroxaban versus warfarin in Japanese patients with atrial fibrillation (J-ROCKET AF) trial revealed that rivaroxaban was not inferior to warfarin with respect to the primary safety outcome. In addition, there was a strong trend for a reduction in the rate of stroke/systemic embolism with rivaroxaban compared with warfarin. METHODS: In this subanalysis of the J-ROCKET AF trial, we investigated the consistency of safety and efficacy profile of rivaroxaban versus warfarin among the subgroups of patients with previous stroke, transient ischemic attack, or non-central nervous system systemic embolism (secondary prevention group) and those without (primary prevention group). RESULTS: Patients in the secondary prevention group were 63.6% of the overall population of J-ROCKET AF. In the secondary prevention group, the rate of the principal safety outcome (% per year) was 17.02 in rivaroxaban-treated patients and 18.26 in warfarin-treated patients (hazard ratio [HR] 0.95; 95% confidence interval [CI] 0.70-1.29), while the rate of the primary efficacy endpoint was 1.66 in rivaroxaban-treated patients and 3.25 in warfarin-treated patients (HR 0.51; 95% CI 0.23-1.14). There were no significant interactions in the principal safety and the primary efficacy endpoints of rivaroxaban compared to warfarin between the primary and secondary prevention groups (P=.090 and .776 for both interactions, respectively). CONCLUSIONS: The safety and efficacy profile of rivaroxaban compared with warfarin was consistent among patients in the primary prevention group and those in the secondary prevention group.\n",
      "Question:  What medication were compared in the ROCKET AF Trial?\n",
      "Answer:  {'answer_start': [310], 'text': ['rivaroxaban']}\n"
     ]
    }
   ],
   "source": [
    "print(\"Context: \", raw_datasets[\"train\"][0][\"context\"])\n",
    "print(\"Question: \", raw_datasets[\"train\"][0][\"question\"])\n",
    "print(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a0d3ba31c14f7c9ef4498975a50c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'answers', 'title', 'id'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_start': [931], 'text': ['met']}\n",
      "{'answer_start': [1323], 'text': ['COL5A2']}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"test\"][0][\"answers\"])\n",
    "print(raw_datasets[\"test\"][2][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical and genetic aspects of Ehlers-Danlos syndrome, classic type. Classic Ehlers-Danlos syndrome is a heritable connective tissue disorder characterized by skin hyperextensibility, fragile and soft skin, delayed wound healing with formation of atrophic scars, easy bruising, and generalized joint hypermobility. It comprises Ehlers-Danlos syndrome type I and Ehlers-Danlos syndrome type II, but it is now apparent that these form a continuum of clinical findings and differ only in phenotypic severity. It is currently estimated that approximately 50% of patients with a clinical diagnosis of classic Ehlers-Danlos syndrome harbor mutations in the COL5A1 and the COL5A2 gene, encoding the α1 and the α2-chain of type V collagen, respectively. However, because no prospective molecular studies of COL5A1 and COL5A2 have been performed in a clinically well-defined patient group, this number may underestimate the real proportion of patients with classic Ehlers-Danlos syndrome harboring a mutation in one of these genes. In the majority of patients with molecularly characterized classic Ehlers-Danlos syndrome, the disease is caused by a mutation leading to a nonfunctional COL5A1 allele and resulting in haploinsufficiency of type V collagen. A smaller proportion of patients harbor a structural mutation in COL5A1 or COL5A2, causing the production of a functionally defective type V collagen protein. Most mutations identified so far result in a reduced amount of type V collagen in the connective tissues available for collagen fibrillogenesis. Inter- and intrafamilial phenotypic variability is observed, but no genotype-phenotype correlations have been observed. No treatment for the underlying defect is presently available for Ehlers-Danlos syndrome. However, a series of preventive guidelines are applicable.\n",
      "Which genes are associated with Ehlers-Danlos syndrome type I/II?\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"test\"][2][\"context\"])\n",
    "print(raw_datasets[\"test\"][2][\"question\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] what medication were compared in the rocket af trial? [SEP] rivaroxaban versus warfarin in japanese patients with nonvalvular atrial fibrillation for the secondary prevention of stroke : a subgroup analysis of j - rocket af. background : the overall analysis of the rivaroxaban versus warfarin in japanese patients with atrial fibrillation ( j - rocket af ) trial revealed that rivaroxaban was not inferior to warfarin with respect to the primary safety outcome. in addition, there was a strong trend for a reduction in the rate of stroke / systemic embolism with rivaroxaban compared with warfarin. methods : in this subanalysis of the j - rocket af trial, we investigated the consistency of safety and efficacy profile of rivaroxaban versus warfarin among the subgroups of patients with previous stroke, transient ischemic attack, or non - central nervous system systemic embolism ( secondary prevention group ) and those without ( primary prevention group ). results : patients in the secondary prevention group were 63. 6 % of the overall population of j - rocket af. in the secondary prevention group, the rate of the principal safety outcome ( % per year ) was 17. 02 in rivaroxaban - treated patients and 18. 26 in warfarin - treated patients ( hazard ratio [ hr ] 0. 95 ; 95 % confidence interval [ ci ] 0. 70 - 1. 29 ), while the rate of the primary efficacy endpoint was 1. 66 in rivaroxaban - treated patients and 3. 25 in warfarin - treated patients ( hr 0. 51 ; 95 % ci 0. 23 - 1. 14 ). there were no significant interactions in the principal safety and the primary efficacy endpoints of rivaroxaban compared to warfarin between the primary and secondary prevention groups ( p =. 090 and. 776 for both interactions, respectively ). conclusions : the safety and efficacy profile of rivaroxaban compared with warfarin was consistent among patients in the primary prevention group and those in the secondary prevention group. [SEP]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = raw_datasets[\"train\"][0][\"context\"]\n",
    "question = raw_datasets[\"train\"][0][\"question\"]\n",
    "\n",
    "inputs = tokenizer(question, context)\n",
    "tokenizer.decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] what medication were compared in the rocket af trial? [SEP] rivaroxaban versus warfarin in japanese patients with nonvalvular atrial fibrillation for the secondary prevention of stroke : a subgroup analysis of j - rocket af. background : the overall analysis of the rivaroxaban versus warfarin in japanese patients with atrial fibrillation ( j - rocket af ) trial revealed that rivaroxaban was not inferior to warfarin with respect to [SEP]\n",
      "[CLS] what medication were compared in the rocket af trial? [SEP]. background : the overall analysis of the rivaroxaban versus warfarin in japanese patients with atrial fibrillation ( j - rocket af ) trial revealed that rivaroxaban was not inferior to warfarin with respect to the primary safety outcome. in addition, there was a strong trend for a reduction in the rate of stroke / systemic embolism with rivaroxaban compared with warfarin [SEP]\n",
      "[CLS] what medication were compared in the rocket af trial? [SEP]oxaban was not inferior to warfarin with respect to the primary safety outcome. in addition, there was a strong trend for a reduction in the rate of stroke / systemic embolism with rivaroxaban compared with warfarin. methods : in this subanalysis of the j - rocket af trial, we investigated the consistency of safety and efficacy profile of rivaroxaban versus warfarin among the subgroup [SEP]\n",
      "[CLS] what medication were compared in the rocket af trial? [SEP]bolism with rivaroxaban compared with warfarin. methods : in this subanalysis of the j - rocket af trial, we investigated the consistency of safety and efficacy profile of rivaroxaban versus warfarin among the subgroups of patients with previous stroke, transient ischemic attack, or non - central nervous system systemic embolism ( secondary prevention group ) and those without ( primary prevention group ). [SEP]\n",
      "[CLS] what medication were compared in the rocket af trial? [SEP] of rivaroxaban versus warfarin among the subgroups of patients with previous stroke, transient ischemic attack, or non - central nervous system systemic embolism ( secondary prevention group ) and those without ( primary prevention group ). results : patients in the secondary prevention group were 63. 6 % of the overall population of j - rocket af. in the secondary prevention group, the rate of the principal safety outcome ( [SEP]\n",
      "[CLS] what medication were compared in the rocket af trial? [SEP] secondary prevention group ) and those without ( primary prevention group ). results : patients in the secondary prevention group were 63. 6 % of the overall population of j - rocket af. in the secondary prevention group, the rate of the principal safety outcome ( % per year ) was 17. 02 in rivaroxaban - treated patients and 18. 26 in warfarin - treated patients ( hazard ratio [ hr ] 0. 95 [SEP]\n",
      "[CLS] what medication were compared in the rocket af trial? [SEP] the secondary prevention group, the rate of the principal safety outcome ( % per year ) was 17. 02 in rivaroxaban - treated patients and 18. 26 in warfarin - treated patients ( hazard ratio [ hr ] 0. 95 ; 95 % confidence interval [ ci ] 0. 70 - 1. 29 ), while the rate of the primary efficacy endpoint was 1. 66 in rivaroxaban - [SEP]\n",
      "[CLS] what medication were compared in the rocket af trial? [SEP]in - treated patients ( hazard ratio [ hr ] 0. 95 ; 95 % confidence interval [ ci ] 0. 70 - 1. 29 ), while the rate of the primary efficacy endpoint was 1. 66 in rivaroxaban - treated patients and 3. 25 in warfarin - treated patients ( hr 0. 51 ; 95 % ci 0. 23 - 1. 14 ). there were no significant interactions in [SEP]\n",
      "[CLS] what medication were compared in the rocket af trial? [SEP] endpoint was 1. 66 in rivaroxaban - treated patients and 3. 25 in warfarin - treated patients ( hr 0. 51 ; 95 % ci 0. 23 - 1. 14 ). there were no significant interactions in the principal safety and the primary efficacy endpoints of rivaroxaban compared to warfarin between the primary and secondary prevention groups ( p =. 090 and. 776 [SEP]\n",
      "[CLS] what medication were compared in the rocket af trial? [SEP] 23 - 1. 14 ). there were no significant interactions in the principal safety and the primary efficacy endpoints of rivaroxaban compared to warfarin between the primary and secondary prevention groups ( p =. 090 and. 776 for both interactions, respectively ). conclusions : the safety and efficacy profile of rivaroxaban compared with warfarin was consistent among patients in the primary prevention group and those in [SEP]\n",
      "[CLS] what medication were compared in the rocket af trial? [SEP] secondary prevention groups ( p =. 090 and. 776 for both interactions, respectively ). conclusions : the safety and efficacy profile of rivaroxaban compared with warfarin was consistent among patients in the primary prevention group and those in the secondary prevention group. [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4 examples gave 40 features.\n",
      "Here is where each comes from: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3].\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    raw_datasets[\"train\"][2:6][\"question\"],\n",
    "    raw_datasets[\"train\"][2:6][\"context\"],\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
    "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([77,\n",
       "  55,\n",
       "  33,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  63,\n",
       "  27,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  20,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  77,\n",
       "  42,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [81,\n",
       "  59,\n",
       "  37,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  64,\n",
       "  28,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  20,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  81,\n",
       "  46,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = raw_datasets[\"train\"][2:6][\"answers\"]\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n",
    "    answer = answers[sample_idx]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # If the answer is not fully inside the context, label is (0, 0)\n",
    "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: Campath-1H, labels give: campath - 1h\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "start = start_positions[idx]\n",
    "end = end_positions[idx]\n",
    "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
    "\n",
    "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: Campath-1H, decoded example: [CLS] what are the names of anti - cd52 monoclonal antibody that is used for treatment of multiple sclerosis patients? [SEP] for 18 months after a single pulse of campath - 1h. the first dose of monoclonal antibody was associated with a transient rehearsal of previous symptoms caused by the release of mediators that impede conduction at previously demyelinated sites ; this effect remained despite selective blockade of tumor necrosis factor - alpha. disease activity persisted for several weeks after [SEP]\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "decoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\n",
    "print(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdc7c6dcb0a48b08bbf6ffa9ef685e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(6878, 10378)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae3c228afac441bb485fa220bdc4380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1580991594498fb6b1f402465a600e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859 1304\n",
      "861 1308\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "\n",
    "test_dataset = raw_datasets[\"test\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"test\"].column_names,\n",
    ")\n",
    "print(len(raw_datasets[\"validation\"]), len(validation_dataset))\n",
    "print(len(raw_datasets[\"test\"]), len(test_dataset))\n",
    "eval_set = validation_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the model with the Trainer API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecbbd7dd900479b859eb83897dc56e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "small_eval_set = raw_datasets[\"test\"]\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "eval_set = small_eval_set.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"test\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "# eval_set_for_model.set_format(\"torch\")\n",
    "\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
    "# trained_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint).to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = trained_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_logits = outputs.start_logits.cpu().numpy()\n",
    "# end_logits = outputs.end_logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import collections\n",
    "\n",
    "# example_to_features = collections.defaultdict(list)\n",
    "# for idx, feature in enumerate(eval_set):\n",
    "#     example_to_features[feature[\"example_id\"]].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "# predicted_answers = []\n",
    "\n",
    "# for example in small_eval_set:\n",
    "#     example_id = example[\"id\"]\n",
    "#     context = example[\"context\"]\n",
    "#     answers = []\n",
    "\n",
    "#     for feature_index in example_to_features[example_id]:\n",
    "#         start_logit = start_logits[feature_index]\n",
    "#         end_logit = end_logits[feature_index]\n",
    "#         offsets = eval_set[\"offset_mapping\"][feature_index]\n",
    "\n",
    "#         start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "#         end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "#         for start_index in start_indexes:\n",
    "#             for end_index in end_indexes:\n",
    "#                 # Skip answers that are not fully in the context\n",
    "#                 if offsets[start_index] is None or offsets[end_index] is None:\n",
    "#                     continue\n",
    "#                 # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "#                 if (\n",
    "#                     end_index < start_index\n",
    "#                     or end_index - start_index + 1 > max_answer_length\n",
    "#                 ):\n",
    "#                     continue\n",
    "\n",
    "#                 answers.append(\n",
    "#                     {\n",
    "#                         \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "#                         \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#     best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "#     predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23941/2439953825.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad\")\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_metric\n",
    "\n",
    "# metric = load_metric(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theoretical_answers = [\n",
    "#     {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '511a4d391159fa8212000003_040', 'prediction_text': 'transferases, and'}\n",
      "{'id': '511a4d391159fa8212000003_040', 'answers': {'answer_start': [931], 'text': ['met']}}\n"
     ]
    }
   ],
   "source": [
    "# print(predicted_answers[0])\n",
    "# print(theoretical_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric\n",
    "import collections\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     \"bert-finetuned-squad\",\n",
    "#     evaluation_strategy=\"no\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     fp16=True,\n",
    "#     # push_to_hub=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=validation_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions, _, _ = trainer.predict(validation_dataset)\n",
    "# start_logits, end_logits = predictions\n",
    "# compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[\"test\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "validation_set.set_format(\"torch\")\n",
    "test_set = test_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "test_set.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    validation_set, collate_fn=default_data_collator, batch_size=8\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_set, collate_fn=default_data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model_checkpoint = 'sultan/BioM-ELECTRA-Large-SQuAD2-BioASQ8B'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader, test_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, test_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 2\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daabd4a8ac0743728e2586b5960d38be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2596 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd25fe08b784f478de9a80261ba7c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d801357ea0144fd1bf4a68b32b0f6c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'exact_match': 30.384167636786962, 'f1': 37.17356400273935}\n",
      "Evaluation!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d823b9b41f1840d7aa022b45c700c7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922b10f5d9fa42b98ea31a9fbba4a380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'exact_match': 30.384167636786962, 'f1': 37.17356400273935}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "output_dir = \"bert-finetuned-squad-accelerate\"\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    accelerator.print(\"Evaluation!\")\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(validation_dataset)]\n",
    "    end_logits = end_logits[: len(validation_dataset)]\n",
    "\n",
    "    metrics = compute_metrics(\n",
    "        start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"]\n",
    "    )\n",
    "    print(f\"epoch {epoch}:\", metrics)\n",
    "\n",
    "    # Save and upload\n",
    "    # accelerator.wait_for_everyone()\n",
    "    # unwrapped_model = accelerator.unwrap_model(model)\n",
    "    # unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    # if accelerator.is_main_process:\n",
    "    #     tokenizer.save_pretrained(output_dir)\n",
    "        # repo.push_to_hub(\n",
    "        #     commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on testset!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578b077d53bb45cc812746c314437cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2c8d532b724345a8f1e82533feaec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'exact_match': 33.44947735191638, 'f1': 40.57459053743507}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "start_logits = []\n",
    "end_logits = []\n",
    "accelerator.print(\"Evaluation on testset!\")\n",
    "for batch in tqdm(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "    end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "\n",
    "start_logits = np.concatenate(start_logits)\n",
    "end_logits = np.concatenate(end_logits)\n",
    "start_logits = start_logits[: len(test_dataset)]\n",
    "end_logits = end_logits[: len(test_dataset)]\n",
    "\n",
    "metrics = compute_metrics(\n",
    "    start_logits, end_logits, test_dataset, raw_datasets[\"test\"]\n",
    ")\n",
    "print(f\"Test metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in bert-finetuned-squad-accelerate/config.json\n",
      "Model weights saved in bert-finetuned-squad-accelerate/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# accelerator.wait_for_everyone()\n",
    "# unwrapped_model = accelerator.unwrap_model(model)\n",
    "# unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the pretrained model to inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/aoyuli/.cache/huggingface/hub/models--huggingface-course--bert-finetuned-squad/snapshots/cdce6f8f43121716ec99d2d2a28ff06ddbefa2e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"huggingface-course/bert-finetuned-squad\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/aoyuli/.cache/huggingface/hub/models--huggingface-course--bert-finetuned-squad/snapshots/cdce6f8f43121716ec99d2d2a28ff06ddbefa2e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"huggingface-course/bert-finetuned-squad\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/aoyuli/.cache/huggingface/hub/models--huggingface-course--bert-finetuned-squad/snapshots/cdce6f8f43121716ec99d2d2a28ff06ddbefa2e0/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at huggingface-course/bert-finetuned-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/aoyuli/.cache/huggingface/hub/models--huggingface-course--bert-finetuned-squad/snapshots/cdce6f8f43121716ec99d2d2a28ff06ddbefa2e0/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/aoyuli/.cache/huggingface/hub/models--huggingface-course--bert-finetuned-squad/snapshots/cdce6f8f43121716ec99d2d2a28ff06ddbefa2e0/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/aoyuli/.cache/huggingface/hub/models--huggingface-course--bert-finetuned-squad/snapshots/cdce6f8f43121716ec99d2d2a28ff06ddbefa2e0/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/aoyuli/.cache/huggingface/hub/models--huggingface-course--bert-finetuned-squad/snapshots/cdce6f8f43121716ec99d2d2a28ff06ddbefa2e0/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9978997707366943,\n",
       " 'start': 78,\n",
       " 'end': 105,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# # Replace this with your own checkpoint\n",
    "# model_checkpoint = \"huggingface-course/bert-finetuned-squad\"\n",
    "# question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n",
    "\n",
    "# context = \"\"\"\n",
    "# 🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n",
    "# between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "# \"\"\"\n",
    "# question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
    "# question_answerer(question=question, context=context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd7228a541701c6b97bb4262e359829b7ab4fb41e5273848797ee85719533ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
