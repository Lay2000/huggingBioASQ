2023-04-12 02:11:40 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-44a167365c0b341b/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'bert-base-uncased'}, 'data': {'task_type': 'list', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 4.54e-06, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/bert_list_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 520.23it/s]
Map:   0%|          | 0/6878 [00:00<?, ? examples/s]Map:  15%|█▍        | 1000/6878 [00:00<00:03, 1559.32 examples/s]Map:  29%|██▉       | 2000/6878 [00:01<00:02, 1626.24 examples/s]Map:  44%|████▎     | 3000/6878 [00:01<00:02, 1618.19 examples/s]Map:  58%|█████▊    | 4000/6878 [00:02<00:01, 1605.02 examples/s]Map:  73%|███████▎  | 5000/6878 [00:03<00:01, 1641.22 examples/s]Map:  87%|████████▋ | 6000/6878 [00:03<00:00, 1659.93 examples/s]Map: 100%|██████████| 6878/6878 [00:04<00:00, 1514.22 examples/s]                                                                 Map:   0%|          | 0/859 [00:00<?, ? examples/s]Map: 100%|██████████| 859/859 [00:00<00:00, 1318.04 examples/s]                                                               Map:   0%|          | 0/861 [00:00<?, ? examples/s]Map: 100%|██████████| 861/861 [00:00<00:00, 1323.29 examples/s]                                                               Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-12 02:12:21 - training - INFO - First Test - Val Metrics:{'exact_match': 0.0, 'f1': 2.4782061021370154} Test Metrics: {'exact_match': 0.0, 'f1': 2.0979048341590265}
2023-04-12 02:12:21 - training - INFO - Epoch [1/5][1/649] lr: 4.5e-06, eta: 1 day, 1:10:19.294164, loss: 6.0574
2023-04-12 02:12:25 - training - INFO - Epoch [1/5][11/649] lr: 4.5e-06, eta: 2:34:57.290772, loss: 5.8037
2023-04-12 02:12:28 - training - INFO - Epoch [1/5][21/649] lr: 4.5e-06, eta: 1:30:22.252160, loss: 5.4972
2023-04-12 02:12:32 - training - INFO - Epoch [1/5][31/649] lr: 4.5e-06, eta: 1:07:24.783646, loss: 5.3327
2023-04-12 02:12:36 - training - INFO - Epoch [1/5][41/649] lr: 4.5e-06, eta: 0:55:38.946072, loss: 4.8700
2023-04-12 02:12:40 - training - INFO - Epoch [1/5][51/649] lr: 4.5e-06, eta: 0:48:28.111448, loss: 4.8219
2023-04-12 02:12:43 - training - INFO - Epoch [1/5][61/649] lr: 4.5e-06, eta: 0:43:37.604608, loss: 4.6925
2023-04-12 02:12:47 - training - INFO - Epoch [1/5][71/649] lr: 4.4e-06, eta: 0:40:07.840836, loss: 4.1648
2023-04-12 02:12:51 - training - INFO - Epoch [1/5][81/649] lr: 4.4e-06, eta: 0:37:28.850968, loss: 4.2989
2023-04-12 02:12:54 - training - INFO - Epoch [1/5][91/649] lr: 4.4e-06, eta: 0:35:24.058146, loss: 3.8703
2023-04-12 02:12:58 - training - INFO - Epoch [1/5][101/649] lr: 4.4e-06, eta: 0:33:43.355784, loss: 3.9225
2023-04-12 02:13:02 - training - INFO - Epoch [1/5][111/649] lr: 4.4e-06, eta: 0:32:20.331482, loss: 3.2872
2023-04-12 02:13:06 - training - INFO - Epoch [1/5][121/649] lr: 4.4e-06, eta: 0:31:10.045144, loss: 4.1530
2023-04-12 02:13:09 - training - INFO - Epoch [1/5][131/649] lr: 4.4e-06, eta: 0:30:09.878598, loss: 3.6469
2023-04-12 02:13:13 - training - INFO - Epoch [1/5][141/649] lr: 4.3e-06, eta: 0:29:18.474976, loss: 3.8916
2023-04-12 02:13:17 - training - INFO - Epoch [1/5][151/649] lr: 4.3e-06, eta: 0:28:32.683700, loss: 3.0589
2023-04-12 02:13:20 - training - INFO - Epoch [1/5][161/649] lr: 4.3e-06, eta: 0:27:52.169472, loss: 2.6219
2023-04-12 02:13:24 - training - INFO - Epoch [1/5][171/649] lr: 4.3e-06, eta: 0:27:16.520750, loss: 4.0851
2023-04-12 02:13:28 - training - INFO - Epoch [1/5][181/649] lr: 4.3e-06, eta: 0:26:43.847736, loss: 2.7575
2023-04-12 02:13:32 - training - INFO - Epoch [1/5][191/649] lr: 4.3e-06, eta: 0:26:14.483592, loss: 3.2478
2023-04-12 02:13:35 - training - INFO - Epoch [1/5][201/649] lr: 4.3e-06, eta: 0:25:47.374784, loss: 3.4548
2023-04-12 02:13:39 - training - INFO - Epoch [1/5][211/649] lr: 4.2e-06, eta: 0:25:22.543118, loss: 2.7756
2023-04-12 02:13:43 - training - INFO - Epoch [1/5][221/649] lr: 4.2e-06, eta: 0:24:59.701392, loss: 2.7911
2023-04-12 02:13:46 - training - INFO - Epoch [1/5][231/649] lr: 4.2e-06, eta: 0:24:38.475504, loss: 3.1154
2023-04-12 02:13:50 - training - INFO - Epoch [1/5][241/649] lr: 4.2e-06, eta: 0:24:18.697340, loss: 2.7384
2023-04-12 02:13:54 - training - INFO - Epoch [1/5][251/649] lr: 4.2e-06, eta: 0:24:00.359508, loss: 3.1980
2023-04-12 02:13:58 - training - INFO - Epoch [1/5][261/649] lr: 4.2e-06, eta: 0:23:43.057664, loss: 3.3928
2023-04-12 02:14:01 - training - INFO - Epoch [1/5][271/649] lr: 4.2e-06, eta: 0:23:26.693078, loss: 2.3990
2023-04-12 02:14:05 - training - INFO - Epoch [1/5][281/649] lr: 4.1e-06, eta: 0:23:11.203788, loss: 2.0329
2023-04-12 02:14:09 - training - INFO - Epoch [1/5][291/649] lr: 4.1e-06, eta: 0:22:56.501966, loss: 2.5731
2023-04-12 02:14:12 - training - INFO - Epoch [1/5][301/649] lr: 4.1e-06, eta: 0:22:42.609792, loss: 3.0403
2023-04-12 02:14:16 - training - INFO - Epoch [1/5][311/649] lr: 4.1e-06, eta: 0:22:29.411148, loss: 2.6085
2023-04-12 02:14:20 - training - INFO - Epoch [1/5][321/649] lr: 4.1e-06, eta: 0:22:16.683208, loss: 2.4306
2023-04-12 02:14:24 - training - INFO - Epoch [1/5][331/649] lr: 4.1e-06, eta: 0:22:04.491678, loss: 3.0726
2023-04-12 02:14:27 - training - INFO - Epoch [1/5][341/649] lr: 4.1e-06, eta: 0:21:52.904208, loss: 2.5380
2023-04-12 02:14:31 - training - INFO - Epoch [1/5][351/649] lr: 4.0e-06, eta: 0:21:41.724094, loss: 2.1588
2023-04-12 02:14:35 - training - INFO - Epoch [1/5][361/649] lr: 4.0e-06, eta: 0:21:30.956268, loss: 2.9889
2023-04-12 02:14:38 - training - INFO - Epoch [1/5][371/649] lr: 4.0e-06, eta: 0:21:20.602668, loss: 2.2900
2023-04-12 02:14:42 - training - INFO - Epoch [1/5][381/649] lr: 4.0e-06, eta: 0:21:10.567776, loss: 2.4403
2023-04-12 02:14:46 - training - INFO - Epoch [1/5][391/649] lr: 4.0e-06, eta: 0:21:00.902908, loss: 2.4400
2023-04-12 02:14:50 - training - INFO - Epoch [1/5][401/649] lr: 4.0e-06, eta: 0:20:51.436788, loss: 3.0716
2023-04-12 02:14:53 - training - INFO - Epoch [1/5][411/649] lr: 4.0e-06, eta: 0:20:42.264062, loss: 3.1976
2023-04-12 02:14:57 - training - INFO - Epoch [1/5][421/649] lr: 4.0e-06, eta: 0:20:33.339640, loss: 2.5236
2023-04-12 02:15:01 - training - INFO - Epoch [1/5][431/649] lr: 3.9e-06, eta: 0:20:24.661242, loss: 2.8345
2023-04-12 02:15:04 - training - INFO - Epoch [1/5][441/649] lr: 3.9e-06, eta: 0:20:16.212568, loss: 2.4045
2023-04-12 02:15:08 - training - INFO - Epoch [1/5][451/649] lr: 3.9e-06, eta: 0:20:08.005458, loss: 2.8727
2023-04-12 02:15:12 - training - INFO - Epoch [1/5][461/649] lr: 3.9e-06, eta: 0:20:00.215808, loss: 2.7599
2023-04-12 02:15:16 - training - INFO - Epoch [1/5][471/649] lr: 3.9e-06, eta: 0:19:52.553696, loss: 2.1504
2023-04-12 02:15:19 - training - INFO - Epoch [1/5][481/649] lr: 3.9e-06, eta: 0:19:44.912980, loss: 2.3251
2023-04-12 02:15:23 - training - INFO - Epoch [1/5][491/649] lr: 3.9e-06, eta: 0:19:37.450668, loss: 2.3968
2023-04-12 02:15:27 - training - INFO - Epoch [1/5][501/649] lr: 3.8e-06, eta: 0:19:30.074528, loss: 2.8380
2023-04-12 02:15:30 - training - INFO - Epoch [1/5][511/649] lr: 3.8e-06, eta: 0:19:22.830348, loss: 2.6604
2023-04-12 02:15:34 - training - INFO - Epoch [1/5][521/649] lr: 3.8e-06, eta: 0:19:15.755064, loss: 1.6552
2023-04-12 02:15:38 - training - INFO - Epoch [1/5][531/649] lr: 3.8e-06, eta: 0:19:08.795490, loss: 2.2410
2023-04-12 02:15:42 - training - INFO - Epoch [1/5][541/649] lr: 3.8e-06, eta: 0:19:01.966800, loss: 2.8112
2023-04-12 02:15:45 - training - INFO - Epoch [1/5][551/649] lr: 3.8e-06, eta: 0:18:55.281234, loss: 2.9155
2023-04-12 02:15:49 - training - INFO - Epoch [1/5][561/649] lr: 3.8e-06, eta: 0:18:48.662260, loss: 2.5644
2023-04-12 02:15:53 - training - INFO - Epoch [1/5][571/649] lr: 3.7e-06, eta: 0:18:42.170840, loss: 2.5409
2023-04-12 02:15:56 - training - INFO - Epoch [1/5][581/649] lr: 3.7e-06, eta: 0:18:35.776440, loss: 1.6130
2023-04-12 02:16:00 - training - INFO - Epoch [1/5][591/649] lr: 3.7e-06, eta: 0:18:29.430388, loss: 3.2638
2023-04-12 02:16:04 - training - INFO - Epoch [1/5][601/649] lr: 3.7e-06, eta: 0:18:23.179916, loss: 2.3700
2023-04-12 02:16:08 - training - INFO - Epoch [1/5][611/649] lr: 3.7e-06, eta: 0:18:17.010954, loss: 2.0335
2023-04-12 02:16:11 - training - INFO - Epoch [1/5][621/649] lr: 3.7e-06, eta: 0:18:10.983104, loss: 2.3860
2023-04-12 02:16:15 - training - INFO - Epoch [1/5][631/649] lr: 3.7e-06, eta: 0:18:04.982524, loss: 2.8283
2023-04-12 02:16:19 - training - INFO - Epoch [1/5][641/649] lr: 3.6e-06, eta: 0:17:59.074164, loss: 2.7941
2023-04-12 02:16:49 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 3.1335, Validation Metrics: {'exact_match': 20.488940628637952, 'f1': 27.314721436918756}, Test Metrics: {'exact_match': 25.43554006968641, 'f1': 31.342251939980123}
2023-04-12 02:16:49 - training - INFO - Epoch [2/5][1/649] lr: 3.6e-06, eta: 11 days, 3:04:15.737068, loss: 1.8019
2023-04-12 02:16:53 - training - INFO - Epoch [2/5][11/649] lr: 3.6e-06, eta: 1 day, 0:30:20.111364, loss: 2.5030
2023-04-12 02:16:57 - training - INFO - Epoch [2/5][21/649] lr: 3.6e-06, eta: 12:57:16.339984, loss: 1.8660
2023-04-12 02:17:01 - training - INFO - Epoch [2/5][31/649] lr: 3.6e-06, eta: 8:51:18.788578, loss: 2.3064
2023-04-12 02:17:04 - training - INFO - Epoch [2/5][41/649] lr: 3.6e-06, eta: 6:45:18.347184, loss: 2.0015
2023-04-12 02:17:08 - training - INFO - Epoch [2/5][51/649] lr: 3.6e-06, eta: 5:28:41.707534, loss: 1.4979
2023-04-12 02:17:12 - training - INFO - Epoch [2/5][61/649] lr: 3.5e-06, eta: 4:37:10.707008, loss: 2.0666
2023-04-12 02:17:15 - training - INFO - Epoch [2/5][71/649] lr: 3.5e-06, eta: 4:00:09.794952, loss: 3.6537
2023-04-12 02:17:19 - training - INFO - Epoch [2/5][81/649] lr: 3.5e-06, eta: 3:32:16.245368, loss: 2.8980
2023-04-12 02:17:23 - training - INFO - Epoch [2/5][91/649] lr: 3.5e-06, eta: 3:10:29.796370, loss: 1.9432
2023-04-12 02:17:27 - training - INFO - Epoch [2/5][101/649] lr: 3.5e-06, eta: 2:53:01.623192, loss: 2.1338
2023-04-12 02:17:30 - training - INFO - Epoch [2/5][111/649] lr: 3.5e-06, eta: 2:38:41.298844, loss: 1.9456
2023-04-12 02:17:34 - training - INFO - Epoch [2/5][121/649] lr: 3.5e-06, eta: 2:26:42.572900, loss: 1.7957
2023-04-12 02:17:38 - training - INFO - Epoch [2/5][131/649] lr: 3.4e-06, eta: 2:16:32.871720, loss: 2.1171
2023-04-12 02:17:41 - training - INFO - Epoch [2/5][141/649] lr: 3.4e-06, eta: 2:07:49.276288, loss: 2.2420
2023-04-12 02:17:45 - training - INFO - Epoch [2/5][151/649] lr: 3.4e-06, eta: 2:00:14.453064, loss: 2.1615
2023-04-12 02:17:49 - training - INFO - Epoch [2/5][161/649] lr: 3.4e-06, eta: 1:53:35.612244, loss: 2.5201
2023-04-12 02:17:53 - training - INFO - Epoch [2/5][171/649] lr: 3.4e-06, eta: 1:47:43.257144, loss: 1.7836
2023-04-12 02:17:56 - training - INFO - Epoch [2/5][181/649] lr: 3.4e-06, eta: 1:42:29.190624, loss: 2.1688
2023-04-12 02:18:00 - training - INFO - Epoch [2/5][191/649] lr: 3.4e-06, eta: 1:37:47.592174, loss: 2.0323
2023-04-12 02:18:04 - training - INFO - Epoch [2/5][201/649] lr: 3.4e-06, eta: 1:33:33.671744, loss: 2.0958
2023-04-12 02:18:08 - training - INFO - Epoch [2/5][211/649] lr: 3.3e-06, eta: 1:29:43.562974, loss: 2.0365
2023-04-12 02:18:11 - training - INFO - Epoch [2/5][221/649] lr: 3.3e-06, eta: 1:26:13.924896, loss: 3.1370
2023-04-12 02:18:15 - training - INFO - Epoch [2/5][231/649] lr: 3.3e-06, eta: 1:23:02.274616, loss: 2.1477
2023-04-12 02:18:19 - training - INFO - Epoch [2/5][241/649] lr: 3.3e-06, eta: 1:20:06.048532, loss: 2.1594
2023-04-12 02:18:22 - training - INFO - Epoch [2/5][251/649] lr: 3.3e-06, eta: 1:17:23.496396, loss: 1.8189
2023-04-12 02:18:26 - training - INFO - Epoch [2/5][261/649] lr: 3.3e-06, eta: 1:14:53.184856, loss: 2.1576
2023-04-12 02:18:30 - training - INFO - Epoch [2/5][271/649] lr: 3.3e-06, eta: 1:12:33.707002, loss: 2.1260
2023-04-12 02:18:34 - training - INFO - Epoch [2/5][281/649] lr: 3.2e-06, eta: 1:10:23.877840, loss: 2.0044
2023-04-12 02:18:37 - training - INFO - Epoch [2/5][291/649] lr: 3.2e-06, eta: 1:08:22.786968, loss: 3.0433
2023-04-12 02:18:41 - training - INFO - Epoch [2/5][301/649] lr: 3.2e-06, eta: 1:06:29.599872, loss: 2.9823
2023-04-12 02:18:45 - training - INFO - Epoch [2/5][311/649] lr: 3.2e-06, eta: 1:04:43.398390, loss: 2.7912
2023-04-12 02:18:48 - training - INFO - Epoch [2/5][321/649] lr: 3.2e-06, eta: 1:03:03.626760, loss: 2.4556
2023-04-12 02:18:52 - training - INFO - Epoch [2/5][331/649] lr: 3.2e-06, eta: 1:01:29.473680, loss: 1.9150
2023-04-12 02:18:56 - training - INFO - Epoch [2/5][341/649] lr: 3.2e-06, eta: 1:00:00.692832, loss: 2.7163
2023-04-12 02:19:00 - training - INFO - Epoch [2/5][351/649] lr: 3.1e-06, eta: 0:58:36.716450, loss: 2.1954
2023-04-12 02:19:03 - training - INFO - Epoch [2/5][361/649] lr: 3.1e-06, eta: 0:57:17.145432, loss: 1.7072
2023-04-12 02:19:07 - training - INFO - Epoch [2/5][371/649] lr: 3.1e-06, eta: 0:56:01.663194, loss: 2.7990
2023-04-12 02:19:11 - training - INFO - Epoch [2/5][381/649] lr: 3.1e-06, eta: 0:54:49.962720, loss: 1.9196
2023-04-12 02:19:14 - training - INFO - Epoch [2/5][391/649] lr: 3.1e-06, eta: 0:53:41.735046, loss: 2.2755
2023-04-12 02:19:18 - training - INFO - Epoch [2/5][401/649] lr: 3.1e-06, eta: 0:52:36.703488, loss: 1.7261
2023-04-12 02:19:22 - training - INFO - Epoch [2/5][411/649] lr: 3.1e-06, eta: 0:51:34.852696, loss: 2.4739
2023-04-12 02:19:26 - training - INFO - Epoch [2/5][421/649] lr: 3.0e-06, eta: 0:50:35.633384, loss: 2.7534
2023-04-12 02:19:29 - training - INFO - Epoch [2/5][431/649] lr: 3.0e-06, eta: 0:49:38.959494, loss: 1.8426
2023-04-12 02:19:33 - training - INFO - Epoch [2/5][441/649] lr: 3.0e-06, eta: 0:48:44.703788, loss: 1.5973
2023-04-12 02:19:37 - training - INFO - Epoch [2/5][451/649] lr: 3.0e-06, eta: 0:47:52.737714, loss: 2.2143
2023-04-12 02:19:41 - training - INFO - Epoch [2/5][461/649] lr: 3.0e-06, eta: 0:47:02.859072, loss: 2.0000
2023-04-12 02:19:44 - training - INFO - Epoch [2/5][471/649] lr: 3.0e-06, eta: 0:46:14.901550, loss: 2.1070
2023-04-12 02:19:48 - training - INFO - Epoch [2/5][481/649] lr: 3.0e-06, eta: 0:45:28.775584, loss: 2.1140
2023-04-12 02:19:52 - training - INFO - Epoch [2/5][491/649] lr: 2.9e-06, eta: 0:44:44.376126, loss: 1.9543
2023-04-12 02:19:55 - training - INFO - Epoch [2/5][501/649] lr: 2.9e-06, eta: 0:44:01.659776, loss: 2.6313
2023-04-12 02:19:59 - training - INFO - Epoch [2/5][511/649] lr: 2.9e-06, eta: 0:43:20.430430, loss: 2.0362
2023-04-12 02:20:03 - training - INFO - Epoch [2/5][521/649] lr: 2.9e-06, eta: 0:42:40.603584, loss: 1.6057
2023-04-12 02:20:07 - training - INFO - Epoch [2/5][531/649] lr: 2.9e-06, eta: 0:42:02.144626, loss: 1.8857
2023-04-12 02:20:10 - training - INFO - Epoch [2/5][541/649] lr: 2.9e-06, eta: 0:41:25.024672, loss: 2.1006
2023-04-12 02:20:14 - training - INFO - Epoch [2/5][551/649] lr: 2.9e-06, eta: 0:40:49.115400, loss: 2.9244
2023-04-12 02:20:18 - training - INFO - Epoch [2/5][561/649] lr: 2.8e-06, eta: 0:40:14.271420, loss: 1.6130
2023-04-12 02:20:21 - training - INFO - Epoch [2/5][571/649] lr: 2.8e-06, eta: 0:39:40.547218, loss: 2.0335
2023-04-12 02:20:25 - training - INFO - Epoch [2/5][581/649] lr: 2.8e-06, eta: 0:39:07.892424, loss: 2.8839
2023-04-12 02:20:29 - training - INFO - Epoch [2/5][591/649] lr: 2.8e-06, eta: 0:38:36.241344, loss: 2.3196
2023-04-12 02:20:33 - training - INFO - Epoch [2/5][601/649] lr: 2.8e-06, eta: 0:38:05.452448, loss: 2.4424
2023-04-12 02:20:36 - training - INFO - Epoch [2/5][611/649] lr: 2.8e-06, eta: 0:37:35.570586, loss: 2.0514
2023-04-12 02:20:40 - training - INFO - Epoch [2/5][621/649] lr: 2.8e-06, eta: 0:37:06.532224, loss: 1.7105
2023-04-12 02:20:44 - training - INFO - Epoch [2/5][631/649] lr: 2.7e-06, eta: 0:36:38.285124, loss: 3.3051
2023-04-12 02:20:47 - training - INFO - Epoch [2/5][641/649] lr: 2.7e-06, eta: 0:36:10.855848, loss: 2.3762
2023-04-12 02:21:18 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 2.3030, Validation Metrics: {'exact_match': 26.30966239813737, 'f1': 33.58085906715966}, Test Metrics: {'exact_match': 31.475029036004646, 'f1': 38.06979870512646}
2023-04-12 02:21:18 - training - INFO - Epoch [3/5][1/649] lr: 2.7e-06, eta: 21 days, 5:18:36.880804, loss: 2.2803
2023-04-12 02:21:22 - training - INFO - Epoch [3/5][11/649] lr: 2.7e-06, eta: 1 day, 22:27:42.515574, loss: 1.7837
2023-04-12 02:21:26 - training - INFO - Epoch [3/5][21/649] lr: 2.7e-06, eta: 1 day, 0:25:11.841784, loss: 2.0236
2023-04-12 02:21:29 - training - INFO - Epoch [3/5][31/649] lr: 2.7e-06, eta: 16:35:53.755940, loss: 1.7857
2023-04-12 02:21:33 - training - INFO - Epoch [3/5][41/649] lr: 2.7e-06, eta: 12:35:29.192352, loss: 1.5731
2023-04-12 02:21:37 - training - INFO - Epoch [3/5][51/649] lr: 2.7e-06, eta: 10:09:19.696198, loss: 2.2540
2023-04-12 02:21:41 - training - INFO - Epoch [3/5][61/649] lr: 2.6e-06, eta: 8:31:04.890672, loss: 2.7205
2023-04-12 02:21:44 - training - INFO - Epoch [3/5][71/649] lr: 2.6e-06, eta: 7:20:29.126718, loss: 2.4737
2023-04-12 02:21:48 - training - INFO - Epoch [3/5][81/649] lr: 2.6e-06, eta: 6:27:18.804820, loss: 1.7827
2023-04-12 02:21:52 - training - INFO - Epoch [3/5][91/649] lr: 2.6e-06, eta: 5:45:48.592154, loss: 1.7866
2023-04-12 02:21:55 - training - INFO - Epoch [3/5][101/649] lr: 2.6e-06, eta: 5:12:30.875736, loss: 2.5959
2023-04-12 02:21:59 - training - INFO - Epoch [3/5][111/649] lr: 2.6e-06, eta: 4:45:12.304408, loss: 2.2466
2023-04-12 02:22:03 - training - INFO - Epoch [3/5][121/649] lr: 2.6e-06, eta: 4:22:23.919708, loss: 1.9340
2023-04-12 02:22:07 - training - INFO - Epoch [3/5][131/649] lr: 2.5e-06, eta: 4:03:03.715236, loss: 2.1277
2023-04-12 02:22:10 - training - INFO - Epoch [3/5][141/649] lr: 2.5e-06, eta: 3:46:27.812768, loss: 2.9461
2023-04-12 02:22:14 - training - INFO - Epoch [3/5][151/649] lr: 2.5e-06, eta: 3:32:03.678968, loss: 2.4548
2023-04-12 02:22:18 - training - INFO - Epoch [3/5][161/649] lr: 2.5e-06, eta: 3:19:25.858320, loss: 1.7816
2023-04-12 02:22:22 - training - INFO - Epoch [3/5][171/649] lr: 2.5e-06, eta: 3:08:16.433568, loss: 1.8501
2023-04-12 02:22:25 - training - INFO - Epoch [3/5][181/649] lr: 2.5e-06, eta: 2:58:20.398008, loss: 2.1139
2023-04-12 02:22:29 - training - INFO - Epoch [3/5][191/649] lr: 2.5e-06, eta: 2:49:26.533896, loss: 1.6608
2023-04-12 02:22:33 - training - INFO - Epoch [3/5][201/649] lr: 2.4e-06, eta: 2:41:25.380936, loss: 1.6062
2023-04-12 02:22:36 - training - INFO - Epoch [3/5][211/649] lr: 2.4e-06, eta: 2:34:09.376550, loss: 2.2071
2023-04-12 02:22:40 - training - INFO - Epoch [3/5][221/649] lr: 2.4e-06, eta: 2:27:32.527152, loss: 1.9926
2023-04-12 02:22:44 - training - INFO - Epoch [3/5][231/649] lr: 2.4e-06, eta: 2:21:29.729710, loss: 2.2088
2023-04-12 02:22:48 - training - INFO - Epoch [3/5][241/649] lr: 2.4e-06, eta: 2:15:56.713136, loss: 2.7814
2023-04-12 02:22:51 - training - INFO - Epoch [3/5][251/649] lr: 2.4e-06, eta: 2:10:50.336862, loss: 2.7657
2023-04-12 02:22:55 - training - INFO - Epoch [3/5][261/649] lr: 2.4e-06, eta: 2:06:07.068904, loss: 1.7990
2023-04-12 02:22:59 - training - INFO - Epoch [3/5][271/649] lr: 2.3e-06, eta: 2:01:44.245116, loss: 1.8879
2023-04-12 02:23:02 - training - INFO - Epoch [3/5][281/649] lr: 2.3e-06, eta: 1:57:39.788580, loss: 1.6195
2023-04-12 02:23:06 - training - INFO - Epoch [3/5][291/649] lr: 2.3e-06, eta: 1:53:51.895994, loss: 1.4553
2023-04-12 02:23:10 - training - INFO - Epoch [3/5][301/649] lr: 2.3e-06, eta: 1:50:18.980480, loss: 2.1321
2023-04-12 02:23:14 - training - INFO - Epoch [3/5][311/649] lr: 2.3e-06, eta: 1:46:59.404224, loss: 1.7525
2023-04-12 02:23:17 - training - INFO - Epoch [3/5][321/649] lr: 2.3e-06, eta: 1:43:52.160968, loss: 1.7099
2023-04-12 02:23:21 - training - INFO - Epoch [3/5][331/649] lr: 2.3e-06, eta: 1:40:56.067124, loss: 1.7023
2023-04-12 02:23:25 - training - INFO - Epoch [3/5][341/649] lr: 2.2e-06, eta: 1:38:10.038000, loss: 2.3558
2023-04-12 02:23:28 - training - INFO - Epoch [3/5][351/649] lr: 2.2e-06, eta: 1:35:33.126866, loss: 1.9334
2023-04-12 02:23:32 - training - INFO - Epoch [3/5][361/649] lr: 2.2e-06, eta: 1:33:04.713148, loss: 1.6286
2023-04-12 02:23:36 - training - INFO - Epoch [3/5][371/649] lr: 2.2e-06, eta: 1:30:44.100366, loss: 2.0872
2023-04-12 02:23:40 - training - INFO - Epoch [3/5][381/649] lr: 2.2e-06, eta: 1:28:30.729520, loss: 1.7892
2023-04-12 02:23:43 - training - INFO - Epoch [3/5][391/649] lr: 2.2e-06, eta: 1:26:24.054118, loss: 1.6014
2023-04-12 02:23:47 - training - INFO - Epoch [3/5][401/649] lr: 2.2e-06, eta: 1:24:23.628240, loss: 1.8766
2023-04-12 02:23:51 - training - INFO - Epoch [3/5][411/649] lr: 2.1e-06, eta: 1:22:28.739302, loss: 1.5405
2023-04-12 02:23:55 - training - INFO - Epoch [3/5][421/649] lr: 2.1e-06, eta: 1:20:39.031312, loss: 1.9283
2023-04-12 02:23:58 - training - INFO - Epoch [3/5][431/649] lr: 2.1e-06, eta: 1:18:54.253902, loss: 1.6885
2023-04-12 02:24:02 - training - INFO - Epoch [3/5][441/649] lr: 2.1e-06, eta: 1:17:14.039012, loss: 2.0038
2023-04-12 02:24:06 - training - INFO - Epoch [3/5][451/649] lr: 2.1e-06, eta: 1:15:38.146118, loss: 2.0770
2023-04-12 02:24:09 - training - INFO - Epoch [3/5][461/649] lr: 2.1e-06, eta: 1:14:06.309696, loss: 2.1831
2023-04-12 02:24:13 - training - INFO - Epoch [3/5][471/649] lr: 2.1e-06, eta: 1:12:38.153728, loss: 2.0888
2023-04-12 02:24:17 - training - INFO - Epoch [3/5][481/649] lr: 2.1e-06, eta: 1:11:13.470152, loss: 2.4200
2023-04-12 02:24:21 - training - INFO - Epoch [3/5][491/649] lr: 2.0e-06, eta: 1:09:52.064442, loss: 1.7574
2023-04-12 02:24:24 - training - INFO - Epoch [3/5][501/649] lr: 2.0e-06, eta: 1:08:33.771872, loss: 1.5998
2023-04-12 02:24:28 - training - INFO - Epoch [3/5][511/649] lr: 2.0e-06, eta: 1:07:18.446080, loss: 2.0990
2023-04-12 02:24:32 - training - INFO - Epoch [3/5][521/649] lr: 2.0e-06, eta: 1:06:05.909736, loss: 2.1890
2023-04-12 02:24:35 - training - INFO - Epoch [3/5][531/649] lr: 2.0e-06, eta: 1:04:55.903576, loss: 2.1087
2023-04-12 02:24:39 - training - INFO - Epoch [3/5][541/649] lr: 2.0e-06, eta: 1:03:48.425952, loss: 1.6790
2023-04-12 02:24:43 - training - INFO - Epoch [3/5][551/649] lr: 2.0e-06, eta: 1:02:43.253988, loss: 2.1444
2023-04-12 02:24:47 - training - INFO - Epoch [3/5][561/649] lr: 1.9e-06, eta: 1:01:40.191924, loss: 1.8033
2023-04-12 02:24:50 - training - INFO - Epoch [3/5][571/649] lr: 1.9e-06, eta: 1:00:39.209714, loss: 1.9069
2023-04-12 02:24:54 - training - INFO - Epoch [3/5][581/649] lr: 1.9e-06, eta: 0:59:40.216200, loss: 1.8155
2023-04-12 02:24:58 - training - INFO - Epoch [3/5][591/649] lr: 1.9e-06, eta: 0:58:43.065570, loss: 2.5151
2023-04-12 02:25:01 - training - INFO - Epoch [3/5][601/649] lr: 1.9e-06, eta: 0:57:47.711760, loss: 1.7452
2023-04-12 02:25:05 - training - INFO - Epoch [3/5][611/649] lr: 1.9e-06, eta: 0:56:54.056466, loss: 1.6923
2023-04-12 02:25:09 - training - INFO - Epoch [3/5][621/649] lr: 1.9e-06, eta: 0:56:02.028864, loss: 1.4427
2023-04-12 02:25:12 - training - INFO - Epoch [3/5][631/649] lr: 1.8e-06, eta: 0:55:11.514532, loss: 2.5464
2023-04-12 02:25:16 - training - INFO - Epoch [3/5][641/649] lr: 1.8e-06, eta: 0:54:22.473480, loss: 2.1175
2023-04-12 02:25:47 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 2.0616, Validation Metrics: {'exact_match': 27.59022118742724, 'f1': 34.97024480753491}, Test Metrics: {'exact_match': 32.98490127758421, 'f1': 40.06926630225176}
2023-04-12 02:25:47 - training - INFO - Epoch [4/5][1/649] lr: 1.8e-06, eta: 31 days, 7:40:10.011800, loss: 2.1993
2023-04-12 02:25:51 - training - INFO - Epoch [4/5][11/649] lr: 1.8e-06, eta: 2 days, 20:25:31.839600, loss: 1.2596
2023-04-12 02:25:55 - training - INFO - Epoch [4/5][21/649] lr: 1.8e-06, eta: 1 day, 11:53:22.070816, loss: 2.1916
2023-04-12 02:25:58 - training - INFO - Epoch [4/5][31/649] lr: 1.8e-06, eta: 1 day, 0:20:37.491094, loss: 1.8075
2023-04-12 02:26:02 - training - INFO - Epoch [4/5][41/649] lr: 1.8e-06, eta: 18:25:47.621388, loss: 1.6530
2023-04-12 02:26:06 - training - INFO - Epoch [4/5][51/649] lr: 1.7e-06, eta: 14:50:05.851920, loss: 1.5386
2023-04-12 02:26:10 - training - INFO - Epoch [4/5][61/649] lr: 1.7e-06, eta: 12:25:04.630416, loss: 2.3617
2023-04-12 02:26:13 - training - INFO - Epoch [4/5][71/649] lr: 1.7e-06, eta: 10:40:53.463882, loss: 1.2377
2023-04-12 02:26:17 - training - INFO - Epoch [4/5][81/649] lr: 1.7e-06, eta: 9:22:24.945920, loss: 1.6401
2023-04-12 02:26:21 - training - INFO - Epoch [4/5][91/649] lr: 1.7e-06, eta: 8:21:10.384238, loss: 1.9557
2023-04-12 02:26:24 - training - INFO - Epoch [4/5][101/649] lr: 1.7e-06, eta: 7:32:03.027048, loss: 1.7234
2023-04-12 02:26:28 - training - INFO - Epoch [4/5][111/649] lr: 1.7e-06, eta: 6:51:46.403230, loss: 1.6855
2023-04-12 02:26:32 - training - INFO - Epoch [4/5][121/649] lr: 1.6e-06, eta: 6:18:08.050000, loss: 2.1119
2023-04-12 02:26:36 - training - INFO - Epoch [4/5][131/649] lr: 1.6e-06, eta: 5:49:37.563762, loss: 2.2672
2023-04-12 02:26:39 - training - INFO - Epoch [4/5][141/649] lr: 1.6e-06, eta: 5:25:09.006272, loss: 1.5112
2023-04-12 02:26:43 - training - INFO - Epoch [4/5][151/649] lr: 1.6e-06, eta: 5:03:54.816964, loss: 2.0215
2023-04-12 02:26:47 - training - INFO - Epoch [4/5][161/649] lr: 1.6e-06, eta: 4:45:18.442068, loss: 2.0575
2023-04-12 02:26:51 - training - INFO - Epoch [4/5][171/649] lr: 1.6e-06, eta: 4:28:51.906270, loss: 1.7502
2023-04-12 02:26:54 - training - INFO - Epoch [4/5][181/649] lr: 1.6e-06, eta: 4:14:13.955480, loss: 1.8148
2023-04-12 02:26:58 - training - INFO - Epoch [4/5][191/649] lr: 1.5e-06, eta: 4:01:07.729470, loss: 1.7910
2023-04-12 02:27:02 - training - INFO - Epoch [4/5][201/649] lr: 1.5e-06, eta: 3:49:19.169180, loss: 1.7184
2023-04-12 02:27:05 - training - INFO - Epoch [4/5][211/649] lr: 1.5e-06, eta: 3:38:37.483830, loss: 2.6641
2023-04-12 02:27:09 - training - INFO - Epoch [4/5][221/649] lr: 1.5e-06, eta: 3:28:53.512320, loss: 2.6259
2023-04-12 02:27:13 - training - INFO - Epoch [4/5][231/649] lr: 1.5e-06, eta: 3:19:59.791914, loss: 2.5478
2023-04-12 02:27:17 - training - INFO - Epoch [4/5][241/649] lr: 1.5e-06, eta: 3:11:50.027268, loss: 1.8081
2023-04-12 02:27:20 - training - INFO - Epoch [4/5][251/649] lr: 1.5e-06, eta: 3:04:19.087500, loss: 1.9790
2023-04-12 02:27:24 - training - INFO - Epoch [4/5][261/649] lr: 1.5e-06, eta: 2:57:22.406160, loss: 2.2544
2023-04-12 02:27:28 - training - INFO - Epoch [4/5][271/649] lr: 1.4e-06, eta: 2:50:56.195880, loss: 1.6690
2023-04-12 02:27:31 - training - INFO - Epoch [4/5][281/649] lr: 1.4e-06, eta: 2:44:57.207996, loss: 1.5492
2023-04-12 02:27:35 - training - INFO - Epoch [4/5][291/649] lr: 1.4e-06, eta: 2:39:22.753788, loss: 2.6942
2023-04-12 02:27:39 - training - INFO - Epoch [4/5][301/649] lr: 1.4e-06, eta: 2:34:10.133376, loss: 1.5206
2023-04-12 02:27:43 - training - INFO - Epoch [4/5][311/649] lr: 1.4e-06, eta: 2:29:17.340630, loss: 1.8595
2023-04-12 02:27:46 - training - INFO - Epoch [4/5][321/649] lr: 1.4e-06, eta: 2:24:42.548992, loss: 1.4404
2023-04-12 02:27:50 - training - INFO - Epoch [4/5][331/649] lr: 1.4e-06, eta: 2:20:24.091342, loss: 1.7561
2023-04-12 02:27:54 - training - INFO - Epoch [4/5][341/649] lr: 1.3e-06, eta: 2:16:20.591232, loss: 1.6886
2023-04-12 02:27:57 - training - INFO - Epoch [4/5][351/649] lr: 1.3e-06, eta: 2:12:30.888780, loss: 1.8202
2023-04-12 02:28:01 - training - INFO - Epoch [4/5][361/649] lr: 1.3e-06, eta: 2:08:53.676720, loss: 2.3254
2023-04-12 02:28:05 - training - INFO - Epoch [4/5][371/649] lr: 1.3e-06, eta: 2:05:27.922806, loss: 1.6990
2023-04-12 02:28:09 - training - INFO - Epoch [4/5][381/649] lr: 1.3e-06, eta: 2:02:12.911136, loss: 1.4122
2023-04-12 02:28:12 - training - INFO - Epoch [4/5][391/649] lr: 1.3e-06, eta: 1:59:07.797336, loss: 2.8794
2023-04-12 02:28:16 - training - INFO - Epoch [4/5][401/649] lr: 1.3e-06, eta: 1:56:11.468760, loss: 2.4495
2023-04-12 02:28:20 - training - INFO - Epoch [4/5][411/649] lr: 1.2e-06, eta: 1:53:23.552626, loss: 1.8408
2023-04-12 02:28:24 - training - INFO - Epoch [4/5][421/649] lr: 1.2e-06, eta: 1:50:43.426112, loss: 1.5209
2023-04-12 02:28:27 - training - INFO - Epoch [4/5][431/649] lr: 1.2e-06, eta: 1:48:10.544466, loss: 2.4927
2023-04-12 02:28:31 - training - INFO - Epoch [4/5][441/649] lr: 1.2e-06, eta: 1:45:44.406108, loss: 2.0182
2023-04-12 02:28:35 - training - INFO - Epoch [4/5][451/649] lr: 1.2e-06, eta: 1:43:24.700062, loss: 1.6502
2023-04-12 02:28:38 - training - INFO - Epoch [4/5][461/649] lr: 1.2e-06, eta: 1:41:10.832160, loss: 1.9283
2023-04-12 02:28:42 - training - INFO - Epoch [4/5][471/649] lr: 1.2e-06, eta: 1:39:02.512732, loss: 1.3705
2023-04-12 02:28:46 - training - INFO - Epoch [4/5][481/649] lr: 1.1e-06, eta: 1:36:59.425104, loss: 1.9714
2023-04-12 02:28:50 - training - INFO - Epoch [4/5][491/649] lr: 1.1e-06, eta: 1:35:01.154544, loss: 1.5306
2023-04-12 02:28:53 - training - INFO - Epoch [4/5][501/649] lr: 1.1e-06, eta: 1:33:07.491952, loss: 1.6525
2023-04-12 02:28:57 - training - INFO - Epoch [4/5][511/649] lr: 1.1e-06, eta: 1:31:18.080258, loss: 1.8155
2023-04-12 02:29:01 - training - INFO - Epoch [4/5][521/649] lr: 1.1e-06, eta: 1:29:32.681400, loss: 2.0096
2023-04-12 02:29:04 - training - INFO - Epoch [4/5][531/649] lr: 1.1e-06, eta: 1:27:51.214934, loss: 1.6604
2023-04-12 02:29:08 - training - INFO - Epoch [4/5][541/649] lr: 1.1e-06, eta: 1:26:13.317136, loss: 2.0430
2023-04-12 02:29:12 - training - INFO - Epoch [4/5][551/649] lr: 1.0e-06, eta: 1:24:38.796150, loss: 2.0670
2023-04-12 02:29:16 - training - INFO - Epoch [4/5][561/649] lr: 1.0e-06, eta: 1:23:07.521528, loss: 2.0345
2023-04-12 02:29:19 - training - INFO - Epoch [4/5][571/649] lr: 1.0e-06, eta: 1:21:39.294778, loss: 1.5427
2023-04-12 02:29:23 - training - INFO - Epoch [4/5][581/649] lr: 1.0e-06, eta: 1:20:13.981200, loss: 2.0034
2023-04-12 02:29:27 - training - INFO - Epoch [4/5][591/649] lr: 9.9e-07, eta: 1:18:51.588356, loss: 1.8882
2023-04-12 02:29:30 - training - INFO - Epoch [4/5][601/649] lr: 9.8e-07, eta: 1:17:31.721400, loss: 1.9368
2023-04-12 02:29:34 - training - INFO - Epoch [4/5][611/649] lr: 9.6e-07, eta: 1:16:14.280786, loss: 2.0500
2023-04-12 02:29:38 - training - INFO - Epoch [4/5][621/649] lr: 9.5e-07, eta: 1:14:59.223232, loss: 2.8164
2023-04-12 02:29:42 - training - INFO - Epoch [4/5][631/649] lr: 9.3e-07, eta: 1:13:46.432584, loss: 1.6775
2023-04-12 02:29:45 - training - INFO - Epoch [4/5][641/649] lr: 9.2e-07, eta: 1:12:35.817564, loss: 1.7937
2023-04-12 02:30:16 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 1.9268, Validation Metrics: {'exact_match': 29.452852153667056, 'f1': 36.616939540482655}, Test Metrics: {'exact_match': 34.37862950058072, 'f1': 40.9379479078366}
2023-04-12 02:30:16 - training - INFO - Epoch [5/5][1/649] lr: 9.1e-07, eta: 41 days, 9:53:55.173088, loss: 2.0481
2023-04-12 02:30:20 - training - INFO - Epoch [5/5][11/649] lr: 8.9e-07, eta: 3 days, 18:22:42.145416, loss: 2.1488
2023-04-12 02:30:24 - training - INFO - Epoch [5/5][21/649] lr: 8.8e-07, eta: 1 day, 23:21:11.498600, loss: 2.0923
2023-04-12 02:30:27 - training - INFO - Epoch [5/5][31/649] lr: 8.6e-07, eta: 1 day, 8:05:08.106700, loss: 2.2304
2023-04-12 02:30:31 - training - INFO - Epoch [5/5][41/649] lr: 8.5e-07, eta: 1 day, 0:15:53.702208, loss: 1.6034
2023-04-12 02:30:35 - training - INFO - Epoch [5/5][51/649] lr: 8.4e-07, eta: 19:30:38.979316, loss: 2.0036
2023-04-12 02:30:38 - training - INFO - Epoch [5/5][61/649] lr: 8.2e-07, eta: 16:18:54.356480, loss: 1.4858
2023-04-12 02:30:42 - training - INFO - Epoch [5/5][71/649] lr: 8.1e-07, eta: 14:01:09.408990, loss: 2.0107
2023-04-12 02:30:46 - training - INFO - Epoch [5/5][81/649] lr: 7.9e-07, eta: 12:17:24.613476, loss: 1.9103
2023-04-12 02:30:50 - training - INFO - Epoch [5/5][91/649] lr: 7.8e-07, eta: 10:56:27.104690, loss: 1.8408
2023-04-12 02:30:53 - training - INFO - Epoch [5/5][101/649] lr: 7.7e-07, eta: 9:51:30.808200, loss: 2.3709
2023-04-12 02:30:57 - training - INFO - Epoch [5/5][111/649] lr: 7.5e-07, eta: 8:58:15.722702, loss: 2.1568
2023-04-12 02:31:01 - training - INFO - Epoch [5/5][121/649] lr: 7.4e-07, eta: 8:13:48.197192, loss: 2.0864
2023-04-12 02:31:04 - training - INFO - Epoch [5/5][131/649] lr: 7.2e-07, eta: 7:36:07.065144, loss: 1.6624
2023-04-12 02:31:08 - training - INFO - Epoch [5/5][141/649] lr: 7.1e-07, eta: 7:03:46.192512, loss: 1.7098
2023-04-12 02:31:12 - training - INFO - Epoch [5/5][151/649] lr: 7.0e-07, eta: 6:35:41.969888, loss: 1.6311
2023-04-12 02:31:16 - training - INFO - Epoch [5/5][161/649] lr: 6.8e-07, eta: 6:11:06.476916, loss: 2.3005
2023-04-12 02:31:19 - training - INFO - Epoch [5/5][171/649] lr: 6.7e-07, eta: 5:49:23.069224, loss: 2.1581
2023-04-12 02:31:23 - training - INFO - Epoch [5/5][181/649] lr: 6.5e-07, eta: 5:30:03.376552, loss: 1.9068
2023-04-12 02:31:27 - training - INFO - Epoch [5/5][191/649] lr: 6.4e-07, eta: 5:12:44.808252, loss: 1.5487
2023-04-12 02:31:30 - training - INFO - Epoch [5/5][201/649] lr: 6.3e-07, eta: 4:57:09.055016, loss: 1.9681
2023-04-12 02:31:34 - training - INFO - Epoch [5/5][211/649] lr: 6.1e-07, eta: 4:43:01.786474, loss: 2.5839
2023-04-12 02:31:38 - training - INFO - Epoch [5/5][221/649] lr: 6.0e-07, eta: 4:30:11.022912, loss: 1.4130
2023-04-12 02:31:42 - training - INFO - Epoch [5/5][231/649] lr: 5.8e-07, eta: 4:18:26.484466, loss: 1.7012
2023-04-12 02:31:45 - training - INFO - Epoch [5/5][241/649] lr: 5.7e-07, eta: 4:07:40.009964, loss: 1.9332
2023-04-12 02:31:49 - training - INFO - Epoch [5/5][251/649] lr: 5.6e-07, eta: 3:57:44.859108, loss: 2.4354
2023-04-12 02:31:53 - training - INFO - Epoch [5/5][261/649] lr: 5.4e-07, eta: 3:48:34.950392, loss: 1.3283
2023-04-12 02:31:56 - training - INFO - Epoch [5/5][271/649] lr: 5.3e-07, eta: 3:40:05.398668, loss: 2.4050
2023-04-12 02:32:00 - training - INFO - Epoch [5/5][281/649] lr: 5.1e-07, eta: 3:32:11.876820, loss: 1.2967
2023-04-12 02:32:04 - training - INFO - Epoch [5/5][291/649] lr: 5.0e-07, eta: 3:24:50.628042, loss: 2.3057
2023-04-12 02:32:08 - training - INFO - Epoch [5/5][301/649] lr: 4.9e-07, eta: 3:17:58.483584, loss: 1.5728
2023-04-12 02:32:11 - training - INFO - Epoch [5/5][311/649] lr: 4.7e-07, eta: 3:11:32.709786, loss: 2.0228
2023-04-12 02:32:15 - training - INFO - Epoch [5/5][321/649] lr: 4.6e-07, eta: 3:05:30.524716, loss: 1.4973
2023-04-12 02:32:19 - training - INFO - Epoch [5/5][331/649] lr: 4.4e-07, eta: 2:59:50.034964, loss: 1.4507
2023-04-12 02:32:22 - training - INFO - Epoch [5/5][341/649] lr: 4.3e-07, eta: 2:54:29.512416, loss: 1.7837
2023-04-12 02:32:26 - training - INFO - Epoch [5/5][351/649] lr: 4.2e-07, eta: 2:49:26.896930, loss: 1.3669
2023-04-12 02:32:30 - training - INFO - Epoch [5/5][361/649] lr: 4.0e-07, eta: 2:44:40.757040, loss: 1.2082
2023-04-12 02:32:34 - training - INFO - Epoch [5/5][371/649] lr: 3.9e-07, eta: 2:40:09.897264, loss: 1.6290
2023-04-12 02:32:37 - training - INFO - Epoch [5/5][381/649] lr: 3.7e-07, eta: 2:35:53.093680, loss: 1.5901
2023-04-12 02:32:41 - training - INFO - Epoch [5/5][391/649] lr: 3.6e-07, eta: 2:31:49.183150, loss: 2.1855
2023-04-12 02:32:45 - training - INFO - Epoch [5/5][401/649] lr: 3.5e-07, eta: 2:27:57.221784, loss: 2.5590
2023-04-12 02:32:49 - training - INFO - Epoch [5/5][411/649] lr: 3.3e-07, eta: 2:24:16.418992, loss: 1.8633
2023-04-12 02:32:52 - training - INFO - Epoch [5/5][421/649] lr: 3.2e-07, eta: 2:20:46.013552, loss: 2.3404
2023-04-12 02:32:56 - training - INFO - Epoch [5/5][431/649] lr: 3.0e-07, eta: 2:17:25.121304, loss: 1.7835
2023-04-12 02:33:00 - training - INFO - Epoch [5/5][441/649] lr: 2.9e-07, eta: 2:14:13.242220, loss: 2.3285
2023-04-12 02:33:03 - training - INFO - Epoch [5/5][451/649] lr: 2.8e-07, eta: 2:11:09.597164, loss: 2.5331
2023-04-12 02:33:07 - training - INFO - Epoch [5/5][461/649] lr: 2.6e-07, eta: 2:08:13.784448, loss: 1.8843
2023-04-12 02:33:11 - training - INFO - Epoch [5/5][471/649] lr: 2.5e-07, eta: 2:05:25.304426, loss: 1.7320
2023-04-12 02:33:15 - training - INFO - Epoch [5/5][481/649] lr: 2.4e-07, eta: 2:02:43.707836, loss: 2.1985
2023-04-12 02:33:18 - training - INFO - Epoch [5/5][491/649] lr: 2.2e-07, eta: 2:00:08.468316, loss: 1.6540
2023-04-12 02:33:22 - training - INFO - Epoch [5/5][501/649] lr: 2.1e-07, eta: 1:57:39.332392, loss: 1.9602
2023-04-12 02:33:26 - training - INFO - Epoch [5/5][511/649] lr: 1.9e-07, eta: 1:55:15.844380, loss: 1.7275
2023-04-12 02:33:29 - training - INFO - Epoch [5/5][521/649] lr: 1.8e-07, eta: 1:52:57.761460, loss: 1.6964
2023-04-12 02:33:33 - training - INFO - Epoch [5/5][531/649] lr: 1.7e-07, eta: 1:50:44.710626, loss: 2.1279
2023-04-12 02:33:37 - training - INFO - Epoch [5/5][541/649] lr: 1.5e-07, eta: 1:48:36.434496, loss: 2.0106
2023-04-12 02:33:41 - training - INFO - Epoch [5/5][551/649] lr: 1.4e-07, eta: 1:46:32.665338, loss: 1.4794
2023-04-12 02:33:44 - training - INFO - Epoch [5/5][561/649] lr: 1.2e-07, eta: 1:44:33.181684, loss: 1.6351
2023-04-12 02:33:48 - training - INFO - Epoch [5/5][571/649] lr: 1.1e-07, eta: 1:42:37.716614, loss: 1.6716
2023-04-12 02:33:52 - training - INFO - Epoch [5/5][581/649] lr: 9.5e-08, eta: 1:40:46.155792, loss: 1.7464
2023-04-12 02:33:55 - training - INFO - Epoch [5/5][591/649] lr: 8.1e-08, eta: 1:38:58.271920, loss: 2.5940
2023-04-12 02:33:59 - training - INFO - Epoch [5/5][601/649] lr: 6.7e-08, eta: 1:37:13.790344, loss: 2.0576
2023-04-12 02:34:03 - training - INFO - Epoch [5/5][611/649] lr: 5.3e-08, eta: 1:35:32.608626, loss: 1.7679
2023-04-12 02:34:07 - training - INFO - Epoch [5/5][621/649] lr: 3.9e-08, eta: 1:33:54.562432, loss: 1.4987
2023-04-12 02:34:10 - training - INFO - Epoch [5/5][631/649] lr: 2.5e-08, eta: 1:32:19.565274, loss: 1.8213
2023-04-12 02:34:14 - training - INFO - Epoch [5/5][641/649] lr: 1.1e-08, eta: 1:30:47.390928, loss: 1.4629
2023-04-12 02:34:45 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 1.8495, Validation Metrics: {'exact_match': 29.10360884749709, 'f1': 36.187743203253724}, Test Metrics: {'exact_match': 34.61091753774681, 'f1': 41.21361139778582}
2023-04-12 02:34:58 - training - INFO - Final Test - Train Loss: 1.8495, Test Metrics: {'exact_match': 34.61091753774681, 'f1': 41.21361139778582}
