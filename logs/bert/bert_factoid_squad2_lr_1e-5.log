2023-04-12 00:23:25 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1380cc367820a3f3/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'deepset/bert-base-uncased-squad2'}, 'data': {'task_type': 'factoid', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 1e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/bert_factoid_squad2'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 562.87it/s]
Map:   0%|          | 0/4429 [00:00<?, ? examples/s]Map:  23%|██▎       | 1000/4429 [00:00<00:02, 1316.79 examples/s]Map:  45%|████▌     | 2000/4429 [00:01<00:01, 1572.60 examples/s]Map:  68%|██████▊   | 3000/4429 [00:01<00:00, 1646.70 examples/s]Map:  90%|█████████ | 4000/4429 [00:02<00:00, 1690.96 examples/s]Map: 100%|██████████| 4429/4429 [00:02<00:00, 1690.25 examples/s]                                                                 Map:   0%|          | 0/553 [00:00<?, ? examples/s]Map: 100%|██████████| 553/553 [00:00<00:00, 1356.11 examples/s]                                                               Map:   0%|          | 0/555 [00:00<?, ? examples/s]Map: 100%|██████████| 555/555 [00:00<00:00, 1333.35 examples/s]                                                               2023-04-12 00:23:53 - training - INFO - First Test - Val Metrics:{'exact_match': 42.13381555153707, 'f1': 54.70506015651534} Test Metrics: {'exact_match': 41.26126126126126, 'f1': 55.9730411835675}
2023-04-12 00:23:53 - training - INFO - Epoch [1/5][1/415] lr: 1.0e-05, eta: 10:18:58.645128, loss: 4.3583
2023-04-12 00:23:57 - training - INFO - Epoch [1/5][11/415] lr: 9.9e-06, eta: 1:07:30.439008, loss: 2.8239
2023-04-12 00:24:01 - training - INFO - Epoch [1/5][21/415] lr: 9.9e-06, eta: 0:41:12.562066, loss: 2.2072
2023-04-12 00:24:04 - training - INFO - Epoch [1/5][31/415] lr: 9.9e-06, eta: 0:31:50.385764, loss: 2.1969
2023-04-12 00:24:08 - training - INFO - Epoch [1/5][41/415] lr: 9.8e-06, eta: 0:27:00.648486, loss: 2.0871
2023-04-12 00:24:12 - training - INFO - Epoch [1/5][51/415] lr: 9.8e-06, eta: 0:24:03.257728, loss: 3.2395
2023-04-12 00:24:15 - training - INFO - Epoch [1/5][61/415] lr: 9.7e-06, eta: 0:22:02.799228, loss: 1.8227
2023-04-12 00:24:19 - training - INFO - Epoch [1/5][71/415] lr: 9.7e-06, eta: 0:20:35.277624, loss: 2.1694
2023-04-12 00:24:23 - training - INFO - Epoch [1/5][81/415] lr: 9.6e-06, eta: 0:19:28.482006, loss: 2.5013
2023-04-12 00:24:26 - training - INFO - Epoch [1/5][91/415] lr: 9.6e-06, eta: 0:18:35.545664, loss: 1.3913
2023-04-12 00:24:30 - training - INFO - Epoch [1/5][101/415] lr: 9.5e-06, eta: 0:17:52.339968, loss: 2.3429
2023-04-12 00:24:34 - training - INFO - Epoch [1/5][111/415] lr: 9.5e-06, eta: 0:17:16.228004, loss: 2.0154
2023-04-12 00:24:38 - training - INFO - Epoch [1/5][121/415] lr: 9.4e-06, eta: 0:16:45.452194, loss: 1.8954
2023-04-12 00:24:41 - training - INFO - Epoch [1/5][131/415] lr: 9.4e-06, eta: 0:16:18.905088, loss: 2.2829
2023-04-12 00:24:45 - training - INFO - Epoch [1/5][141/415] lr: 9.3e-06, eta: 0:15:55.506238, loss: 1.7822
2023-04-12 00:24:49 - training - INFO - Epoch [1/5][151/415] lr: 9.3e-06, eta: 0:15:34.719604, loss: 1.6975
2023-04-12 00:24:52 - training - INFO - Epoch [1/5][161/415] lr: 9.2e-06, eta: 0:15:16.059540, loss: 2.0408
2023-04-12 00:24:56 - training - INFO - Epoch [1/5][171/415] lr: 9.2e-06, eta: 0:14:59.152576, loss: 2.0163
2023-04-12 00:25:00 - training - INFO - Epoch [1/5][181/415] lr: 9.1e-06, eta: 0:14:43.740400, loss: 1.4387
2023-04-12 00:25:03 - training - INFO - Epoch [1/5][191/415] lr: 9.1e-06, eta: 0:14:29.498028, loss: 1.6091
2023-04-12 00:25:07 - training - INFO - Epoch [1/5][201/415] lr: 9.0e-06, eta: 0:14:16.314930, loss: 1.2043
2023-04-12 00:25:11 - training - INFO - Epoch [1/5][211/415] lr: 9.0e-06, eta: 0:14:04.063936, loss: 1.1810
2023-04-12 00:25:15 - training - INFO - Epoch [1/5][221/415] lr: 8.9e-06, eta: 0:13:52.575780, loss: 1.9698
2023-04-12 00:25:18 - training - INFO - Epoch [1/5][231/415] lr: 8.9e-06, eta: 0:13:41.809948, loss: 1.8760
2023-04-12 00:25:22 - training - INFO - Epoch [1/5][241/415] lr: 8.8e-06, eta: 0:13:31.623862, loss: 2.0828
2023-04-12 00:25:26 - training - INFO - Epoch [1/5][251/415] lr: 8.8e-06, eta: 0:13:21.950784, loss: 1.4078
2023-04-12 00:25:29 - training - INFO - Epoch [1/5][261/415] lr: 8.7e-06, eta: 0:13:12.741582, loss: 1.7458
2023-04-12 00:25:33 - training - INFO - Epoch [1/5][271/415] lr: 8.7e-06, eta: 0:13:03.964280, loss: 1.8476
2023-04-12 00:25:37 - training - INFO - Epoch [1/5][281/415] lr: 8.6e-06, eta: 0:12:55.481616, loss: 1.8360
2023-04-12 00:25:40 - training - INFO - Epoch [1/5][291/415] lr: 8.6e-06, eta: 0:12:47.316240, loss: 1.6380
2023-04-12 00:25:44 - training - INFO - Epoch [1/5][301/415] lr: 8.5e-06, eta: 0:12:39.513264, loss: 1.6081
2023-04-12 00:25:48 - training - INFO - Epoch [1/5][311/415] lr: 8.5e-06, eta: 0:12:31.922640, loss: 2.3368
2023-04-12 00:25:52 - training - INFO - Epoch [1/5][321/415] lr: 8.5e-06, eta: 0:12:24.604572, loss: 1.0344
2023-04-12 00:25:55 - training - INFO - Epoch [1/5][331/415] lr: 8.4e-06, eta: 0:12:17.502720, loss: 1.3467
2023-04-12 00:25:59 - training - INFO - Epoch [1/5][341/415] lr: 8.4e-06, eta: 0:12:10.572348, loss: 1.9054
2023-04-12 00:26:03 - training - INFO - Epoch [1/5][351/415] lr: 8.3e-06, eta: 0:12:03.866224, loss: 1.3423
2023-04-12 00:26:06 - training - INFO - Epoch [1/5][361/415] lr: 8.3e-06, eta: 0:11:57.320998, loss: 1.3574
2023-04-12 00:26:10 - training - INFO - Epoch [1/5][371/415] lr: 8.2e-06, eta: 0:11:50.898576, loss: 1.6084
2023-04-12 00:26:14 - training - INFO - Epoch [1/5][381/415] lr: 8.2e-06, eta: 0:11:44.609136, loss: 1.5699
2023-04-12 00:26:17 - training - INFO - Epoch [1/5][391/415] lr: 8.1e-06, eta: 0:11:38.459208, loss: 1.2026
2023-04-12 00:26:21 - training - INFO - Epoch [1/5][401/415] lr: 8.1e-06, eta: 0:11:32.430012, loss: 2.4202
2023-04-12 00:26:25 - training - INFO - Epoch [1/5][411/415] lr: 8.0e-06, eta: 0:11:26.519808, loss: 1.7108
2023-04-12 00:26:44 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 1.8105, Validation Metrics: {'exact_match': 70.5244122965642, 'f1': 77.59380217885742}, Test Metrics: {'exact_match': 72.7927927927928, 'f1': 81.18271849850802}
2023-04-12 00:26:44 - training - INFO - Epoch [2/5][1/415] lr: 8.0e-06, eta: 4 days, 12:42:37.378896, loss: 1.5042
2023-04-12 00:26:48 - training - INFO - Epoch [2/5][11/415] lr: 7.9e-06, eta: 10:01:39.188688, loss: 0.8480
2023-04-12 00:26:51 - training - INFO - Epoch [2/5][21/415] lr: 7.9e-06, eta: 5:19:38.656042, loss: 1.2628
2023-04-12 00:26:55 - training - INFO - Epoch [2/5][31/415] lr: 7.9e-06, eta: 3:39:33.015856, loss: 1.5377
2023-04-12 00:26:59 - training - INFO - Epoch [2/5][41/415] lr: 7.8e-06, eta: 2:48:14.758272, loss: 1.2263
2023-04-12 00:27:02 - training - INFO - Epoch [2/5][51/415] lr: 7.8e-06, eta: 2:17:02.327960, loss: 1.3611
2023-04-12 00:27:06 - training - INFO - Epoch [2/5][61/415] lr: 7.7e-06, eta: 1:56:02.583288, loss: 1.0805
2023-04-12 00:27:10 - training - INFO - Epoch [2/5][71/415] lr: 7.7e-06, eta: 1:40:56.723268, loss: 0.9140
2023-04-12 00:27:14 - training - INFO - Epoch [2/5][81/415] lr: 7.6e-06, eta: 1:29:33.552834, loss: 1.1771
2023-04-12 00:27:17 - training - INFO - Epoch [2/5][91/415] lr: 7.6e-06, eta: 1:20:39.694208, loss: 1.3903
2023-04-12 00:27:21 - training - INFO - Epoch [2/5][101/415] lr: 7.5e-06, eta: 1:13:30.871416, loss: 0.9204
2023-04-12 00:27:25 - training - INFO - Epoch [2/5][111/415] lr: 7.5e-06, eta: 1:07:38.657064, loss: 0.9365
2023-04-12 00:27:28 - training - INFO - Epoch [2/5][121/415] lr: 7.4e-06, eta: 1:02:44.054682, loss: 1.4259
2023-04-12 00:27:32 - training - INFO - Epoch [2/5][131/415] lr: 7.4e-06, eta: 0:58:33.857760, loss: 1.2454
2023-04-12 00:27:36 - training - INFO - Epoch [2/5][141/415] lr: 7.3e-06, eta: 0:54:58.632334, loss: 1.6897
2023-04-12 00:27:40 - training - INFO - Epoch [2/5][151/415] lr: 7.3e-06, eta: 0:51:51.521660, loss: 0.7921
2023-04-12 00:27:43 - training - INFO - Epoch [2/5][161/415] lr: 7.2e-06, eta: 0:49:07.100640, loss: 1.1281
2023-04-12 00:27:47 - training - INFO - Epoch [2/5][171/415] lr: 7.2e-06, eta: 0:46:41.781696, loss: 0.6455
2023-04-12 00:27:51 - training - INFO - Epoch [2/5][181/415] lr: 7.1e-06, eta: 0:44:32.168840, loss: 1.1035
2023-04-12 00:27:54 - training - INFO - Epoch [2/5][191/415] lr: 7.1e-06, eta: 0:42:35.578176, loss: 1.4419
2023-04-12 00:27:58 - training - INFO - Epoch [2/5][201/415] lr: 7.0e-06, eta: 0:40:50.238134, loss: 1.1494
2023-04-12 00:28:02 - training - INFO - Epoch [2/5][211/415] lr: 7.0e-06, eta: 0:39:14.511600, loss: 1.3790
2023-04-12 00:28:06 - training - INFO - Epoch [2/5][221/415] lr: 6.9e-06, eta: 0:37:47.199126, loss: 1.4145
2023-04-12 00:28:09 - training - INFO - Epoch [2/5][231/415] lr: 6.9e-06, eta: 0:36:26.884424, loss: 0.8491
2023-04-12 00:28:13 - training - INFO - Epoch [2/5][241/415] lr: 6.8e-06, eta: 0:35:12.955068, loss: 0.7447
2023-04-12 00:28:17 - training - INFO - Epoch [2/5][251/415] lr: 6.8e-06, eta: 0:34:04.618272, loss: 1.0920
2023-04-12 00:28:20 - training - INFO - Epoch [2/5][261/415] lr: 6.7e-06, eta: 0:33:01.236288, loss: 1.2409
2023-04-12 00:28:24 - training - INFO - Epoch [2/5][271/415] lr: 6.7e-06, eta: 0:32:02.257612, loss: 1.6650
2023-04-12 00:28:28 - training - INFO - Epoch [2/5][281/415] lr: 6.6e-06, eta: 0:31:07.191612, loss: 1.0979
2023-04-12 00:28:31 - training - INFO - Epoch [2/5][291/415] lr: 6.6e-06, eta: 0:30:15.769472, loss: 1.0720
2023-04-12 00:28:35 - training - INFO - Epoch [2/5][301/415] lr: 6.5e-06, eta: 0:29:27.429104, loss: 0.7049
2023-04-12 00:28:39 - training - INFO - Epoch [2/5][311/415] lr: 6.5e-06, eta: 0:28:41.958588, loss: 0.5715
2023-04-12 00:28:43 - training - INFO - Epoch [2/5][321/415] lr: 6.5e-06, eta: 0:27:59.097184, loss: 0.9606
2023-04-12 00:28:46 - training - INFO - Epoch [2/5][331/415] lr: 6.4e-06, eta: 0:27:18.617056, loss: 1.0308
2023-04-12 00:28:50 - training - INFO - Epoch [2/5][341/415] lr: 6.4e-06, eta: 0:26:40.308600, loss: 0.8811
2023-04-12 00:28:54 - training - INFO - Epoch [2/5][351/415] lr: 6.3e-06, eta: 0:26:03.940392, loss: 0.8636
2023-04-12 00:28:57 - training - INFO - Epoch [2/5][361/415] lr: 6.3e-06, eta: 0:25:29.385060, loss: 0.8500
2023-04-12 00:29:01 - training - INFO - Epoch [2/5][371/415] lr: 6.2e-06, eta: 0:24:56.483472, loss: 1.0199
2023-04-12 00:29:05 - training - INFO - Epoch [2/5][381/415] lr: 6.2e-06, eta: 0:24:25.096556, loss: 1.7496
2023-04-12 00:29:09 - training - INFO - Epoch [2/5][391/415] lr: 6.1e-06, eta: 0:23:55.194052, loss: 0.9736
2023-04-12 00:29:12 - training - INFO - Epoch [2/5][401/415] lr: 6.1e-06, eta: 0:23:26.553390, loss: 0.8102
2023-04-12 00:29:16 - training - INFO - Epoch [2/5][411/415] lr: 6.0e-06, eta: 0:22:59.146496, loss: 1.5276
2023-04-12 00:29:35 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.2171, Validation Metrics: {'exact_match': 74.50271247739602, 'f1': 79.24927152079907}, Test Metrics: {'exact_match': 76.3963963963964, 'f1': 82.3719043719044}
2023-04-12 00:29:35 - training - INFO - Epoch [3/5][1/415] lr: 6.0e-06, eta: 8 days, 15:15:13.517174, loss: 0.6622
2023-04-12 00:29:39 - training - INFO - Epoch [3/5][11/415] lr: 5.9e-06, eta: 18:56:34.791168, loss: 0.9359
2023-04-12 00:29:42 - training - INFO - Epoch [3/5][21/415] lr: 5.9e-06, eta: 9:58:30.010110, loss: 0.8293
2023-04-12 00:29:46 - training - INFO - Epoch [3/5][31/415] lr: 5.9e-06, eta: 6:47:31.482860, loss: 0.9610
2023-04-12 00:29:50 - training - INFO - Epoch [3/5][41/415] lr: 5.8e-06, eta: 5:09:40.893066, loss: 1.4666
2023-04-12 00:29:54 - training - INFO - Epoch [3/5][51/415] lr: 5.8e-06, eta: 4:10:11.166016, loss: 1.1931
2023-04-12 00:29:57 - training - INFO - Epoch [3/5][61/415] lr: 5.7e-06, eta: 3:30:10.733504, loss: 1.1092
2023-04-12 00:30:01 - training - INFO - Epoch [3/5][71/415] lr: 5.7e-06, eta: 3:01:25.347240, loss: 1.1410
2023-04-12 00:30:05 - training - INFO - Epoch [3/5][81/415] lr: 5.6e-06, eta: 2:39:45.331478, loss: 1.0350
2023-04-12 00:30:08 - training - INFO - Epoch [3/5][91/415] lr: 5.6e-06, eta: 2:22:50.016960, loss: 1.0695
2023-04-12 00:30:12 - training - INFO - Epoch [3/5][101/415] lr: 5.5e-06, eta: 2:09:14.951778, loss: 1.4179
2023-04-12 00:30:16 - training - INFO - Epoch [3/5][111/415] lr: 5.5e-06, eta: 1:58:06.100216, loss: 1.3954
2023-04-12 00:30:20 - training - INFO - Epoch [3/5][121/415] lr: 5.4e-06, eta: 1:48:47.196312, loss: 0.7206
2023-04-12 00:30:23 - training - INFO - Epoch [3/5][131/415] lr: 5.4e-06, eta: 1:40:53.013360, loss: 0.8257
2023-04-12 00:30:27 - training - INFO - Epoch [3/5][141/415] lr: 5.3e-06, eta: 1:34:05.616760, loss: 1.0711
2023-04-12 00:30:31 - training - INFO - Epoch [3/5][151/415] lr: 5.3e-06, eta: 1:28:11.658008, loss: 0.7414
2023-04-12 00:30:34 - training - INFO - Epoch [3/5][161/415] lr: 5.2e-06, eta: 1:23:01.219452, loss: 0.9776
2023-04-12 00:30:38 - training - INFO - Epoch [3/5][171/415] lr: 5.2e-06, eta: 1:18:26.682288, loss: 1.3296
2023-04-12 00:30:42 - training - INFO - Epoch [3/5][181/415] lr: 5.1e-06, eta: 1:14:22.032932, loss: 0.7482
2023-04-12 00:30:45 - training - INFO - Epoch [3/5][191/415] lr: 5.1e-06, eta: 1:10:42.638004, loss: 0.8099
2023-04-12 00:30:49 - training - INFO - Epoch [3/5][201/415] lr: 5.0e-06, eta: 1:07:24.687932, loss: 0.9752
2023-04-12 00:30:53 - training - INFO - Epoch [3/5][211/415] lr: 5.0e-06, eta: 1:04:25.143800, loss: 0.6759
2023-04-12 00:30:57 - training - INFO - Epoch [3/5][221/415] lr: 4.9e-06, eta: 1:01:41.527686, loss: 0.9657
2023-04-12 00:31:00 - training - INFO - Epoch [3/5][231/415] lr: 4.9e-06, eta: 0:59:11.754216, loss: 1.4405
2023-04-12 00:31:04 - training - INFO - Epoch [3/5][241/415] lr: 4.8e-06, eta: 0:56:54.163396, loss: 0.8863
2023-04-12 00:31:08 - training - INFO - Epoch [3/5][251/415] lr: 4.8e-06, eta: 0:54:47.203680, loss: 0.7256
2023-04-12 00:31:11 - training - INFO - Epoch [3/5][261/415] lr: 4.7e-06, eta: 0:52:49.685644, loss: 0.7511
2023-04-12 00:31:15 - training - INFO - Epoch [3/5][271/415] lr: 4.7e-06, eta: 0:51:00.538316, loss: 0.8590
2023-04-12 00:31:19 - training - INFO - Epoch [3/5][281/415] lr: 4.6e-06, eta: 0:49:18.890844, loss: 0.6431
2023-04-12 00:31:22 - training - INFO - Epoch [3/5][291/415] lr: 4.6e-06, eta: 0:47:43.969376, loss: 0.8009
2023-04-12 00:31:26 - training - INFO - Epoch [3/5][301/415] lr: 4.5e-06, eta: 0:46:15.126742, loss: 1.2705
2023-04-12 00:31:30 - training - INFO - Epoch [3/5][311/415] lr: 4.5e-06, eta: 0:44:51.759924, loss: 0.7945
2023-04-12 00:31:34 - training - INFO - Epoch [3/5][321/415] lr: 4.5e-06, eta: 0:43:33.354760, loss: 1.6224
2023-04-12 00:31:37 - training - INFO - Epoch [3/5][331/415] lr: 4.4e-06, eta: 0:42:19.485488, loss: 1.6279
2023-04-12 00:31:41 - training - INFO - Epoch [3/5][341/415] lr: 4.4e-06, eta: 0:41:09.734466, loss: 1.0679
2023-04-12 00:31:45 - training - INFO - Epoch [3/5][351/415] lr: 4.3e-06, eta: 0:40:03.724928, loss: 1.1495
2023-04-12 00:31:48 - training - INFO - Epoch [3/5][361/415] lr: 4.3e-06, eta: 0:39:01.159456, loss: 1.0249
2023-04-12 00:31:52 - training - INFO - Epoch [3/5][371/415] lr: 4.2e-06, eta: 0:38:01.824696, loss: 1.4521
2023-04-12 00:31:56 - training - INFO - Epoch [3/5][381/415] lr: 4.2e-06, eta: 0:37:05.356980, loss: 1.1554
2023-04-12 00:32:00 - training - INFO - Epoch [3/5][391/415] lr: 4.1e-06, eta: 0:36:11.578624, loss: 0.5047
2023-04-12 00:32:03 - training - INFO - Epoch [3/5][401/415] lr: 4.1e-06, eta: 0:35:20.318532, loss: 1.3678
2023-04-12 00:32:07 - training - INFO - Epoch [3/5][411/415] lr: 4.0e-06, eta: 0:34:31.372160, loss: 1.2343
2023-04-12 00:32:26 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 1.0085, Validation Metrics: {'exact_match': 75.22603978300181, 'f1': 79.578888859458}, Test Metrics: {'exact_match': 76.93693693693693, 'f1': 81.95078195078199}
2023-04-12 00:32:26 - training - INFO - Epoch [4/5][1/415] lr: 4.0e-06, eta: 12 days, 17:41:26.203962, loss: 0.4777
2023-04-12 00:32:30 - training - INFO - Epoch [4/5][11/415] lr: 3.9e-06, eta: 1 day, 3:50:58.504848, loss: 1.2847
2023-04-12 00:32:33 - training - INFO - Epoch [4/5][21/415] lr: 3.9e-06, eta: 14:37:03.360868, loss: 0.2935
2023-04-12 00:32:37 - training - INFO - Epoch [4/5][31/415] lr: 3.9e-06, eta: 9:55:18.497332, loss: 1.9170
2023-04-12 00:32:41 - training - INFO - Epoch [4/5][41/415] lr: 3.8e-06, eta: 7:30:58.395564, loss: 1.1815
2023-04-12 00:32:44 - training - INFO - Epoch [4/5][51/415] lr: 3.8e-06, eta: 6:03:12.669096, loss: 0.5757
2023-04-12 00:32:48 - training - INFO - Epoch [4/5][61/415] lr: 3.7e-06, eta: 5:04:12.259674, loss: 0.6887
2023-04-12 00:32:52 - training - INFO - Epoch [4/5][71/415] lr: 3.7e-06, eta: 4:21:48.269832, loss: 1.0733
2023-04-12 00:32:56 - training - INFO - Epoch [4/5][81/415] lr: 3.6e-06, eta: 3:49:51.455138, loss: 0.8428
2023-04-12 00:32:59 - training - INFO - Epoch [4/5][91/415] lr: 3.6e-06, eta: 3:24:55.151552, loss: 0.7323
2023-04-12 00:33:03 - training - INFO - Epoch [4/5][101/415] lr: 3.5e-06, eta: 3:04:54.501810, loss: 0.8916
2023-04-12 00:33:07 - training - INFO - Epoch [4/5][111/415] lr: 3.5e-06, eta: 2:48:29.420932, loss: 1.6055
2023-04-12 00:33:10 - training - INFO - Epoch [4/5][121/415] lr: 3.4e-06, eta: 2:34:46.721088, loss: 1.1332
2023-04-12 00:33:14 - training - INFO - Epoch [4/5][131/415] lr: 3.4e-06, eta: 2:23:08.864160, loss: 0.7087
2023-04-12 00:33:18 - training - INFO - Epoch [4/5][141/415] lr: 3.3e-06, eta: 2:13:09.491314, loss: 1.3407
2023-04-12 00:33:21 - training - INFO - Epoch [4/5][151/415] lr: 3.3e-06, eta: 2:04:29.016100, loss: 1.1575
2023-04-12 00:33:25 - training - INFO - Epoch [4/5][161/415] lr: 3.2e-06, eta: 1:56:52.740966, loss: 0.7478
2023-04-12 00:33:29 - training - INFO - Epoch [4/5][171/415] lr: 3.2e-06, eta: 1:50:09.379952, loss: 1.1661
2023-04-12 00:33:33 - training - INFO - Epoch [4/5][181/415] lr: 3.1e-06, eta: 1:44:10.175378, loss: 0.5476
2023-04-12 00:33:36 - training - INFO - Epoch [4/5][191/415] lr: 3.1e-06, eta: 1:38:48.218892, loss: 0.5864
2023-04-12 00:33:40 - training - INFO - Epoch [4/5][201/415] lr: 3.0e-06, eta: 1:33:57.925252, loss: 0.8305
2023-04-12 00:33:44 - training - INFO - Epoch [4/5][211/415] lr: 3.0e-06, eta: 1:29:34.795536, loss: 0.5989
2023-04-12 00:33:47 - training - INFO - Epoch [4/5][221/415] lr: 2.9e-06, eta: 1:25:35.248134, loss: 0.8919
2023-04-12 00:33:51 - training - INFO - Epoch [4/5][231/415] lr: 2.9e-06, eta: 1:21:56.030240, loss: 0.9379
2023-04-12 00:33:55 - training - INFO - Epoch [4/5][241/415] lr: 2.8e-06, eta: 1:18:34.727990, loss: 0.3722
2023-04-12 00:33:59 - training - INFO - Epoch [4/5][251/415] lr: 2.8e-06, eta: 1:15:29.092320, loss: 0.8265
2023-04-12 00:34:02 - training - INFO - Epoch [4/5][261/415] lr: 2.7e-06, eta: 1:12:37.420284, loss: 0.7874
2023-04-12 00:34:06 - training - INFO - Epoch [4/5][271/415] lr: 2.7e-06, eta: 1:09:58.174992, loss: 1.1400
2023-04-12 00:34:10 - training - INFO - Epoch [4/5][281/415] lr: 2.6e-06, eta: 1:07:29.972940, loss: 1.5390
2023-04-12 00:34:13 - training - INFO - Epoch [4/5][291/415] lr: 2.6e-06, eta: 1:05:11.709008, loss: 1.4969
2023-04-12 00:34:17 - training - INFO - Epoch [4/5][301/415] lr: 2.5e-06, eta: 1:03:02.377332, loss: 0.5032
2023-04-12 00:34:21 - training - INFO - Epoch [4/5][311/415] lr: 2.5e-06, eta: 1:01:01.125552, loss: 0.5850
2023-04-12 00:34:24 - training - INFO - Epoch [4/5][321/415] lr: 2.5e-06, eta: 0:59:07.203654, loss: 1.1564
2023-04-12 00:34:28 - training - INFO - Epoch [4/5][331/415] lr: 2.4e-06, eta: 0:57:19.963264, loss: 1.0299
2023-04-12 00:34:32 - training - INFO - Epoch [4/5][341/415] lr: 2.4e-06, eta: 0:55:38.780586, loss: 0.7761
2023-04-12 00:34:36 - training - INFO - Epoch [4/5][351/415] lr: 2.3e-06, eta: 0:54:03.137080, loss: 0.7310
2023-04-12 00:34:39 - training - INFO - Epoch [4/5][361/415] lr: 2.3e-06, eta: 0:52:32.589338, loss: 0.8173
2023-04-12 00:34:43 - training - INFO - Epoch [4/5][371/415] lr: 2.2e-06, eta: 0:51:06.738216, loss: 0.8706
2023-04-12 00:34:47 - training - INFO - Epoch [4/5][381/415] lr: 2.2e-06, eta: 0:49:45.188822, loss: 0.5237
2023-04-12 00:34:50 - training - INFO - Epoch [4/5][391/415] lr: 2.1e-06, eta: 0:48:27.626396, loss: 1.0853
2023-04-12 00:34:54 - training - INFO - Epoch [4/5][401/415] lr: 2.1e-06, eta: 0:47:13.732134, loss: 1.3453
2023-04-12 00:34:58 - training - INFO - Epoch [4/5][411/415] lr: 2.0e-06, eta: 0:46:03.261696, loss: 0.4711
2023-04-12 00:35:16 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 0.9040, Validation Metrics: {'exact_match': 75.40687160940325, 'f1': 80.0271115862569}, Test Metrics: {'exact_match': 78.01801801801801, 'f1': 82.88008719587668}
2023-04-12 00:35:17 - training - INFO - Epoch [5/5][1/415] lr: 2.0e-06, eta: 16 days, 20:08:00.474868, loss: 0.8090
2023-04-12 00:35:20 - training - INFO - Epoch [5/5][11/415] lr: 1.9e-06, eta: 1 day, 12:45:17.425920, loss: 0.6886
2023-04-12 00:35:24 - training - INFO - Epoch [5/5][21/415] lr: 1.9e-06, eta: 19:15:35.224530, loss: 0.8287
2023-04-12 00:35:28 - training - INFO - Epoch [5/5][31/415] lr: 1.9e-06, eta: 13:03:04.444836, loss: 1.0347
2023-04-12 00:35:32 - training - INFO - Epoch [5/5][41/415] lr: 1.8e-06, eta: 9:52:15.129210, loss: 0.6642
2023-04-12 00:35:35 - training - INFO - Epoch [5/5][51/415] lr: 1.8e-06, eta: 7:56:14.170152, loss: 0.8063
2023-04-12 00:35:39 - training - INFO - Epoch [5/5][61/415] lr: 1.7e-06, eta: 6:38:14.273232, loss: 1.0818
2023-04-12 00:35:43 - training - INFO - Epoch [5/5][71/415] lr: 1.7e-06, eta: 5:42:11.491020, loss: 0.8355
2023-04-12 00:35:46 - training - INFO - Epoch [5/5][81/415] lr: 1.6e-06, eta: 4:59:58.438212, loss: 0.9115
2023-04-12 00:35:50 - training - INFO - Epoch [5/5][91/415] lr: 1.6e-06, eta: 4:27:01.004352, loss: 0.7926
2023-04-12 00:35:54 - training - INFO - Epoch [5/5][101/415] lr: 1.5e-06, eta: 4:00:34.446642, loss: 0.8052
2023-04-12 00:35:58 - training - INFO - Epoch [5/5][111/415] lr: 1.5e-06, eta: 3:38:53.048032, loss: 0.7450
2023-04-12 00:36:01 - training - INFO - Epoch [5/5][121/415] lr: 1.4e-06, eta: 3:20:46.154026, loss: 0.8978
2023-04-12 00:36:05 - training - INFO - Epoch [5/5][131/415] lr: 1.4e-06, eta: 3:05:24.658584, loss: 0.9806
2023-04-12 00:36:09 - training - INFO - Epoch [5/5][141/415] lr: 1.3e-06, eta: 2:52:13.340726, loss: 0.9911
2023-04-12 00:36:12 - training - INFO - Epoch [5/5][151/415] lr: 1.3e-06, eta: 2:40:46.287612, loss: 0.9557
2023-04-12 00:36:16 - training - INFO - Epoch [5/5][161/415] lr: 1.2e-06, eta: 2:30:44.193576, loss: 0.8134
2023-04-12 00:36:20 - training - INFO - Epoch [5/5][171/415] lr: 1.2e-06, eta: 2:21:52.125216, loss: 1.1085
2023-04-12 00:36:23 - training - INFO - Epoch [5/5][181/415] lr: 1.1e-06, eta: 2:13:58.372750, loss: 0.8510
2023-04-12 00:36:27 - training - INFO - Epoch [5/5][191/415] lr: 1.1e-06, eta: 2:06:53.841228, loss: 1.3167
2023-04-12 00:36:31 - training - INFO - Epoch [5/5][201/415] lr: 1.0e-06, eta: 2:00:31.153202, loss: 0.8032
2023-04-12 00:36:35 - training - INFO - Epoch [5/5][211/415] lr: 9.8e-07, eta: 1:54:44.400672, loss: 0.6604
2023-04-12 00:36:38 - training - INFO - Epoch [5/5][221/415] lr: 9.3e-07, eta: 1:49:28.699752, loss: 0.9273
2023-04-12 00:36:42 - training - INFO - Epoch [5/5][231/415] lr: 8.9e-07, eta: 1:44:40.033352, loss: 0.4593
2023-04-12 00:36:46 - training - INFO - Epoch [5/5][241/415] lr: 8.4e-07, eta: 1:40:15.061500, loss: 0.8145
2023-04-12 00:36:49 - training - INFO - Epoch [5/5][251/415] lr: 7.9e-07, eta: 1:36:10.831392, loss: 0.7249
2023-04-12 00:36:53 - training - INFO - Epoch [5/5][261/415] lr: 7.4e-07, eta: 1:32:25.013432, loss: 0.8296
2023-04-12 00:36:57 - training - INFO - Epoch [5/5][271/415] lr: 6.9e-07, eta: 1:28:55.609620, loss: 0.5741
2023-04-12 00:37:01 - training - INFO - Epoch [5/5][281/415] lr: 6.5e-07, eta: 1:25:40.846932, loss: 0.6588
2023-04-12 00:37:04 - training - INFO - Epoch [5/5][291/415] lr: 6.0e-07, eta: 1:22:39.288080, loss: 0.8129
2023-04-12 00:37:08 - training - INFO - Epoch [5/5][301/415] lr: 5.5e-07, eta: 1:19:49.464714, loss: 0.7706
2023-04-12 00:37:12 - training - INFO - Epoch [5/5][311/415] lr: 5.0e-07, eta: 1:17:10.350060, loss: 0.5743
2023-04-12 00:37:15 - training - INFO - Epoch [5/5][321/415] lr: 4.5e-07, eta: 1:14:40.917490, loss: 0.8258
2023-04-12 00:37:19 - training - INFO - Epoch [5/5][331/415] lr: 4.0e-07, eta: 1:12:20.315472, loss: 0.6192
2023-04-12 00:37:23 - training - INFO - Epoch [5/5][341/415] lr: 3.6e-07, eta: 1:10:07.707060, loss: 0.7637
2023-04-12 00:37:26 - training - INFO - Epoch [5/5][351/415] lr: 3.1e-07, eta: 1:08:02.450964, loss: 0.7559
2023-04-12 00:37:30 - training - INFO - Epoch [5/5][361/415] lr: 2.6e-07, eta: 1:06:03.945518, loss: 0.8615
2023-04-12 00:37:34 - training - INFO - Epoch [5/5][371/415] lr: 2.1e-07, eta: 1:04:11.619360, loss: 0.3673
2023-04-12 00:37:38 - training - INFO - Epoch [5/5][381/415] lr: 1.6e-07, eta: 1:02:24.985090, loss: 0.6225
2023-04-12 00:37:41 - training - INFO - Epoch [5/5][391/415] lr: 1.2e-07, eta: 1:00:43.608492, loss: 1.2913
2023-04-12 00:37:45 - training - INFO - Epoch [5/5][401/415] lr: 6.7e-08, eta: 0:59:07.190934, loss: 1.1098
2023-04-12 00:37:49 - training - INFO - Epoch [5/5][411/415] lr: 1.9e-08, eta: 0:57:35.226112, loss: 1.1145
2023-04-12 00:38:07 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 0.8357, Validation Metrics: {'exact_match': 75.22603978300181, 'f1': 79.67000272005886}, Test Metrics: {'exact_match': 79.63963963963964, 'f1': 83.81880581880581}
2023-04-12 00:38:16 - training - INFO - Final Test - Train Loss: 0.8357, Test Metrics: {'exact_match': 79.63963963963964, 'f1': 83.81880581880581}
