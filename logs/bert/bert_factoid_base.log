2023-04-10 22:01:57 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1380cc367820a3f3/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'bert-base-uncased'}, 'data': {'task_type': 'factoid', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 4.54e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/bert_factoid_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 175.99it/s]
Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 38.0kB/s]
Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]Downloading: 100%|██████████| 570/570 [00:00<00:00, 1.03MB/s]
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading: 100%|██████████| 232k/232k [00:00<00:00, 19.6MB/s]
Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]Downloading: 100%|██████████| 466k/466k [00:00<00:00, 16.7MB/s]
Map:   0%|          | 0/4429 [00:00<?, ? examples/s]Map:  23%|██▎       | 1000/4429 [00:00<00:02, 1364.54 examples/s]Map:  45%|████▌     | 2000/4429 [00:01<00:01, 1616.22 examples/s]Map:  68%|██████▊   | 3000/4429 [00:01<00:00, 1685.75 examples/s]Map:  90%|█████████ | 4000/4429 [00:02<00:00, 1725.26 examples/s]Map: 100%|██████████| 4429/4429 [00:02<00:00, 1731.69 examples/s]                                                                 Map:   0%|          | 0/553 [00:00<?, ? examples/s]Map: 100%|██████████| 553/553 [00:00<00:00, 1346.20 examples/s]                                                               Map:   0%|          | 0/555 [00:00<?, ? examples/s]Map: 100%|██████████| 555/555 [00:00<00:00, 1354.40 examples/s]                                                               Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]Downloading:   2%|▏         | 10.1M/440M [00:00<00:04, 101MB/s]Downloading:   5%|▍         | 21.8M/440M [00:00<00:03, 111MB/s]Downloading:   8%|▊         | 33.6M/440M [00:00<00:03, 114MB/s]Downloading:  10%|█         | 45.4M/440M [00:00<00:03, 115MB/s]Downloading:  13%|█▎        | 57.1M/440M [00:00<00:03, 116MB/s]Downloading:  16%|█▌        | 68.8M/440M [00:00<00:03, 116MB/s]Downloading:  18%|█▊        | 80.6M/440M [00:00<00:03, 117MB/s]Downloading:  21%|██        | 92.3M/440M [00:00<00:02, 117MB/s]Downloading:  24%|██▎       | 104M/440M [00:00<00:02, 117MB/s] Downloading:  26%|██▋       | 116M/440M [00:01<00:02, 117MB/s]Downloading:  29%|██▉       | 127M/440M [00:01<00:02, 117MB/s]Downloading:  32%|███▏      | 139M/440M [00:01<00:02, 117MB/s]Downloading:  34%|███▍      | 151M/440M [00:01<00:02, 117MB/s]Downloading:  37%|███▋      | 163M/440M [00:01<00:02, 117MB/s]Downloading:  40%|███▉      | 175M/440M [00:01<00:02, 117MB/s]Downloading:  42%|████▏     | 186M/440M [00:01<00:02, 117MB/s]Downloading:  45%|████▍     | 198M/440M [00:01<00:02, 117MB/s]Downloading:  48%|████▊     | 210M/440M [00:01<00:01, 117MB/s]Downloading:  50%|█████     | 222M/440M [00:01<00:01, 115MB/s]Downloading:  53%|█████▎    | 233M/440M [00:02<00:01, 115MB/s]Downloading:  56%|█████▌    | 245M/440M [00:02<00:01, 116MB/s]Downloading:  58%|█████▊    | 257M/440M [00:02<00:01, 116MB/s]Downloading:  61%|██████    | 269M/440M [00:02<00:01, 117MB/s]Downloading:  64%|██████▎   | 280M/440M [00:02<00:01, 117MB/s]Downloading:  66%|██████▋   | 292M/440M [00:02<00:01, 117MB/s]Downloading:  69%|██████▉   | 304M/440M [00:02<00:01, 117MB/s]Downloading:  72%|███████▏  | 316M/440M [00:02<00:01, 117MB/s]Downloading:  74%|███████▍  | 327M/440M [00:02<00:00, 117MB/s]Downloading:  77%|███████▋  | 339M/440M [00:02<00:00, 117MB/s]Downloading:  80%|███████▉  | 351M/440M [00:03<00:00, 117MB/s]Downloading:  82%|████████▏ | 363M/440M [00:03<00:00, 116MB/s]Downloading:  85%|████████▍ | 374M/440M [00:03<00:00, 117MB/s]Downloading:  88%|████████▊ | 386M/440M [00:03<00:00, 117MB/s]Downloading:  90%|█████████ | 398M/440M [00:03<00:00, 117MB/s]Downloading:  93%|█████████▎| 410M/440M [00:03<00:00, 117MB/s]Downloading:  96%|█████████▌| 421M/440M [00:03<00:00, 118MB/s]Downloading:  98%|█████████▊| 433M/440M [00:03<00:00, 118MB/s]Downloading: 100%|██████████| 440M/440M [00:03<00:00, 117MB/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-10 22:02:33 - training - INFO - First Test - Val Metrics:{'exact_match': 0.0, 'f1': 2.7729340999330634} Test Metrics: {'exact_match': 0.0, 'f1': 3.7102255713778094}
2023-04-10 22:02:34 - training - INFO - Epoch [1/5][1/415] lr: 4.5e-05, eta: 10:11:43.538594, loss: 5.9590
2023-04-10 22:02:37 - training - INFO - Epoch [1/5][11/415] lr: 4.5e-05, eta: 1:06:40.446864, loss: 4.1307
2023-04-10 22:02:41 - training - INFO - Epoch [1/5][21/415] lr: 4.5e-05, eta: 0:40:40.082164, loss: 2.9279
2023-04-10 22:02:44 - training - INFO - Epoch [1/5][31/415] lr: 4.5e-05, eta: 0:31:24.435140, loss: 3.7959
2023-04-10 22:02:48 - training - INFO - Epoch [1/5][41/415] lr: 4.5e-05, eta: 0:26:37.822938, loss: 2.6557
2023-04-10 22:02:52 - training - INFO - Epoch [1/5][51/415] lr: 4.4e-05, eta: 0:23:42.459104, loss: 2.1414
2023-04-10 22:02:55 - training - INFO - Epoch [1/5][61/415] lr: 4.4e-05, eta: 0:21:43.509136, loss: 1.7881
2023-04-10 22:02:59 - training - INFO - Epoch [1/5][71/415] lr: 4.4e-05, eta: 0:20:17.043228, loss: 2.9472
2023-04-10 22:03:03 - training - INFO - Epoch [1/5][81/415] lr: 4.4e-05, eta: 0:19:11.066410, loss: 2.9034
2023-04-10 22:03:06 - training - INFO - Epoch [1/5][91/415] lr: 4.3e-05, eta: 0:18:18.836416, loss: 2.1795
2023-04-10 22:03:10 - training - INFO - Epoch [1/5][101/415] lr: 4.3e-05, eta: 0:17:36.255816, loss: 2.4879
2023-04-10 22:03:14 - training - INFO - Epoch [1/5][111/415] lr: 4.3e-05, eta: 0:17:00.698656, loss: 1.6207
2023-04-10 22:03:17 - training - INFO - Epoch [1/5][121/415] lr: 4.3e-05, eta: 0:16:30.416164, loss: 2.6819
2023-04-10 22:03:21 - training - INFO - Epoch [1/5][131/415] lr: 4.3e-05, eta: 0:16:04.183176, loss: 2.7167
2023-04-10 22:03:25 - training - INFO - Epoch [1/5][141/415] lr: 4.2e-05, eta: 0:15:41.154024, loss: 1.8820
2023-04-10 22:03:28 - training - INFO - Epoch [1/5][151/415] lr: 4.2e-05, eta: 0:15:20.776376, loss: 1.8041
2023-04-10 22:03:32 - training - INFO - Epoch [1/5][161/415] lr: 4.2e-05, eta: 0:15:02.456742, loss: 1.9479
2023-04-10 22:03:35 - training - INFO - Epoch [1/5][171/415] lr: 4.2e-05, eta: 0:14:45.936912, loss: 1.7655
2023-04-10 22:03:39 - training - INFO - Epoch [1/5][181/415] lr: 4.1e-05, eta: 0:14:30.810062, loss: 2.2246
2023-04-10 22:03:43 - training - INFO - Epoch [1/5][191/415] lr: 4.1e-05, eta: 0:14:16.897836, loss: 1.3100
2023-04-10 22:03:46 - training - INFO - Epoch [1/5][201/415] lr: 4.1e-05, eta: 0:14:04.010246, loss: 2.0848
2023-04-10 22:03:50 - training - INFO - Epoch [1/5][211/415] lr: 4.1e-05, eta: 0:13:52.022496, loss: 1.5924
2023-04-10 22:03:54 - training - INFO - Epoch [1/5][221/415] lr: 4.1e-05, eta: 0:13:40.871478, loss: 2.2198
2023-04-10 22:03:57 - training - INFO - Epoch [1/5][231/415] lr: 4.0e-05, eta: 0:13:30.378992, loss: 1.6792
2023-04-10 22:04:01 - training - INFO - Epoch [1/5][241/415] lr: 4.0e-05, eta: 0:13:20.434628, loss: 1.2858
2023-04-10 22:04:05 - training - INFO - Epoch [1/5][251/415] lr: 4.0e-05, eta: 0:13:10.997664, loss: 1.8641
2023-04-10 22:04:08 - training - INFO - Epoch [1/5][261/415] lr: 4.0e-05, eta: 0:13:02.031726, loss: 1.4682
2023-04-10 22:04:12 - training - INFO - Epoch [1/5][271/415] lr: 3.9e-05, eta: 0:12:53.461392, loss: 1.9652
2023-04-10 22:04:16 - training - INFO - Epoch [1/5][281/415] lr: 3.9e-05, eta: 0:12:45.245052, loss: 1.5216
2023-04-10 22:04:19 - training - INFO - Epoch [1/5][291/415] lr: 3.9e-05, eta: 0:12:37.349032, loss: 1.3340
2023-04-10 22:04:23 - training - INFO - Epoch [1/5][301/415] lr: 3.9e-05, eta: 0:12:29.703044, loss: 1.3559
2023-04-10 22:04:27 - training - INFO - Epoch [1/5][311/415] lr: 3.9e-05, eta: 0:12:22.326480, loss: 0.8771
2023-04-10 22:04:30 - training - INFO - Epoch [1/5][321/415] lr: 3.8e-05, eta: 0:12:15.176822, loss: 1.3667
2023-04-10 22:04:34 - training - INFO - Epoch [1/5][331/415] lr: 3.8e-05, eta: 0:12:08.222896, loss: 1.2000
2023-04-10 22:04:38 - training - INFO - Epoch [1/5][341/415] lr: 3.8e-05, eta: 0:12:01.487922, loss: 1.2645
2023-04-10 22:04:41 - training - INFO - Epoch [1/5][351/415] lr: 3.8e-05, eta: 0:11:54.934180, loss: 1.6361
2023-04-10 22:04:45 - training - INFO - Epoch [1/5][361/415] lr: 3.8e-05, eta: 0:11:48.529892, loss: 1.3906
2023-04-10 22:04:49 - training - INFO - Epoch [1/5][371/415] lr: 3.7e-05, eta: 0:11:42.279744, loss: 0.6958
2023-04-10 22:04:52 - training - INFO - Epoch [1/5][381/415] lr: 3.7e-05, eta: 0:11:36.181486, loss: 1.2691
2023-04-10 22:04:56 - training - INFO - Epoch [1/5][391/415] lr: 3.7e-05, eta: 0:11:30.189084, loss: 1.5310
2023-04-10 22:05:00 - training - INFO - Epoch [1/5][401/415] lr: 3.7e-05, eta: 0:11:24.302742, loss: 1.1337
2023-04-10 22:05:03 - training - INFO - Epoch [1/5][411/415] lr: 3.6e-05, eta: 0:11:18.520960, loss: 1.6114
2023-04-10 22:05:13 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 1.9467, Validation Metrics: {'exact_match': 75.22603978300181, 'f1': 78.82853294039323}
2023-04-10 22:05:14 - training - INFO - Epoch [2/5][1/415] lr: 3.6e-05, eta: 4 days, 6:22:54.179870, loss: 0.9805
2023-04-10 22:05:17 - training - INFO - Epoch [2/5][11/415] lr: 3.6e-05, eta: 9:27:14.179392, loss: 0.6561
2023-04-10 22:05:21 - training - INFO - Epoch [2/5][21/415] lr: 3.6e-05, eta: 5:01:40.112966, loss: 0.7858
2023-04-10 22:05:25 - training - INFO - Epoch [2/5][31/415] lr: 3.6e-05, eta: 3:27:24.209260, loss: 1.4434
2023-04-10 22:05:28 - training - INFO - Epoch [2/5][41/415] lr: 3.5e-05, eta: 2:39:05.258934, loss: 0.9030
2023-04-10 22:05:32 - training - INFO - Epoch [2/5][51/415] lr: 3.5e-05, eta: 2:09:41.885320, loss: 0.5862
2023-04-10 22:05:36 - training - INFO - Epoch [2/5][61/415] lr: 3.5e-05, eta: 1:49:55.316290, loss: 1.5352
2023-04-10 22:05:39 - training - INFO - Epoch [2/5][71/415] lr: 3.5e-05, eta: 1:35:41.940960, loss: 1.1449
2023-04-10 22:05:43 - training - INFO - Epoch [2/5][81/415] lr: 3.5e-05, eta: 1:24:58.462588, loss: 1.2274
2023-04-10 22:05:47 - training - INFO - Epoch [2/5][91/415] lr: 3.4e-05, eta: 1:16:35.576896, loss: 1.7585
2023-04-10 22:05:50 - training - INFO - Epoch [2/5][101/415] lr: 3.4e-05, eta: 1:09:51.583704, loss: 1.4313
2023-04-10 22:05:54 - training - INFO - Epoch [2/5][111/415] lr: 3.4e-05, eta: 1:04:19.762784, loss: 1.0518
2023-04-10 22:05:58 - training - INFO - Epoch [2/5][121/415] lr: 3.4e-05, eta: 0:59:42.176362, loss: 0.9514
2023-04-10 22:06:01 - training - INFO - Epoch [2/5][131/415] lr: 3.3e-05, eta: 0:55:46.347168, loss: 1.8667
2023-04-10 22:06:05 - training - INFO - Epoch [2/5][141/415] lr: 3.3e-05, eta: 0:52:23.465580, loss: 1.7479
2023-04-10 22:06:09 - training - INFO - Epoch [2/5][151/415] lr: 3.3e-05, eta: 0:49:26.950376, loss: 0.7816
2023-04-10 22:06:12 - training - INFO - Epoch [2/5][161/415] lr: 3.3e-05, eta: 0:46:51.970326, loss: 1.1433
2023-04-10 22:06:16 - training - INFO - Epoch [2/5][171/415] lr: 3.3e-05, eta: 0:44:34.638288, loss: 1.2550
2023-04-10 22:06:20 - training - INFO - Epoch [2/5][181/415] lr: 3.2e-05, eta: 0:42:32.111968, loss: 1.1862
2023-04-10 22:06:23 - training - INFO - Epoch [2/5][191/415] lr: 3.2e-05, eta: 0:40:41.997468, loss: 0.8735
2023-04-10 22:06:27 - training - INFO - Epoch [2/5][201/415] lr: 3.2e-05, eta: 0:39:02.447528, loss: 1.8744
2023-04-10 22:06:31 - training - INFO - Epoch [2/5][211/415] lr: 3.2e-05, eta: 0:37:32.015832, loss: 0.8413
2023-04-10 22:06:34 - training - INFO - Epoch [2/5][221/415] lr: 3.1e-05, eta: 0:36:09.478494, loss: 0.3848
2023-04-10 22:06:38 - training - INFO - Epoch [2/5][231/415] lr: 3.1e-05, eta: 0:34:53.791928, loss: 1.5943
2023-04-10 22:06:42 - training - INFO - Epoch [2/5][241/415] lr: 3.1e-05, eta: 0:33:44.075760, loss: 1.1699
2023-04-10 22:06:46 - training - INFO - Epoch [2/5][251/415] lr: 3.1e-05, eta: 0:32:39.625344, loss: 1.1082
2023-04-10 22:06:49 - training - INFO - Epoch [2/5][261/415] lr: 3.1e-05, eta: 0:31:39.791316, loss: 0.8219
2023-04-10 22:06:53 - training - INFO - Epoch [2/5][271/415] lr: 3.0e-05, eta: 0:30:44.119156, loss: 1.0111
2023-04-10 22:06:57 - training - INFO - Epoch [2/5][281/415] lr: 3.0e-05, eta: 0:29:52.168326, loss: 1.3826
2023-04-10 22:07:00 - training - INFO - Epoch [2/5][291/415] lr: 3.0e-05, eta: 0:29:03.544232, loss: 0.7028
2023-04-10 22:07:04 - training - INFO - Epoch [2/5][301/415] lr: 3.0e-05, eta: 0:28:17.918462, loss: 1.3499
2023-04-10 22:07:08 - training - INFO - Epoch [2/5][311/415] lr: 3.0e-05, eta: 0:27:34.986564, loss: 0.9159
2023-04-10 22:07:11 - training - INFO - Epoch [2/5][321/415] lr: 2.9e-05, eta: 0:26:54.499118, loss: 1.0367
2023-04-10 22:07:15 - training - INFO - Epoch [2/5][331/415] lr: 2.9e-05, eta: 0:26:16.228944, loss: 1.7041
2023-04-10 22:07:19 - training - INFO - Epoch [2/5][341/415] lr: 2.9e-05, eta: 0:25:39.970602, loss: 1.0483
2023-04-10 22:07:22 - training - INFO - Epoch [2/5][351/415] lr: 2.9e-05, eta: 0:25:05.584716, loss: 0.9722
2023-04-10 22:07:26 - training - INFO - Epoch [2/5][361/415] lr: 2.8e-05, eta: 0:24:32.900190, loss: 0.9594
2023-04-10 22:07:30 - training - INFO - Epoch [2/5][371/415] lr: 2.8e-05, eta: 0:24:01.771440, loss: 1.0927
2023-04-10 22:07:33 - training - INFO - Epoch [2/5][381/415] lr: 2.8e-05, eta: 0:23:32.113318, loss: 1.2584
2023-04-10 22:07:37 - training - INFO - Epoch [2/5][391/415] lr: 2.8e-05, eta: 0:23:03.806792, loss: 0.7658
2023-04-10 22:07:41 - training - INFO - Epoch [2/5][401/415] lr: 2.8e-05, eta: 0:22:36.686604, loss: 1.3799
2023-04-10 22:07:45 - training - INFO - Epoch [2/5][411/415] lr: 2.7e-05, eta: 0:22:10.707456, loss: 0.7718
2023-04-10 22:07:54 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.0420, Validation Metrics: {'exact_match': 81.0126582278481, 'f1': 83.30588861376307}
2023-04-10 22:07:55 - training - INFO - Epoch [3/5][1/415] lr: 2.7e-05, eta: 8 days, 3:17:10.460354, loss: 0.8064
2023-04-10 22:07:59 - training - INFO - Epoch [3/5][11/415] lr: 2.7e-05, eta: 17:51:34.262544, loss: 0.3378
2023-04-10 22:08:02 - training - INFO - Epoch [3/5][21/415] lr: 2.7e-05, eta: 9:24:35.576514, loss: 1.2774
2023-04-10 22:08:06 - training - INFO - Epoch [3/5][31/415] lr: 2.7e-05, eta: 6:24:39.742196, loss: 1.2463
2023-04-10 22:08:10 - training - INFO - Epoch [3/5][41/415] lr: 2.6e-05, eta: 4:52:28.276014, loss: 0.6108
2023-04-10 22:08:13 - training - INFO - Epoch [3/5][51/415] lr: 2.6e-05, eta: 3:56:24.532032, loss: 0.5819
2023-04-10 22:08:17 - training - INFO - Epoch [3/5][61/415] lr: 2.6e-05, eta: 3:18:42.338234, loss: 0.5780
2023-04-10 22:08:21 - training - INFO - Epoch [3/5][71/415] lr: 2.6e-05, eta: 2:51:36.473844, loss: 0.9865
2023-04-10 22:08:24 - training - INFO - Epoch [3/5][81/415] lr: 2.5e-05, eta: 2:31:11.170602, loss: 0.6380
2023-04-10 22:08:28 - training - INFO - Epoch [3/5][91/415] lr: 2.5e-05, eta: 2:15:14.306048, loss: 0.4375
2023-04-10 22:08:32 - training - INFO - Epoch [3/5][101/415] lr: 2.5e-05, eta: 2:02:26.183754, loss: 0.9727
2023-04-10 22:08:35 - training - INFO - Epoch [3/5][111/415] lr: 2.5e-05, eta: 1:51:55.776232, loss: 1.2732
2023-04-10 22:08:39 - training - INFO - Epoch [3/5][121/415] lr: 2.5e-05, eta: 1:43:08.898338, loss: 0.7757
2023-04-10 22:08:43 - training - INFO - Epoch [3/5][131/415] lr: 2.4e-05, eta: 1:35:42.016128, loss: 0.6848
2023-04-10 22:08:47 - training - INFO - Epoch [3/5][141/415] lr: 2.4e-05, eta: 1:29:17.984544, loss: 0.5079
2023-04-10 22:08:50 - training - INFO - Epoch [3/5][151/415] lr: 2.4e-05, eta: 1:23:44.252792, loss: 0.6506
2023-04-10 22:08:54 - training - INFO - Epoch [3/5][161/415] lr: 2.4e-05, eta: 1:18:51.568776, loss: 0.5617
2023-04-10 22:08:58 - training - INFO - Epoch [3/5][171/415] lr: 2.3e-05, eta: 1:14:32.694016, loss: 0.6528
2023-04-10 22:09:01 - training - INFO - Epoch [3/5][181/415] lr: 2.3e-05, eta: 1:10:42.016422, loss: 0.6560
2023-04-10 22:09:05 - training - INFO - Epoch [3/5][191/415] lr: 2.3e-05, eta: 1:07:15.113520, loss: 0.7425
2023-04-10 22:09:09 - training - INFO - Epoch [3/5][201/415] lr: 2.3e-05, eta: 1:04:08.446400, loss: 1.0474
2023-04-10 22:09:12 - training - INFO - Epoch [3/5][211/415] lr: 2.3e-05, eta: 1:01:19.155744, loss: 1.3584
2023-04-10 22:09:16 - training - INFO - Epoch [3/5][221/415] lr: 2.2e-05, eta: 0:58:44.839632, loss: 1.1794
2023-04-10 22:09:20 - training - INFO - Epoch [3/5][231/415] lr: 2.2e-05, eta: 0:56:23.585104, loss: 0.9608
2023-04-10 22:09:23 - training - INFO - Epoch [3/5][241/415] lr: 2.2e-05, eta: 0:54:13.690230, loss: 1.0848
2023-04-10 22:09:27 - training - INFO - Epoch [3/5][251/415] lr: 2.2e-05, eta: 0:52:13.854528, loss: 0.6256
2023-04-10 22:09:31 - training - INFO - Epoch [3/5][261/415] lr: 2.2e-05, eta: 0:50:22.904020, loss: 1.0074
2023-04-10 22:09:35 - training - INFO - Epoch [3/5][271/415] lr: 2.1e-05, eta: 0:48:39.880436, loss: 1.0486
2023-04-10 22:09:38 - training - INFO - Epoch [3/5][281/415] lr: 2.1e-05, eta: 0:47:03.967692, loss: 0.6200
2023-04-10 22:09:42 - training - INFO - Epoch [3/5][291/415] lr: 2.1e-05, eta: 0:45:34.351072, loss: 1.0499
2023-04-10 22:09:46 - training - INFO - Epoch [3/5][301/415] lr: 2.1e-05, eta: 0:44:10.441152, loss: 1.1973
2023-04-10 22:09:49 - training - INFO - Epoch [3/5][311/415] lr: 2.0e-05, eta: 0:42:51.691500, loss: 0.3920
2023-04-10 22:09:53 - training - INFO - Epoch [3/5][321/415] lr: 2.0e-05, eta: 0:41:37.624086, loss: 0.9212
2023-04-10 22:09:57 - training - INFO - Epoch [3/5][331/415] lr: 2.0e-05, eta: 0:40:27.806704, loss: 0.5662
2023-04-10 22:10:00 - training - INFO - Epoch [3/5][341/415] lr: 2.0e-05, eta: 0:39:21.872730, loss: 0.5113
2023-04-10 22:10:04 - training - INFO - Epoch [3/5][351/415] lr: 2.0e-05, eta: 0:38:19.481544, loss: 0.7705
2023-04-10 22:10:08 - training - INFO - Epoch [3/5][361/415] lr: 1.9e-05, eta: 0:37:20.365972, loss: 0.8162
2023-04-10 22:10:11 - training - INFO - Epoch [3/5][371/415] lr: 1.9e-05, eta: 0:36:24.228096, loss: 0.6944
2023-04-10 22:10:15 - training - INFO - Epoch [3/5][381/415] lr: 1.9e-05, eta: 0:35:30.869048, loss: 1.0398
2023-04-10 22:10:19 - training - INFO - Epoch [3/5][391/415] lr: 1.9e-05, eta: 0:34:40.046488, loss: 0.5002
2023-04-10 22:10:23 - training - INFO - Epoch [3/5][401/415] lr: 1.8e-05, eta: 0:33:51.558030, loss: 0.6267
2023-04-10 22:10:26 - training - INFO - Epoch [3/5][411/415] lr: 1.8e-05, eta: 0:33:05.271808, loss: 1.1094
2023-04-10 22:10:36 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 0.8082, Validation Metrics: {'exact_match': 79.20433996383363, 'f1': 81.8404185226066}
2023-04-10 22:10:36 - training - INFO - Epoch [4/5][1/415] lr: 1.8e-05, eta: 12 days, 0:21:11.376550, loss: 0.5023
2023-04-10 22:10:40 - training - INFO - Epoch [4/5][11/415] lr: 1.8e-05, eta: 1 day, 2:16:47.431776, loss: 0.6206
2023-04-10 22:10:44 - training - INFO - Epoch [4/5][21/415] lr: 1.8e-05, eta: 13:47:57.501744, loss: 0.4974
2023-04-10 22:10:47 - training - INFO - Epoch [4/5][31/415] lr: 1.7e-05, eta: 9:22:11.972568, loss: 0.7149
2023-04-10 22:10:51 - training - INFO - Epoch [4/5][41/415] lr: 1.7e-05, eta: 7:06:03.171654, loss: 0.4941
2023-04-10 22:10:55 - training - INFO - Epoch [4/5][51/415] lr: 1.7e-05, eta: 5:43:16.379848, loss: 0.6696
2023-04-10 22:10:59 - training - INFO - Epoch [4/5][61/415] lr: 1.7e-05, eta: 4:47:36.912678, loss: 0.5469
2023-04-10 22:11:02 - training - INFO - Epoch [4/5][71/415] lr: 1.7e-05, eta: 4:07:37.096884, loss: 0.6341
2023-04-10 22:11:06 - training - INFO - Epoch [4/5][81/415] lr: 1.6e-05, eta: 3:37:28.841682, loss: 0.9481
2023-04-10 22:11:10 - training - INFO - Epoch [4/5][91/415] lr: 1.6e-05, eta: 3:13:57.233344, loss: 0.9497
2023-04-10 22:11:13 - training - INFO - Epoch [4/5][101/415] lr: 1.6e-05, eta: 2:55:04.437678, loss: 0.8816
2023-04-10 22:11:17 - training - INFO - Epoch [4/5][111/415] lr: 1.6e-05, eta: 2:39:35.071524, loss: 0.9260
2023-04-10 22:11:21 - training - INFO - Epoch [4/5][121/415] lr: 1.6e-05, eta: 2:26:38.684186, loss: 0.4561
2023-04-10 22:11:24 - training - INFO - Epoch [4/5][131/415] lr: 1.5e-05, eta: 2:15:40.336704, loss: 0.3948
2023-04-10 22:11:28 - training - INFO - Epoch [4/5][141/415] lr: 1.5e-05, eta: 2:06:14.814638, loss: 0.3109
2023-04-10 22:11:32 - training - INFO - Epoch [4/5][151/415] lr: 1.5e-05, eta: 1:58:03.740872, loss: 0.9550
2023-04-10 22:11:36 - training - INFO - Epoch [4/5][161/415] lr: 1.5e-05, eta: 1:50:53.148216, loss: 0.9306
2023-04-10 22:11:39 - training - INFO - Epoch [4/5][171/415] lr: 1.4e-05, eta: 1:44:32.594720, loss: 0.5379
2023-04-10 22:11:43 - training - INFO - Epoch [4/5][181/415] lr: 1.4e-05, eta: 1:38:53.616006, loss: 0.4126
2023-04-10 22:11:47 - training - INFO - Epoch [4/5][191/415] lr: 1.4e-05, eta: 1:33:49.742424, loss: 0.6922
2023-04-10 22:11:50 - training - INFO - Epoch [4/5][201/415] lr: 1.4e-05, eta: 1:29:15.663372, loss: 0.7160
2023-04-10 22:11:54 - training - INFO - Epoch [4/5][211/415] lr: 1.4e-05, eta: 1:25:07.242568, loss: 0.7131
2023-04-10 22:11:58 - training - INFO - Epoch [4/5][221/415] lr: 1.3e-05, eta: 1:21:20.983158, loss: 0.7465
2023-04-10 22:12:01 - training - INFO - Epoch [4/5][231/415] lr: 1.3e-05, eta: 1:17:53.992332, loss: 0.5038
2023-04-10 22:12:05 - training - INFO - Epoch [4/5][241/415] lr: 1.3e-05, eta: 1:14:43.884244, loss: 0.6238
2023-04-10 22:12:09 - training - INFO - Epoch [4/5][251/415] lr: 1.3e-05, eta: 1:11:48.636384, loss: 0.5046
2023-04-10 22:12:13 - training - INFO - Epoch [4/5][261/415] lr: 1.2e-05, eta: 1:09:06.533714, loss: 0.7489
2023-04-10 22:12:16 - training - INFO - Epoch [4/5][271/415] lr: 1.2e-05, eta: 1:06:36.096324, loss: 0.5123
2023-04-10 22:12:20 - training - INFO - Epoch [4/5][281/415] lr: 1.2e-05, eta: 1:04:16.093566, loss: 0.8138
2023-04-10 22:12:24 - training - INFO - Epoch [4/5][291/415] lr: 1.2e-05, eta: 1:02:05.459408, loss: 0.7882
2023-04-10 22:12:27 - training - INFO - Epoch [4/5][301/415] lr: 1.2e-05, eta: 1:00:03.276066, loss: 0.6987
2023-04-10 22:12:31 - training - INFO - Epoch [4/5][311/415] lr: 1.1e-05, eta: 0:58:08.749236, loss: 1.0441
2023-04-10 22:12:35 - training - INFO - Epoch [4/5][321/415] lr: 1.1e-05, eta: 0:56:21.129672, loss: 0.4535
2023-04-10 22:12:38 - training - INFO - Epoch [4/5][331/415] lr: 1.1e-05, eta: 0:54:39.797792, loss: 0.5457
2023-04-10 22:12:42 - training - INFO - Epoch [4/5][341/415] lr: 1.1e-05, eta: 0:53:04.163274, loss: 0.3702
2023-04-10 22:12:46 - training - INFO - Epoch [4/5][351/415] lr: 1.0e-05, eta: 0:51:33.755928, loss: 0.3691
2023-04-10 22:12:49 - training - INFO - Epoch [4/5][361/415] lr: 1.0e-05, eta: 0:50:08.145416, loss: 0.6608
2023-04-10 22:12:53 - training - INFO - Epoch [4/5][371/415] lr: 1.0e-05, eta: 0:48:46.969320, loss: 0.4445
2023-04-10 22:12:57 - training - INFO - Epoch [4/5][381/415] lr: 9.8e-06, eta: 0:47:29.850080, loss: 0.7061
2023-04-10 22:13:01 - training - INFO - Epoch [4/5][391/415] lr: 9.6e-06, eta: 0:46:16.478160, loss: 1.0164
2023-04-10 22:13:04 - training - INFO - Epoch [4/5][401/415] lr: 9.4e-06, eta: 0:45:06.593508, loss: 1.0841
2023-04-10 22:13:08 - training - INFO - Epoch [4/5][411/415] lr: 9.2e-06, eta: 0:43:59.937664, loss: 0.6048
2023-04-10 22:13:18 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 0.6741, Validation Metrics: {'exact_match': 79.20433996383363, 'f1': 81.97090968812063}
2023-04-10 22:13:18 - training - INFO - Epoch [5/5][1/415] lr: 9.1e-06, eta: 15 days, 21:31:44.664510, loss: 0.4250
2023-04-10 22:13:22 - training - INFO - Epoch [5/5][11/415] lr: 8.8e-06, eta: 1 day, 10:42:33.598176, loss: 0.5392
2023-04-10 22:13:26 - training - INFO - Epoch [5/5][21/415] lr: 8.6e-06, eta: 18:11:35.941134, loss: 0.5473
2023-04-10 22:13:29 - training - INFO - Epoch [5/5][31/415] lr: 8.4e-06, eta: 12:19:55.847608, loss: 0.6865
2023-04-10 22:13:33 - training - INFO - Epoch [5/5][41/415] lr: 8.2e-06, eta: 9:19:46.656876, loss: 0.7837
2023-04-10 22:13:37 - training - INFO - Epoch [5/5][51/415] lr: 8.0e-06, eta: 7:30:15.287376, loss: 0.4843
2023-04-10 22:13:40 - training - INFO - Epoch [5/5][61/415] lr: 7.7e-06, eta: 6:16:37.118266, loss: 0.6525
2023-04-10 22:13:44 - training - INFO - Epoch [5/5][71/415] lr: 7.5e-06, eta: 5:23:42.383232, loss: 0.3946
2023-04-10 22:13:48 - training - INFO - Epoch [5/5][81/415] lr: 7.3e-06, eta: 4:43:50.696174, loss: 0.5244
2023-04-10 22:13:51 - training - INFO - Epoch [5/5][91/415] lr: 7.1e-06, eta: 4:12:43.789376, loss: 0.2224
2023-04-10 22:13:55 - training - INFO - Epoch [5/5][101/415] lr: 6.9e-06, eta: 3:47:45.725640, loss: 0.5037
2023-04-10 22:13:59 - training - INFO - Epoch [5/5][111/415] lr: 6.7e-06, eta: 3:27:16.923944, loss: 0.2934
2023-04-10 22:14:02 - training - INFO - Epoch [5/5][121/415] lr: 6.4e-06, eta: 3:10:10.638974, loss: 0.6422
2023-04-10 22:14:06 - training - INFO - Epoch [5/5][131/415] lr: 6.2e-06, eta: 2:55:40.471032, loss: 0.5876
2023-04-10 22:14:10 - training - INFO - Epoch [5/5][141/415] lr: 6.0e-06, eta: 2:43:13.188064, loss: 0.5228
2023-04-10 22:14:14 - training - INFO - Epoch [5/5][151/415] lr: 5.8e-06, eta: 2:32:24.462236, loss: 0.5335
2023-04-10 22:14:17 - training - INFO - Epoch [5/5][161/415] lr: 5.6e-06, eta: 2:22:55.849260, loss: 0.6657
2023-04-10 22:14:21 - training - INFO - Epoch [5/5][171/415] lr: 5.3e-06, eta: 2:14:33.298912, loss: 0.7727
2023-04-10 22:14:25 - training - INFO - Epoch [5/5][181/415] lr: 5.1e-06, eta: 2:07:05.851974, loss: 0.7959
2023-04-10 22:14:28 - training - INFO - Epoch [5/5][191/415] lr: 4.9e-06, eta: 2:00:24.870588, loss: 0.7848
2023-04-10 22:14:32 - training - INFO - Epoch [5/5][201/415] lr: 4.7e-06, eta: 1:54:23.410686, loss: 1.1281
2023-04-10 22:14:36 - training - INFO - Epoch [5/5][211/415] lr: 4.5e-06, eta: 1:48:55.871816, loss: 0.5199
2023-04-10 22:14:39 - training - INFO - Epoch [5/5][221/415] lr: 4.2e-06, eta: 1:43:57.690300, loss: 0.6166
2023-04-10 22:14:43 - training - INFO - Epoch [5/5][231/415] lr: 4.0e-06, eta: 1:39:24.984108, loss: 0.5038
2023-04-10 22:14:47 - training - INFO - Epoch [5/5][241/415] lr: 3.8e-06, eta: 1:35:14.622956, loss: 0.6855
2023-04-10 22:14:51 - training - INFO - Epoch [5/5][251/415] lr: 3.6e-06, eta: 1:31:23.885184, loss: 0.7880
2023-04-10 22:14:54 - training - INFO - Epoch [5/5][261/415] lr: 3.4e-06, eta: 1:27:50.580628, loss: 0.6298
2023-04-10 22:14:58 - training - INFO - Epoch [5/5][271/415] lr: 3.2e-06, eta: 1:24:32.721720, loss: 0.8765
2023-04-10 22:15:02 - training - INFO - Epoch [5/5][281/415] lr: 2.9e-06, eta: 1:21:28.648206, loss: 0.6834
2023-04-10 22:15:05 - training - INFO - Epoch [5/5][291/415] lr: 2.7e-06, eta: 1:18:36.997688, loss: 1.1351
2023-04-10 22:15:09 - training - INFO - Epoch [5/5][301/415] lr: 2.5e-06, eta: 1:15:56.487068, loss: 0.4690
2023-04-10 22:15:13 - training - INFO - Epoch [5/5][311/415] lr: 2.3e-06, eta: 1:13:26.080392, loss: 0.6862
2023-04-10 22:15:16 - training - INFO - Epoch [5/5][321/415] lr: 2.1e-06, eta: 1:11:04.784348, loss: 1.1005
2023-04-10 22:15:20 - training - INFO - Epoch [5/5][331/415] lr: 1.8e-06, eta: 1:08:51.799344, loss: 0.6234
2023-04-10 22:15:24 - training - INFO - Epoch [5/5][341/415] lr: 1.6e-06, eta: 1:06:46.420872, loss: 0.6181
2023-04-10 22:15:27 - training - INFO - Epoch [5/5][351/415] lr: 1.4e-06, eta: 1:04:47.952732, loss: 0.8251
2023-04-10 22:15:31 - training - INFO - Epoch [5/5][361/415] lr: 1.2e-06, eta: 1:02:55.888866, loss: 0.6807
2023-04-10 22:15:35 - training - INFO - Epoch [5/5][371/415] lr: 9.6e-07, eta: 1:01:09.649200, loss: 0.5342
2023-04-10 22:15:39 - training - INFO - Epoch [5/5][381/415] lr: 7.4e-07, eta: 0:59:28.797232, loss: 0.5322
2023-04-10 22:15:42 - training - INFO - Epoch [5/5][391/415] lr: 5.3e-07, eta: 0:57:52.913200, loss: 0.7487
2023-04-10 22:15:46 - training - INFO - Epoch [5/5][401/415] lr: 3.1e-07, eta: 0:56:21.613920, loss: 0.9044
2023-04-10 22:15:50 - training - INFO - Epoch [5/5][411/415] lr: 8.8e-08, eta: 0:54:54.575232, loss: 0.4914
2023-04-10 22:15:59 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 0.5982, Validation Metrics: {'exact_match': 81.55515370705244, 'f1': 83.79214862894791}
2023-04-10 22:16:08 - training - INFO - Final Test - Train Loss: 0.5982, Test Metrics: {'exact_match': 81.44144144144144, 'f1': 84.20906646603243}
