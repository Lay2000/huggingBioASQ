2023-04-11 23:07:26 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1380cc367820a3f3/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'bert-base-uncased'}, 'data': {'task_type': 'factoid', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 4.54e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/bert_factoid_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 577.09it/s]
Map:   0%|          | 0/4429 [00:00<?, ? examples/s]Map:  23%|██▎       | 1000/4429 [00:00<00:02, 1384.09 examples/s]Map:  45%|████▌     | 2000/4429 [00:01<00:01, 1552.03 examples/s]Map:  68%|██████▊   | 3000/4429 [00:01<00:00, 1639.85 examples/s]Map:  90%|█████████ | 4000/4429 [00:02<00:00, 1648.95 examples/s]Map: 100%|██████████| 4429/4429 [00:02<00:00, 1655.21 examples/s]                                                                 Map:   0%|          | 0/553 [00:00<?, ? examples/s]Map: 100%|██████████| 553/553 [00:00<00:00, 1401.14 examples/s]                                                               Map:   0%|          | 0/555 [00:00<?, ? examples/s]Map: 100%|██████████| 555/555 [00:00<00:00, 1370.80 examples/s]                                                               Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-11 23:07:54 - training - INFO - First Test - Val Metrics:{'exact_match': 0.0, 'f1': 2.785352181533233} Test Metrics: {'exact_match': 0.0, 'f1': 3.142615266454275}
2023-04-11 23:07:54 - training - INFO - Epoch [1/5][1/415] lr: 4.5e-05, eta: 10:13:01.201598, loss: 5.9099
2023-04-11 23:07:58 - training - INFO - Epoch [1/5][11/415] lr: 4.5e-05, eta: 1:06:52.087824, loss: 4.1512
2023-04-11 23:08:02 - training - INFO - Epoch [1/5][21/415] lr: 4.5e-05, eta: 0:40:48.731558, loss: 4.1930
2023-04-11 23:08:05 - training - INFO - Epoch [1/5][31/415] lr: 4.5e-05, eta: 0:31:31.756748, loss: 2.6754
2023-04-11 23:08:09 - training - INFO - Epoch [1/5][41/415] lr: 4.5e-05, eta: 0:26:45.053808, loss: 2.6074
2023-04-11 23:08:13 - training - INFO - Epoch [1/5][51/415] lr: 4.4e-05, eta: 0:23:49.462144, loss: 1.8602
2023-04-11 23:08:16 - training - INFO - Epoch [1/5][61/415] lr: 4.4e-05, eta: 0:21:50.286246, loss: 1.8580
2023-04-11 23:08:20 - training - INFO - Epoch [1/5][71/415] lr: 4.4e-05, eta: 0:20:23.624364, loss: 2.5412
2023-04-11 23:08:24 - training - INFO - Epoch [1/5][81/415] lr: 4.4e-05, eta: 0:19:17.499054, loss: 2.3896
2023-04-11 23:08:27 - training - INFO - Epoch [1/5][91/415] lr: 4.3e-05, eta: 0:18:25.030464, loss: 2.7276
2023-04-11 23:08:31 - training - INFO - Epoch [1/5][101/415] lr: 4.3e-05, eta: 0:17:42.258750, loss: 2.5123
2023-04-11 23:08:35 - training - INFO - Epoch [1/5][111/415] lr: 4.3e-05, eta: 0:17:06.533700, loss: 2.1178
2023-04-11 23:08:38 - training - INFO - Epoch [1/5][121/415] lr: 4.3e-05, eta: 0:16:36.106212, loss: 1.3035
2023-04-11 23:08:42 - training - INFO - Epoch [1/5][131/415] lr: 4.3e-05, eta: 0:16:09.799392, loss: 2.1245
2023-04-11 23:08:46 - training - INFO - Epoch [1/5][141/415] lr: 4.2e-05, eta: 0:15:46.722010, loss: 2.6136
2023-04-11 23:08:49 - training - INFO - Epoch [1/5][151/415] lr: 4.2e-05, eta: 0:15:26.205904, loss: 2.0860
2023-04-11 23:08:53 - training - INFO - Epoch [1/5][161/415] lr: 4.2e-05, eta: 0:15:07.827426, loss: 2.4452
2023-04-11 23:08:57 - training - INFO - Epoch [1/5][171/415] lr: 4.2e-05, eta: 0:14:51.186240, loss: 1.7681
2023-04-11 23:09:00 - training - INFO - Epoch [1/5][181/415] lr: 4.1e-05, eta: 0:14:35.940908, loss: 2.8339
2023-04-11 23:09:04 - training - INFO - Epoch [1/5][191/415] lr: 4.1e-05, eta: 0:14:21.930000, loss: 1.6265
2023-04-11 23:09:08 - training - INFO - Epoch [1/5][201/415] lr: 4.1e-05, eta: 0:14:08.959480, loss: 1.2138
2023-04-11 23:09:11 - training - INFO - Epoch [1/5][211/415] lr: 4.1e-05, eta: 0:13:56.898720, loss: 2.0073
2023-04-11 23:09:15 - training - INFO - Epoch [1/5][221/415] lr: 4.1e-05, eta: 0:13:45.528726, loss: 1.6266
2023-04-11 23:09:19 - training - INFO - Epoch [1/5][231/415] lr: 4.0e-05, eta: 0:13:34.880196, loss: 1.7777
2023-04-11 23:09:22 - training - INFO - Epoch [1/5][241/415] lr: 4.0e-05, eta: 0:13:24.821556, loss: 1.3794
2023-04-11 23:09:26 - training - INFO - Epoch [1/5][251/415] lr: 4.0e-05, eta: 0:13:15.285888, loss: 1.3057
2023-04-11 23:09:30 - training - INFO - Epoch [1/5][261/415] lr: 4.0e-05, eta: 0:13:06.193042, loss: 1.8824
2023-04-11 23:09:33 - training - INFO - Epoch [1/5][271/415] lr: 3.9e-05, eta: 0:12:57.498744, loss: 0.9912
2023-04-11 23:09:37 - training - INFO - Epoch [1/5][281/415] lr: 3.9e-05, eta: 0:12:49.159560, loss: 1.5007
2023-04-11 23:09:41 - training - INFO - Epoch [1/5][291/415] lr: 3.9e-05, eta: 0:12:41.159656, loss: 1.6978
2023-04-11 23:09:44 - training - INFO - Epoch [1/5][301/415] lr: 3.9e-05, eta: 0:12:33.421348, loss: 1.2258
2023-04-11 23:09:48 - training - INFO - Epoch [1/5][311/415] lr: 3.9e-05, eta: 0:12:25.953264, loss: 1.9543
2023-04-11 23:09:52 - training - INFO - Epoch [1/5][321/415] lr: 3.8e-05, eta: 0:12:18.726918, loss: 1.4571
2023-04-11 23:09:55 - training - INFO - Epoch [1/5][331/415] lr: 3.8e-05, eta: 0:12:11.726592, loss: 1.7875
2023-04-11 23:09:59 - training - INFO - Epoch [1/5][341/415] lr: 3.8e-05, eta: 0:12:04.900434, loss: 1.8891
2023-04-11 23:10:03 - training - INFO - Epoch [1/5][351/415] lr: 3.8e-05, eta: 0:11:58.294256, loss: 1.2441
2023-04-11 23:10:07 - training - INFO - Epoch [1/5][361/415] lr: 3.8e-05, eta: 0:11:51.841340, loss: 1.3196
2023-04-11 23:10:10 - training - INFO - Epoch [1/5][371/415] lr: 3.7e-05, eta: 0:11:45.541200, loss: 1.0058
2023-04-11 23:10:14 - training - INFO - Epoch [1/5][381/415] lr: 3.7e-05, eta: 0:11:39.374676, loss: 1.1820
2023-04-11 23:10:18 - training - INFO - Epoch [1/5][391/415] lr: 3.7e-05, eta: 0:11:33.333112, loss: 1.5319
2023-04-11 23:10:21 - training - INFO - Epoch [1/5][401/415] lr: 3.7e-05, eta: 0:11:27.404664, loss: 1.5800
2023-04-11 23:10:25 - training - INFO - Epoch [1/5][411/415] lr: 3.6e-05, eta: 0:11:21.602688, loss: 1.1941
2023-04-11 23:10:43 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 1.9655, Validation Metrics: {'exact_match': 77.21518987341773, 'f1': 80.88717473614088}, Test Metrics: {'exact_match': 81.98198198198199, 'f1': 85.4687648099413}
2023-04-11 23:10:44 - training - INFO - Epoch [2/5][1/415] lr: 3.6e-05, eta: 4 days, 11:50:53.720856, loss: 0.8121
2023-04-11 23:10:47 - training - INFO - Epoch [2/5][11/415] lr: 3.6e-05, eta: 9:56:55.233888, loss: 1.4274
2023-04-11 23:10:51 - training - INFO - Epoch [2/5][21/415] lr: 3.6e-05, eta: 5:17:09.498670, loss: 0.9779
2023-04-11 23:10:55 - training - INFO - Epoch [2/5][31/415] lr: 3.6e-05, eta: 3:37:50.907836, loss: 1.0246
2023-04-11 23:10:59 - training - INFO - Epoch [2/5][41/415] lr: 3.5e-05, eta: 2:46:57.441864, loss: 1.0154
2023-04-11 23:11:02 - training - INFO - Epoch [2/5][51/415] lr: 3.5e-05, eta: 2:15:59.836960, loss: 0.9058
2023-04-11 23:11:06 - training - INFO - Epoch [2/5][61/415] lr: 3.5e-05, eta: 1:55:10.134700, loss: 1.2565
2023-04-11 23:11:10 - training - INFO - Epoch [2/5][71/415] lr: 3.5e-05, eta: 1:40:11.336676, loss: 1.4934
2023-04-11 23:11:13 - training - INFO - Epoch [2/5][81/415] lr: 3.5e-05, eta: 1:28:53.622984, loss: 1.0163
2023-04-11 23:11:17 - training - INFO - Epoch [2/5][91/415] lr: 3.4e-05, eta: 1:20:04.105216, loss: 0.7724
2023-04-11 23:11:21 - training - INFO - Epoch [2/5][101/415] lr: 3.4e-05, eta: 1:12:58.637970, loss: 0.7322
2023-04-11 23:11:24 - training - INFO - Epoch [2/5][111/415] lr: 3.4e-05, eta: 1:07:09.185280, loss: 1.1939
2023-04-11 23:11:28 - training - INFO - Epoch [2/5][121/415] lr: 3.4e-05, eta: 1:02:16.899944, loss: 0.9948
2023-04-11 23:11:32 - training - INFO - Epoch [2/5][131/415] lr: 3.3e-05, eta: 0:58:08.651856, loss: 1.4263
2023-04-11 23:11:35 - training - INFO - Epoch [2/5][141/415] lr: 3.3e-05, eta: 0:54:35.126498, loss: 1.0961
2023-04-11 23:11:39 - training - INFO - Epoch [2/5][151/415] lr: 3.3e-05, eta: 0:51:29.412976, loss: 1.0080
2023-04-11 23:11:43 - training - INFO - Epoch [2/5][161/415] lr: 3.3e-05, eta: 0:48:46.283976, loss: 0.8217
2023-04-11 23:11:46 - training - INFO - Epoch [2/5][171/415] lr: 3.3e-05, eta: 0:46:21.795408, loss: 1.3091
2023-04-11 23:11:50 - training - INFO - Epoch [2/5][181/415] lr: 3.2e-05, eta: 0:44:12.812160, loss: 0.8596
2023-04-11 23:11:54 - training - INFO - Epoch [2/5][191/415] lr: 3.2e-05, eta: 0:42:16.983096, loss: 0.3856
2023-04-11 23:11:58 - training - INFO - Epoch [2/5][201/415] lr: 3.2e-05, eta: 0:40:32.347056, loss: 0.8692
2023-04-11 23:12:01 - training - INFO - Epoch [2/5][211/415] lr: 3.2e-05, eta: 0:38:57.275192, loss: 1.0581
2023-04-11 23:12:05 - training - INFO - Epoch [2/5][221/415] lr: 3.1e-05, eta: 0:37:30.468630, loss: 1.0374
2023-04-11 23:12:09 - training - INFO - Epoch [2/5][231/415] lr: 3.1e-05, eta: 0:36:10.902476, loss: 0.8940
2023-04-11 23:12:12 - training - INFO - Epoch [2/5][241/415] lr: 3.1e-05, eta: 0:34:57.622828, loss: 1.1719
2023-04-11 23:12:16 - training - INFO - Epoch [2/5][251/415] lr: 3.1e-05, eta: 0:33:49.929600, loss: 0.4765
2023-04-11 23:12:20 - training - INFO - Epoch [2/5][261/415] lr: 3.1e-05, eta: 0:32:47.101600, loss: 1.3001
2023-04-11 23:12:23 - training - INFO - Epoch [2/5][271/415] lr: 3.0e-05, eta: 0:31:48.668080, loss: 0.8735
2023-04-11 23:12:27 - training - INFO - Epoch [2/5][281/415] lr: 3.0e-05, eta: 0:30:54.111558, loss: 0.5127
2023-04-11 23:12:31 - training - INFO - Epoch [2/5][291/415] lr: 3.0e-05, eta: 0:30:03.040632, loss: 1.0338
2023-04-11 23:12:34 - training - INFO - Epoch [2/5][301/415] lr: 3.0e-05, eta: 0:29:15.129962, loss: 1.0786
2023-04-11 23:12:38 - training - INFO - Epoch [2/5][311/415] lr: 3.0e-05, eta: 0:28:30.039240, loss: 0.6013
2023-04-11 23:12:42 - training - INFO - Epoch [2/5][321/415] lr: 2.9e-05, eta: 0:27:47.543586, loss: 0.7050
2023-04-11 23:12:45 - training - INFO - Epoch [2/5][331/415] lr: 2.9e-05, eta: 0:27:07.420576, loss: 0.9347
2023-04-11 23:12:49 - training - INFO - Epoch [2/5][341/415] lr: 2.9e-05, eta: 0:26:29.436420, loss: 1.1655
2023-04-11 23:12:53 - training - INFO - Epoch [2/5][351/415] lr: 2.9e-05, eta: 0:25:53.379168, loss: 0.8225
2023-04-11 23:12:57 - training - INFO - Epoch [2/5][361/415] lr: 2.8e-05, eta: 0:25:19.109630, loss: 0.8491
2023-04-11 23:13:00 - training - INFO - Epoch [2/5][371/415] lr: 2.8e-05, eta: 0:24:46.480992, loss: 0.8072
2023-04-11 23:13:04 - training - INFO - Epoch [2/5][381/415] lr: 2.8e-05, eta: 0:24:15.383160, loss: 0.6909
2023-04-11 23:13:08 - training - INFO - Epoch [2/5][391/415] lr: 2.8e-05, eta: 0:23:45.697976, loss: 0.9294
2023-04-11 23:13:11 - training - INFO - Epoch [2/5][401/415] lr: 2.8e-05, eta: 0:23:17.311236, loss: 1.5109
2023-04-11 23:13:15 - training - INFO - Epoch [2/5][411/415] lr: 2.7e-05, eta: 0:22:50.104320, loss: 1.2299
2023-04-11 23:13:34 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.0270, Validation Metrics: {'exact_match': 78.84267631103074, 'f1': 81.34439921180133}, Test Metrics: {'exact_match': 81.62162162162163, 'f1': 84.59989418874869}
2023-04-11 23:13:34 - training - INFO - Epoch [3/5][1/415] lr: 2.7e-05, eta: 8 days, 13:55:48.074930, loss: 1.3627
2023-04-11 23:13:38 - training - INFO - Epoch [3/5][11/415] lr: 2.7e-05, eta: 18:49:21.320208, loss: 0.8723
2023-04-11 23:13:41 - training - INFO - Epoch [3/5][21/415] lr: 2.7e-05, eta: 9:54:42.737064, loss: 0.6585
2023-04-11 23:13:45 - training - INFO - Epoch [3/5][31/415] lr: 2.7e-05, eta: 6:44:58.052044, loss: 0.5632
2023-04-11 23:13:49 - training - INFO - Epoch [3/5][41/415] lr: 2.6e-05, eta: 5:07:44.857434, loss: 0.7149
2023-04-11 23:13:53 - training - INFO - Epoch [3/5][51/415] lr: 2.6e-05, eta: 4:08:37.851520, loss: 1.0523
2023-04-11 23:13:56 - training - INFO - Epoch [3/5][61/415] lr: 2.6e-05, eta: 3:28:52.739340, loss: 0.6772
2023-04-11 23:14:00 - training - INFO - Epoch [3/5][71/415] lr: 2.6e-05, eta: 3:00:18.143100, loss: 0.6575
2023-04-11 23:14:04 - training - INFO - Epoch [3/5][81/415] lr: 2.5e-05, eta: 2:38:46.045870, loss: 1.1784
2023-04-11 23:14:07 - training - INFO - Epoch [3/5][91/415] lr: 2.5e-05, eta: 2:21:57.167168, loss: 1.2427
2023-04-11 23:14:11 - training - INFO - Epoch [3/5][101/415] lr: 2.5e-05, eta: 2:08:27.281652, loss: 0.6769
2023-04-11 23:14:15 - training - INFO - Epoch [3/5][111/415] lr: 2.5e-05, eta: 1:57:22.703672, loss: 0.7889
2023-04-11 23:14:18 - training - INFO - Epoch [3/5][121/415] lr: 2.5e-05, eta: 1:48:07.328850, loss: 0.5538
2023-04-11 23:14:22 - training - INFO - Epoch [3/5][131/415] lr: 2.4e-05, eta: 1:40:16.176504, loss: 0.7854
2023-04-11 23:14:26 - training - INFO - Epoch [3/5][141/415] lr: 2.4e-05, eta: 1:33:31.334676, loss: 1.1625
2023-04-11 23:14:29 - training - INFO - Epoch [3/5][151/415] lr: 2.4e-05, eta: 1:27:39.608016, loss: 0.9178
2023-04-11 23:14:33 - training - INFO - Epoch [3/5][161/415] lr: 2.4e-05, eta: 1:22:31.137114, loss: 0.6973
2023-04-11 23:14:37 - training - INFO - Epoch [3/5][171/415] lr: 2.3e-05, eta: 1:17:58.383136, loss: 0.7659
2023-04-11 23:14:40 - training - INFO - Epoch [3/5][181/415] lr: 2.3e-05, eta: 1:13:55.318062, loss: 0.8678
2023-04-11 23:14:44 - training - INFO - Epoch [3/5][191/415] lr: 2.3e-05, eta: 1:10:17.409360, loss: 0.6195
2023-04-11 23:14:48 - training - INFO - Epoch [3/5][201/415] lr: 2.3e-05, eta: 1:07:00.762574, loss: 0.6037
2023-04-11 23:14:52 - training - INFO - Epoch [3/5][211/415] lr: 2.3e-05, eta: 1:04:02.371312, loss: 1.0537
2023-04-11 23:14:55 - training - INFO - Epoch [3/5][221/415] lr: 2.2e-05, eta: 1:01:19.804368, loss: 1.3068
2023-04-11 23:14:59 - training - INFO - Epoch [3/5][231/415] lr: 2.2e-05, eta: 0:58:50.992620, loss: 0.4468
2023-04-11 23:15:03 - training - INFO - Epoch [3/5][241/415] lr: 2.2e-05, eta: 0:56:34.229650, loss: 0.9626
2023-04-11 23:15:06 - training - INFO - Epoch [3/5][251/415] lr: 2.2e-05, eta: 0:54:28.075392, loss: 0.5827
2023-04-11 23:15:10 - training - INFO - Epoch [3/5][261/415] lr: 2.2e-05, eta: 0:52:31.311638, loss: 0.8151
2023-04-11 23:15:14 - training - INFO - Epoch [3/5][271/415] lr: 2.1e-05, eta: 0:50:42.841076, loss: 0.5148
2023-04-11 23:15:17 - training - INFO - Epoch [3/5][281/415] lr: 2.1e-05, eta: 0:49:01.829904, loss: 0.4859
2023-04-11 23:15:21 - training - INFO - Epoch [3/5][291/415] lr: 2.1e-05, eta: 0:47:27.522680, loss: 0.6415
2023-04-11 23:15:25 - training - INFO - Epoch [3/5][301/415] lr: 2.1e-05, eta: 0:45:59.229928, loss: 0.9917
2023-04-11 23:15:28 - training - INFO - Epoch [3/5][311/415] lr: 2.0e-05, eta: 0:44:36.388428, loss: 0.7984
2023-04-11 23:15:32 - training - INFO - Epoch [3/5][321/415] lr: 2.0e-05, eta: 0:43:18.463300, loss: 0.8658
2023-04-11 23:15:36 - training - INFO - Epoch [3/5][331/415] lr: 2.0e-05, eta: 0:42:05.039936, loss: 0.6911
2023-04-11 23:15:40 - training - INFO - Epoch [3/5][341/415] lr: 2.0e-05, eta: 0:40:55.720278, loss: 0.6149
2023-04-11 23:15:43 - training - INFO - Epoch [3/5][351/415] lr: 2.0e-05, eta: 0:39:50.113948, loss: 1.5208
2023-04-11 23:15:47 - training - INFO - Epoch [3/5][361/415] lr: 1.9e-05, eta: 0:38:47.954800, loss: 0.5563
2023-04-11 23:15:51 - training - INFO - Epoch [3/5][371/415] lr: 1.9e-05, eta: 0:37:48.944160, loss: 0.7596
2023-04-11 23:15:54 - training - INFO - Epoch [3/5][381/415] lr: 1.9e-05, eta: 0:36:52.845096, loss: 0.6778
2023-04-11 23:15:58 - training - INFO - Epoch [3/5][391/415] lr: 1.9e-05, eta: 0:35:59.415092, loss: 0.7523
2023-04-11 23:16:02 - training - INFO - Epoch [3/5][401/415] lr: 1.8e-05, eta: 0:35:08.483352, loss: 0.5662
2023-04-11 23:16:05 - training - INFO - Epoch [3/5][411/415] lr: 1.8e-05, eta: 0:34:19.863936, loss: 1.0972
2023-04-11 23:16:24 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 0.7968, Validation Metrics: {'exact_match': 80.28933092224231, 'f1': 83.04341954569813}, Test Metrics: {'exact_match': 82.34234234234235, 'f1': 85.36447031183873}
2023-04-11 23:16:24 - training - INFO - Epoch [4/5][1/415] lr: 1.8e-05, eta: 12 days, 16:05:07.637606, loss: 0.7223
2023-04-11 23:16:28 - training - INFO - Epoch [4/5][11/415] lr: 1.8e-05, eta: 1 day, 3:42:10.901040, loss: 0.7077
2023-04-11 23:16:32 - training - INFO - Epoch [4/5][21/415] lr: 1.8e-05, eta: 14:32:27.753094, loss: 0.4692
2023-04-11 23:16:36 - training - INFO - Epoch [4/5][31/415] lr: 1.7e-05, eta: 9:52:12.006860, loss: 0.4260
2023-04-11 23:16:39 - training - INFO - Epoch [4/5][41/415] lr: 1.7e-05, eta: 7:28:37.699716, loss: 0.2003
2023-04-11 23:16:43 - training - INFO - Epoch [4/5][51/415] lr: 1.7e-05, eta: 6:01:20.205536, loss: 0.4584
2023-04-11 23:16:47 - training - INFO - Epoch [4/5][61/415] lr: 1.7e-05, eta: 5:02:38.451582, loss: 0.8446
2023-04-11 23:16:50 - training - INFO - Epoch [4/5][71/415] lr: 1.7e-05, eta: 4:20:27.855324, loss: 0.3189
2023-04-11 23:16:54 - training - INFO - Epoch [4/5][81/415] lr: 1.6e-05, eta: 3:48:41.025064, loss: 0.6239
2023-04-11 23:16:58 - training - INFO - Epoch [4/5][91/415] lr: 1.6e-05, eta: 3:23:52.496832, loss: 0.6272
2023-04-11 23:17:01 - training - INFO - Epoch [4/5][101/415] lr: 1.6e-05, eta: 3:03:58.128318, loss: 0.5975
2023-04-11 23:17:05 - training - INFO - Epoch [4/5][111/415] lr: 1.6e-05, eta: 2:47:38.211596, loss: 0.4822
2023-04-11 23:17:09 - training - INFO - Epoch [4/5][121/415] lr: 1.6e-05, eta: 2:33:59.721526, loss: 0.6230
2023-04-11 23:17:12 - training - INFO - Epoch [4/5][131/415] lr: 1.5e-05, eta: 2:22:25.610160, loss: 0.6612
2023-04-11 23:17:16 - training - INFO - Epoch [4/5][141/415] lr: 1.5e-05, eta: 2:12:29.418834, loss: 0.8802
2023-04-11 23:17:20 - training - INFO - Epoch [4/5][151/415] lr: 1.5e-05, eta: 2:03:51.703968, loss: 0.6519
2023-04-11 23:17:24 - training - INFO - Epoch [4/5][161/415] lr: 1.5e-05, eta: 1:56:17.814294, loss: 0.8783
2023-04-11 23:17:27 - training - INFO - Epoch [4/5][171/415] lr: 1.4e-05, eta: 1:49:36.545472, loss: 0.3870
2023-04-11 23:17:31 - training - INFO - Epoch [4/5][181/415] lr: 1.4e-05, eta: 1:43:39.217948, loss: 0.4923
2023-04-11 23:17:35 - training - INFO - Epoch [4/5][191/415] lr: 1.4e-05, eta: 1:38:18.903852, loss: 0.6338
2023-04-11 23:17:38 - training - INFO - Epoch [4/5][201/415] lr: 1.4e-05, eta: 1:33:30.128210, loss: 0.8988
2023-04-11 23:17:42 - training - INFO - Epoch [4/5][211/415] lr: 1.4e-05, eta: 1:29:08.341648, loss: 0.7609
2023-04-11 23:17:46 - training - INFO - Epoch [4/5][221/415] lr: 1.3e-05, eta: 1:25:09.918786, loss: 0.8188
2023-04-11 23:17:49 - training - INFO - Epoch [4/5][231/415] lr: 1.3e-05, eta: 1:21:31.774264, loss: 0.8636
2023-04-11 23:17:53 - training - INFO - Epoch [4/5][241/415] lr: 1.3e-05, eta: 1:18:11.478372, loss: 0.9932
2023-04-11 23:17:57 - training - INFO - Epoch [4/5][251/415] lr: 1.3e-05, eta: 1:15:06.901536, loss: 0.9302
2023-04-11 23:18:01 - training - INFO - Epoch [4/5][261/415] lr: 1.2e-05, eta: 1:12:16.140250, loss: 0.6851
2023-04-11 23:18:04 - training - INFO - Epoch [4/5][271/415] lr: 1.2e-05, eta: 1:09:37.719436, loss: 0.5026
2023-04-11 23:18:08 - training - INFO - Epoch [4/5][281/415] lr: 1.2e-05, eta: 1:07:10.299936, loss: 0.6726
2023-04-11 23:18:12 - training - INFO - Epoch [4/5][291/415] lr: 1.2e-05, eta: 1:04:52.748656, loss: 0.9798
2023-04-11 23:18:15 - training - INFO - Epoch [4/5][301/415] lr: 1.2e-05, eta: 1:02:44.082070, loss: 1.1996
2023-04-11 23:18:19 - training - INFO - Epoch [4/5][311/415] lr: 1.1e-05, eta: 1:00:43.492608, loss: 0.8460
2023-04-11 23:18:23 - training - INFO - Epoch [4/5][321/415] lr: 1.1e-05, eta: 0:58:50.167052, loss: 0.4094
2023-04-11 23:18:26 - training - INFO - Epoch [4/5][331/415] lr: 1.1e-05, eta: 0:57:03.485952, loss: 0.6985
2023-04-11 23:18:30 - training - INFO - Epoch [4/5][341/415] lr: 1.1e-05, eta: 0:55:22.798308, loss: 0.9323
2023-04-11 23:18:34 - training - INFO - Epoch [4/5][351/415] lr: 1.0e-05, eta: 0:53:47.650388, loss: 0.6426
2023-04-11 23:18:37 - training - INFO - Epoch [4/5][361/415] lr: 1.0e-05, eta: 0:52:17.574698, loss: 1.0610
2023-04-11 23:18:41 - training - INFO - Epoch [4/5][371/415] lr: 1.0e-05, eta: 0:50:52.155384, loss: 0.8566
2023-04-11 23:18:45 - training - INFO - Epoch [4/5][381/415] lr: 9.8e-06, eta: 0:49:31.040534, loss: 0.5523
2023-04-11 23:18:49 - training - INFO - Epoch [4/5][391/415] lr: 9.6e-06, eta: 0:48:13.884956, loss: 0.5335
2023-04-11 23:18:52 - training - INFO - Epoch [4/5][401/415] lr: 9.4e-06, eta: 0:47:00.381984, loss: 1.3450
2023-04-11 23:18:56 - training - INFO - Epoch [4/5][411/415] lr: 9.2e-06, eta: 0:45:50.295808, loss: 0.6188
2023-04-11 23:19:14 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 0.6695, Validation Metrics: {'exact_match': 78.3001808318264, 'f1': 81.40186198996433}, Test Metrics: {'exact_match': 82.70270270270271, 'f1': 85.70075294223902}
2023-04-11 23:19:15 - training - INFO - Epoch [5/5][1/415] lr: 9.1e-06, eta: 16 days, 18:15:02.505984, loss: 0.5440
2023-04-11 23:19:19 - training - INFO - Epoch [5/5][11/415] lr: 8.8e-06, eta: 1 day, 12:35:02.537616, loss: 0.1126
2023-04-11 23:19:22 - training - INFO - Epoch [5/5][21/415] lr: 8.6e-06, eta: 19:10:13.831042, loss: 0.5366
2023-04-11 23:19:26 - training - INFO - Epoch [5/5][31/415] lr: 8.4e-06, eta: 12:59:27.243264, loss: 0.4892
2023-04-11 23:19:30 - training - INFO - Epoch [5/5][41/415] lr: 8.2e-06, eta: 9:49:31.190844, loss: 0.4130
2023-04-11 23:19:33 - training - INFO - Epoch [5/5][51/415] lr: 8.0e-06, eta: 7:54:02.334888, loss: 0.9625
2023-04-11 23:19:37 - training - INFO - Epoch [5/5][61/415] lr: 7.7e-06, eta: 6:36:24.173894, loss: 0.4203
2023-04-11 23:19:41 - training - INFO - Epoch [5/5][71/415] lr: 7.5e-06, eta: 5:40:37.138692, loss: 0.4780
2023-04-11 23:19:44 - training - INFO - Epoch [5/5][81/415] lr: 7.3e-06, eta: 4:58:35.521710, loss: 0.6226
2023-04-11 23:19:48 - training - INFO - Epoch [5/5][91/415] lr: 7.1e-06, eta: 4:25:47.292800, loss: 0.4688
2023-04-11 23:19:52 - training - INFO - Epoch [5/5][101/415] lr: 6.9e-06, eta: 3:59:28.061022, loss: 0.5396
2023-04-11 23:19:55 - training - INFO - Epoch [5/5][111/415] lr: 6.7e-06, eta: 3:37:52.765016, loss: 1.0048
2023-04-11 23:19:59 - training - INFO - Epoch [5/5][121/415] lr: 6.4e-06, eta: 3:19:50.828470, loss: 0.3302
2023-04-11 23:20:03 - training - INFO - Epoch [5/5][131/415] lr: 6.2e-06, eta: 3:04:33.710232, loss: 1.0100
2023-04-11 23:20:07 - training - INFO - Epoch [5/5][141/415] lr: 6.0e-06, eta: 2:51:26.044756, loss: 0.5629
2023-04-11 23:20:10 - training - INFO - Epoch [5/5][151/415] lr: 5.8e-06, eta: 2:40:02.189532, loss: 0.3959
2023-04-11 23:20:14 - training - INFO - Epoch [5/5][161/415] lr: 5.6e-06, eta: 2:30:02.797584, loss: 0.6772
2023-04-11 23:20:18 - training - INFO - Epoch [5/5][171/415] lr: 5.3e-06, eta: 2:21:13.070368, loss: 0.4875
2023-04-11 23:20:21 - training - INFO - Epoch [5/5][181/415] lr: 5.1e-06, eta: 2:13:21.581800, loss: 0.5650
2023-04-11 23:20:25 - training - INFO - Epoch [5/5][191/415] lr: 4.9e-06, eta: 2:06:19.036212, loss: 0.8440
2023-04-11 23:20:29 - training - INFO - Epoch [5/5][201/415] lr: 4.7e-06, eta: 1:59:58.123952, loss: 0.6334
2023-04-11 23:20:32 - training - INFO - Epoch [5/5][211/415] lr: 4.5e-06, eta: 1:54:13.053784, loss: 0.5597
2023-04-11 23:20:36 - training - INFO - Epoch [5/5][221/415] lr: 4.2e-06, eta: 1:48:58.828104, loss: 0.9655
2023-04-11 23:20:40 - training - INFO - Epoch [5/5][231/415] lr: 4.0e-06, eta: 1:44:11.528800, loss: 0.8565
2023-04-11 23:20:43 - training - INFO - Epoch [5/5][241/415] lr: 3.8e-06, eta: 1:39:47.738568, loss: 0.4595
2023-04-11 23:20:47 - training - INFO - Epoch [5/5][251/415] lr: 3.6e-06, eta: 1:35:44.644224, loss: 0.7254
2023-04-11 23:20:51 - training - INFO - Epoch [5/5][261/415] lr: 3.4e-06, eta: 1:31:59.873206, loss: 0.5655
2023-04-11 23:20:55 - training - INFO - Epoch [5/5][271/415] lr: 3.2e-06, eta: 1:28:31.434216, loss: 0.5724
2023-04-11 23:20:58 - training - INFO - Epoch [5/5][281/415] lr: 2.9e-06, eta: 1:25:17.559018, loss: 0.5222
2023-04-11 23:21:02 - training - INFO - Epoch [5/5][291/415] lr: 2.7e-06, eta: 1:22:16.757944, loss: 0.7323
2023-04-11 23:21:06 - training - INFO - Epoch [5/5][301/415] lr: 2.5e-06, eta: 1:19:27.731440, loss: 0.4637
2023-04-11 23:21:09 - training - INFO - Epoch [5/5][311/415] lr: 2.3e-06, eta: 1:16:49.335528, loss: 0.4721
2023-04-11 23:21:13 - training - INFO - Epoch [5/5][321/415] lr: 2.1e-06, eta: 1:14:20.616694, loss: 0.9458
2023-04-11 23:21:17 - training - INFO - Epoch [5/5][331/415] lr: 1.8e-06, eta: 1:12:00.634432, loss: 0.7324
2023-04-11 23:21:20 - training - INFO - Epoch [5/5][341/415] lr: 1.6e-06, eta: 1:09:48.638262, loss: 0.5476
2023-04-11 23:21:24 - training - INFO - Epoch [5/5][351/415] lr: 1.4e-06, eta: 1:07:43.964512, loss: 0.6797
2023-04-11 23:21:28 - training - INFO - Epoch [5/5][361/415] lr: 1.2e-06, eta: 1:05:46.006794, loss: 0.4237
2023-04-11 23:21:31 - training - INFO - Epoch [5/5][371/415] lr: 9.6e-07, eta: 1:03:54.192552, loss: 0.3680
2023-04-11 23:21:35 - training - INFO - Epoch [5/5][381/415] lr: 7.4e-07, eta: 1:02:08.046784, loss: 0.2226
2023-04-11 23:21:39 - training - INFO - Epoch [5/5][391/415] lr: 5.3e-07, eta: 1:00:27.154128, loss: 0.2878
2023-04-11 23:21:42 - training - INFO - Epoch [5/5][401/415] lr: 3.1e-07, eta: 0:58:51.117186, loss: 0.4557
2023-04-11 23:21:46 - training - INFO - Epoch [5/5][411/415] lr: 8.8e-08, eta: 0:57:19.574528, loss: 0.4906
2023-04-11 23:22:05 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 0.5867, Validation Metrics: {'exact_match': 79.56600361663652, 'f1': 82.40645564826504}, Test Metrics: {'exact_match': 83.78378378378379, 'f1': 86.72878097026705}
2023-04-11 23:22:13 - training - INFO - Final Test - Train Loss: 0.5867, Test Metrics: {'exact_match': 83.78378378378379, 'f1': 86.72878097026705}
