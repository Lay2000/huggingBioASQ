2023-04-12 00:53:55 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1380cc367820a3f3/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'csarron/bert-base-uncased-squad-v1'}, 'data': {'task_type': 'factoid', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 1e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/bert_factoid_squad'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 582.84it/s]
Map:   0%|          | 0/4429 [00:00<?, ? examples/s]Map:  23%|██▎       | 1000/4429 [00:00<00:02, 1334.58 examples/s]Map:  45%|████▌     | 2000/4429 [00:01<00:01, 1545.83 examples/s]Map:  68%|██████▊   | 3000/4429 [00:01<00:00, 1633.05 examples/s]Map:  90%|█████████ | 4000/4429 [00:02<00:00, 1685.08 examples/s]Map: 100%|██████████| 4429/4429 [00:02<00:00, 1693.90 examples/s]                                                                 Map:   0%|          | 0/553 [00:00<?, ? examples/s]Map: 100%|██████████| 553/553 [00:00<00:00, 1374.19 examples/s]                                                               Map:   0%|          | 0/555 [00:00<?, ? examples/s]Map: 100%|██████████| 555/555 [00:00<00:00, 1333.63 examples/s]                                                               2023-04-12 00:54:23 - training - INFO - First Test - Val Metrics:{'exact_match': 39.78300180831826, 'f1': 51.749982890494245} Test Metrics: {'exact_match': 39.27927927927928, 'f1': 53.06691851428693}
2023-04-12 00:54:23 - training - INFO - Epoch [1/5][1/415] lr: 1.0e-05, eta: 10:15:19.777982, loss: 4.7401
2023-04-12 00:54:27 - training - INFO - Epoch [1/5][11/415] lr: 9.9e-06, eta: 1:07:11.270640, loss: 2.3758
2023-04-12 00:54:31 - training - INFO - Epoch [1/5][21/415] lr: 9.9e-06, eta: 0:41:01.145934, loss: 1.9435
2023-04-12 00:54:35 - training - INFO - Epoch [1/5][31/415] lr: 9.9e-06, eta: 0:31:42.011496, loss: 2.3194
2023-04-12 00:54:38 - training - INFO - Epoch [1/5][41/415] lr: 9.8e-06, eta: 0:26:53.899674, loss: 2.1799
2023-04-12 00:54:42 - training - INFO - Epoch [1/5][51/415] lr: 9.8e-06, eta: 0:23:57.460992, loss: 2.8984
2023-04-12 00:54:46 - training - INFO - Epoch [1/5][61/415] lr: 9.7e-06, eta: 0:21:57.464142, loss: 1.6554
2023-04-12 00:54:49 - training - INFO - Epoch [1/5][71/415] lr: 9.7e-06, eta: 0:20:30.161412, loss: 2.2388
2023-04-12 00:54:53 - training - INFO - Epoch [1/5][81/415] lr: 9.6e-06, eta: 0:19:23.658520, loss: 2.2765
2023-04-12 00:54:57 - training - INFO - Epoch [1/5][91/415] lr: 9.6e-06, eta: 0:18:31.105472, loss: 1.2577
2023-04-12 00:55:00 - training - INFO - Epoch [1/5][101/415] lr: 9.5e-06, eta: 0:17:48.076128, loss: 2.2777
2023-04-12 00:55:04 - training - INFO - Epoch [1/5][111/415] lr: 9.5e-06, eta: 0:17:12.162524, loss: 1.9517
2023-04-12 00:55:08 - training - INFO - Epoch [1/5][121/415] lr: 9.4e-06, eta: 0:16:41.546148, loss: 1.8533
2023-04-12 00:55:11 - training - INFO - Epoch [1/5][131/415] lr: 9.4e-06, eta: 0:16:15.139560, loss: 2.0671
2023-04-12 00:55:15 - training - INFO - Epoch [1/5][141/415] lr: 9.3e-06, eta: 0:15:51.866450, loss: 1.9876
2023-04-12 00:55:19 - training - INFO - Epoch [1/5][151/415] lr: 9.3e-06, eta: 0:15:31.216000, loss: 1.7035
2023-04-12 00:55:22 - training - INFO - Epoch [1/5][161/415] lr: 9.2e-06, eta: 0:15:12.777030, loss: 1.9722
2023-04-12 00:55:26 - training - INFO - Epoch [1/5][171/415] lr: 9.2e-06, eta: 0:14:56.064288, loss: 1.7128
2023-04-12 00:55:30 - training - INFO - Epoch [1/5][181/415] lr: 9.1e-06, eta: 0:14:40.797124, loss: 1.7019
2023-04-12 00:55:34 - training - INFO - Epoch [1/5][191/415] lr: 9.1e-06, eta: 0:14:26.751156, loss: 1.3959
2023-04-12 00:55:37 - training - INFO - Epoch [1/5][201/415] lr: 9.0e-06, eta: 0:14:13.689456, loss: 1.4022
2023-04-12 00:55:41 - training - INFO - Epoch [1/5][211/415] lr: 9.0e-06, eta: 0:14:01.551264, loss: 1.6392
2023-04-12 00:55:45 - training - INFO - Epoch [1/5][221/415] lr: 8.9e-06, eta: 0:13:50.185974, loss: 1.6860
2023-04-12 00:55:48 - training - INFO - Epoch [1/5][231/415] lr: 8.9e-06, eta: 0:13:39.499416, loss: 1.7706
2023-04-12 00:55:52 - training - INFO - Epoch [1/5][241/415] lr: 8.8e-06, eta: 0:13:29.371710, loss: 2.0945
2023-04-12 00:55:56 - training - INFO - Epoch [1/5][251/415] lr: 8.8e-06, eta: 0:13:19.758336, loss: 1.2538
2023-04-12 00:55:59 - training - INFO - Epoch [1/5][261/415] lr: 8.7e-06, eta: 0:13:10.582922, loss: 1.6771
2023-04-12 00:56:03 - training - INFO - Epoch [1/5][271/415] lr: 8.7e-06, eta: 0:13:01.803088, loss: 1.9785
2023-04-12 00:56:07 - training - INFO - Epoch [1/5][281/415] lr: 8.6e-06, eta: 0:12:53.396988, loss: 1.9340
2023-04-12 00:56:11 - training - INFO - Epoch [1/5][291/415] lr: 8.6e-06, eta: 0:12:45.325296, loss: 1.8204
2023-04-12 00:56:14 - training - INFO - Epoch [1/5][301/415] lr: 8.5e-06, eta: 0:12:37.554768, loss: 1.5417
2023-04-12 00:56:18 - training - INFO - Epoch [1/5][311/415] lr: 8.5e-06, eta: 0:12:30.065148, loss: 2.2525
2023-04-12 00:56:22 - training - INFO - Epoch [1/5][321/415] lr: 8.5e-06, eta: 0:12:22.776904, loss: 0.9965
2023-04-12 00:56:25 - training - INFO - Epoch [1/5][331/415] lr: 8.4e-06, eta: 0:12:15.748256, loss: 1.5027
2023-04-12 00:56:29 - training - INFO - Epoch [1/5][341/415] lr: 8.4e-06, eta: 0:12:08.952792, loss: 1.6963
2023-04-12 00:56:33 - training - INFO - Epoch [1/5][351/415] lr: 8.3e-06, eta: 0:12:02.268076, loss: 1.3959
2023-04-12 00:56:36 - training - INFO - Epoch [1/5][361/415] lr: 8.3e-06, eta: 0:11:55.754402, loss: 1.2955
2023-04-12 00:56:40 - training - INFO - Epoch [1/5][371/415] lr: 8.2e-06, eta: 0:11:49.409280, loss: 1.5611
2023-04-12 00:56:44 - training - INFO - Epoch [1/5][381/415] lr: 8.2e-06, eta: 0:11:43.201422, loss: 1.3333
2023-04-12 00:56:48 - training - INFO - Epoch [1/5][391/415] lr: 8.1e-06, eta: 0:11:37.144004, loss: 1.5672
2023-04-12 00:56:51 - training - INFO - Epoch [1/5][401/415] lr: 8.1e-06, eta: 0:11:31.186230, loss: 2.4219
2023-04-12 00:56:55 - training - INFO - Epoch [1/5][411/415] lr: 8.0e-06, eta: 0:11:25.346688, loss: 1.7345
2023-04-12 00:57:14 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 1.7691, Validation Metrics: {'exact_match': 69.80108499095842, 'f1': 77.04648517221649}, Test Metrics: {'exact_match': 71.89189189189189, 'f1': 79.80749188421828}
2023-04-12 00:57:14 - training - INFO - Epoch [2/5][1/415] lr: 8.0e-06, eta: 4 days, 12:25:31.064144, loss: 1.4559
2023-04-12 00:57:18 - training - INFO - Epoch [2/5][11/415] lr: 7.9e-06, eta: 10:00:06.451104, loss: 0.8679
2023-04-12 00:57:21 - training - INFO - Epoch [2/5][21/415] lr: 7.9e-06, eta: 5:18:50.728006, loss: 1.1638
2023-04-12 00:57:25 - training - INFO - Epoch [2/5][31/415] lr: 7.9e-06, eta: 3:39:00.538740, loss: 1.3712
2023-04-12 00:57:29 - training - INFO - Epoch [2/5][41/415] lr: 7.8e-06, eta: 2:47:50.519094, loss: 0.9312
2023-04-12 00:57:32 - training - INFO - Epoch [2/5][51/415] lr: 7.8e-06, eta: 2:16:42.861128, loss: 1.3903
2023-04-12 00:57:36 - training - INFO - Epoch [2/5][61/415] lr: 7.7e-06, eta: 1:55:46.555876, loss: 1.0481
2023-04-12 00:57:40 - training - INFO - Epoch [2/5][71/415] lr: 7.7e-06, eta: 1:40:43.003884, loss: 0.8753
2023-04-12 00:57:43 - training - INFO - Epoch [2/5][81/415] lr: 7.6e-06, eta: 1:29:21.730408, loss: 1.1732
2023-04-12 00:57:47 - training - INFO - Epoch [2/5][91/415] lr: 7.6e-06, eta: 1:20:29.417088, loss: 1.1494
2023-04-12 00:57:51 - training - INFO - Epoch [2/5][101/415] lr: 7.5e-06, eta: 1:13:21.727848, loss: 0.8850
2023-04-12 00:57:55 - training - INFO - Epoch [2/5][111/415] lr: 7.5e-06, eta: 1:07:30.569312, loss: 1.2759
2023-04-12 00:57:58 - training - INFO - Epoch [2/5][121/415] lr: 7.4e-06, eta: 1:02:36.719366, loss: 1.4462
2023-04-12 00:58:02 - training - INFO - Epoch [2/5][131/415] lr: 7.4e-06, eta: 0:58:27.184008, loss: 1.3017
2023-04-12 00:58:06 - training - INFO - Epoch [2/5][141/415] lr: 7.3e-06, eta: 0:54:52.567310, loss: 1.8251
2023-04-12 00:58:09 - training - INFO - Epoch [2/5][151/415] lr: 7.3e-06, eta: 0:51:45.797760, loss: 0.6968
2023-04-12 00:58:13 - training - INFO - Epoch [2/5][161/415] lr: 7.2e-06, eta: 0:49:01.764408, loss: 0.9920
2023-04-12 00:58:17 - training - INFO - Epoch [2/5][171/415] lr: 7.2e-06, eta: 0:46:36.532368, loss: 0.6961
2023-04-12 00:58:21 - training - INFO - Epoch [2/5][181/415] lr: 7.1e-06, eta: 0:44:26.916778, loss: 1.0301
2023-04-12 00:58:24 - training - INFO - Epoch [2/5][191/415] lr: 7.1e-06, eta: 0:42:30.465000, loss: 1.4999
2023-04-12 00:58:28 - training - INFO - Epoch [2/5][201/415] lr: 7.0e-06, eta: 0:40:45.257042, loss: 0.9935
2023-04-12 00:58:32 - training - INFO - Epoch [2/5][211/415] lr: 7.0e-06, eta: 0:39:09.652152, loss: 1.4369
2023-04-12 00:58:35 - training - INFO - Epoch [2/5][221/415] lr: 6.9e-06, eta: 0:37:42.384288, loss: 1.1441
2023-04-12 00:58:39 - training - INFO - Epoch [2/5][231/415] lr: 6.9e-06, eta: 0:36:22.366624, loss: 0.8190
2023-04-12 00:58:43 - training - INFO - Epoch [2/5][241/415] lr: 6.8e-06, eta: 0:35:08.661674, loss: 0.5426
2023-04-12 00:58:46 - training - INFO - Epoch [2/5][251/415] lr: 6.8e-06, eta: 0:34:00.539808, loss: 0.8865
2023-04-12 00:58:50 - training - INFO - Epoch [2/5][261/415] lr: 6.7e-06, eta: 0:32:57.352514, loss: 1.1651
2023-04-12 00:58:54 - training - INFO - Epoch [2/5][271/415] lr: 6.7e-06, eta: 0:31:58.550392, loss: 1.6114
2023-04-12 00:58:58 - training - INFO - Epoch [2/5][281/415] lr: 6.6e-06, eta: 0:31:03.680754, loss: 1.0981
2023-04-12 00:59:01 - training - INFO - Epoch [2/5][291/415] lr: 6.6e-06, eta: 0:30:12.340624, loss: 1.1920
2023-04-12 00:59:05 - training - INFO - Epoch [2/5][301/415] lr: 6.5e-06, eta: 0:29:24.203972, loss: 0.7933
2023-04-12 00:59:09 - training - INFO - Epoch [2/5][311/415] lr: 6.5e-06, eta: 0:28:38.924508, loss: 0.5351
2023-04-12 00:59:12 - training - INFO - Epoch [2/5][321/415] lr: 6.5e-06, eta: 0:27:56.187298, loss: 1.0502
2023-04-12 00:59:16 - training - INFO - Epoch [2/5][331/415] lr: 6.4e-06, eta: 0:27:15.821424, loss: 1.2615
2023-04-12 00:59:20 - training - INFO - Epoch [2/5][341/415] lr: 6.4e-06, eta: 0:26:37.591422, loss: 0.8370
2023-04-12 00:59:24 - training - INFO - Epoch [2/5][351/415] lr: 6.3e-06, eta: 0:26:01.309568, loss: 1.0269
2023-04-12 00:59:27 - training - INFO - Epoch [2/5][361/415] lr: 6.3e-06, eta: 0:25:26.855196, loss: 1.0298
2023-04-12 00:59:31 - training - INFO - Epoch [2/5][371/415] lr: 6.2e-06, eta: 0:24:54.062088, loss: 1.3484
2023-04-12 00:59:35 - training - INFO - Epoch [2/5][381/415] lr: 6.2e-06, eta: 0:24:22.782552, loss: 1.8840
2023-04-12 00:59:38 - training - INFO - Epoch [2/5][391/415] lr: 6.1e-06, eta: 0:23:52.920652, loss: 1.1281
2023-04-12 00:59:42 - training - INFO - Epoch [2/5][401/415] lr: 6.1e-06, eta: 0:23:24.382212, loss: 0.8526
2023-04-12 00:59:46 - training - INFO - Epoch [2/5][411/415] lr: 6.0e-06, eta: 0:22:57.028224, loss: 1.6058
2023-04-12 01:00:05 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.2151, Validation Metrics: {'exact_match': 73.5985533453888, 'f1': 77.43471957097047}, Test Metrics: {'exact_match': 75.31531531531532, 'f1': 80.21867216604062}
2023-04-12 01:00:05 - training - INFO - Epoch [3/5][1/415] lr: 6.0e-06, eta: 8 days, 14:57:35.773026, loss: 0.7278
2023-04-12 01:00:09 - training - INFO - Epoch [3/5][11/415] lr: 5.9e-06, eta: 18:55:00.206304, loss: 0.8875
2023-04-12 01:00:12 - training - INFO - Epoch [3/5][21/415] lr: 5.9e-06, eta: 9:57:41.081776, loss: 0.8216
2023-04-12 01:00:16 - training - INFO - Epoch [3/5][31/415] lr: 5.9e-06, eta: 6:46:59.398192, loss: 0.6843
2023-04-12 01:00:20 - training - INFO - Epoch [3/5][41/415] lr: 5.8e-06, eta: 5:09:17.255952, loss: 1.5136
2023-04-12 01:00:23 - training - INFO - Epoch [3/5][51/415] lr: 5.8e-06, eta: 4:09:52.231496, loss: 1.1424
2023-04-12 01:00:27 - training - INFO - Epoch [3/5][61/415] lr: 5.7e-06, eta: 3:29:54.933674, loss: 1.1526
2023-04-12 01:00:31 - training - INFO - Epoch [3/5][71/415] lr: 5.7e-06, eta: 3:01:11.862324, loss: 1.3130
2023-04-12 01:00:35 - training - INFO - Epoch [3/5][81/415] lr: 5.6e-06, eta: 2:39:33.321616, loss: 0.9253
2023-04-12 01:00:38 - training - INFO - Epoch [3/5][91/415] lr: 5.6e-06, eta: 2:22:39.404544, loss: 1.0824
2023-04-12 01:00:42 - training - INFO - Epoch [3/5][101/415] lr: 5.5e-06, eta: 2:09:05.516058, loss: 1.2137
2023-04-12 01:00:46 - training - INFO - Epoch [3/5][111/415] lr: 5.5e-06, eta: 1:57:57.572528, loss: 1.3750
2023-04-12 01:00:49 - training - INFO - Epoch [3/5][121/415] lr: 5.4e-06, eta: 1:48:39.347094, loss: 0.6351
2023-04-12 01:00:53 - training - INFO - Epoch [3/5][131/415] lr: 5.4e-06, eta: 1:40:45.890544, loss: 0.9241
2023-04-12 01:00:57 - training - INFO - Epoch [3/5][141/415] lr: 5.3e-06, eta: 1:33:59.021820, loss: 1.1813
2023-04-12 01:01:01 - training - INFO - Epoch [3/5][151/415] lr: 5.3e-06, eta: 1:28:05.537764, loss: 0.6537
2023-04-12 01:01:04 - training - INFO - Epoch [3/5][161/415] lr: 5.2e-06, eta: 1:22:55.525302, loss: 0.9550
2023-04-12 01:01:08 - training - INFO - Epoch [3/5][171/415] lr: 5.2e-06, eta: 1:18:21.322528, loss: 1.4254
2023-04-12 01:01:12 - training - INFO - Epoch [3/5][181/415] lr: 5.1e-06, eta: 1:14:17.000574, loss: 0.6059
2023-04-12 01:01:15 - training - INFO - Epoch [3/5][191/415] lr: 5.1e-06, eta: 1:10:37.912932, loss: 1.0428
2023-04-12 01:01:19 - training - INFO - Epoch [3/5][201/415] lr: 5.0e-06, eta: 1:07:20.267166, loss: 0.8426
2023-04-12 01:01:23 - training - INFO - Epoch [3/5][211/415] lr: 5.0e-06, eta: 1:04:21.026224, loss: 0.5944
2023-04-12 01:01:26 - training - INFO - Epoch [3/5][221/415] lr: 4.9e-06, eta: 1:01:37.647264, loss: 0.9355
2023-04-12 01:01:30 - training - INFO - Epoch [3/5][231/415] lr: 4.9e-06, eta: 0:59:08.071748, loss: 1.4216
2023-04-12 01:01:34 - training - INFO - Epoch [3/5][241/415] lr: 4.8e-06, eta: 0:56:50.645784, loss: 0.6858
2023-04-12 01:01:38 - training - INFO - Epoch [3/5][251/415] lr: 4.8e-06, eta: 0:54:43.840224, loss: 0.6740
2023-04-12 01:01:41 - training - INFO - Epoch [3/5][261/415] lr: 4.7e-06, eta: 0:52:46.451282, loss: 0.6845
2023-04-12 01:01:45 - training - INFO - Epoch [3/5][271/415] lr: 4.7e-06, eta: 0:50:57.478732, loss: 0.9035
2023-04-12 01:01:49 - training - INFO - Epoch [3/5][281/415] lr: 4.6e-06, eta: 0:49:16.011474, loss: 0.7243
2023-04-12 01:01:52 - training - INFO - Epoch [3/5][291/415] lr: 4.6e-06, eta: 0:47:41.234504, loss: 0.7513
2023-04-12 01:01:56 - training - INFO - Epoch [3/5][301/415] lr: 4.5e-06, eta: 0:46:12.506544, loss: 1.2680
2023-04-12 01:02:00 - training - INFO - Epoch [3/5][311/415] lr: 4.5e-06, eta: 0:44:49.293852, loss: 0.7577
2023-04-12 01:02:04 - training - INFO - Epoch [3/5][321/415] lr: 4.5e-06, eta: 0:43:30.993876, loss: 1.4851
2023-04-12 01:02:07 - training - INFO - Epoch [3/5][331/415] lr: 4.4e-06, eta: 0:42:17.188640, loss: 1.2829
2023-04-12 01:02:11 - training - INFO - Epoch [3/5][341/415] lr: 4.4e-06, eta: 0:41:07.518414, loss: 1.2831
2023-04-12 01:02:15 - training - INFO - Epoch [3/5][351/415] lr: 4.3e-06, eta: 0:40:01.607856, loss: 1.3778
2023-04-12 01:02:18 - training - INFO - Epoch [3/5][361/415] lr: 4.3e-06, eta: 0:38:59.143792, loss: 0.9800
2023-04-12 01:02:22 - training - INFO - Epoch [3/5][371/415] lr: 4.2e-06, eta: 0:37:59.839536, loss: 1.1795
2023-04-12 01:02:26 - training - INFO - Epoch [3/5][381/415] lr: 4.2e-06, eta: 0:37:03.471558, loss: 1.1050
2023-04-12 01:02:29 - training - INFO - Epoch [3/5][391/415] lr: 4.1e-06, eta: 0:36:09.796952, loss: 0.5081
2023-04-12 01:02:33 - training - INFO - Epoch [3/5][401/415] lr: 4.1e-06, eta: 0:35:18.602682, loss: 1.3478
2023-04-12 01:02:37 - training - INFO - Epoch [3/5][411/415] lr: 4.0e-06, eta: 0:34:29.701504, loss: 1.2616
2023-04-12 01:02:56 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 1.0147, Validation Metrics: {'exact_match': 75.40687160940325, 'f1': 79.01262611900047}, Test Metrics: {'exact_match': 74.5945945945946, 'f1': 79.0216916687505}
2023-04-12 01:02:56 - training - INFO - Epoch [4/5][1/415] lr: 4.0e-06, eta: 12 days, 17:28:33.620296, loss: 0.4885
2023-04-12 01:03:00 - training - INFO - Epoch [4/5][11/415] lr: 3.9e-06, eta: 1 day, 3:49:46.143072, loss: 1.0993
2023-04-12 01:03:03 - training - INFO - Epoch [4/5][21/415] lr: 3.9e-06, eta: 14:36:26.218386, loss: 0.3889
2023-04-12 01:03:07 - training - INFO - Epoch [4/5][31/415] lr: 3.9e-06, eta: 9:54:53.760844, loss: 1.8418
2023-04-12 01:03:11 - training - INFO - Epoch [4/5][41/415] lr: 3.8e-06, eta: 7:30:39.880062, loss: 1.1764
2023-04-12 01:03:14 - training - INFO - Epoch [4/5][51/415] lr: 3.8e-06, eta: 6:02:58.183328, loss: 0.4770
2023-04-12 01:03:18 - training - INFO - Epoch [4/5][61/415] lr: 3.7e-06, eta: 5:04:00.350892, loss: 0.6680
2023-04-12 01:03:22 - training - INFO - Epoch [4/5][71/415] lr: 3.7e-06, eta: 4:21:38.033400, loss: 0.8960
2023-04-12 01:03:26 - training - INFO - Epoch [4/5][81/415] lr: 3.6e-06, eta: 3:49:42.555916, loss: 0.9534
2023-04-12 01:03:29 - training - INFO - Epoch [4/5][91/415] lr: 3.6e-06, eta: 3:24:47.314752, loss: 0.7843
2023-04-12 01:03:33 - training - INFO - Epoch [4/5][101/415] lr: 3.5e-06, eta: 3:04:47.519772, loss: 0.8477
2023-04-12 01:03:37 - training - INFO - Epoch [4/5][111/415] lr: 3.5e-06, eta: 2:48:23.167556, loss: 1.5142
2023-04-12 01:03:40 - training - INFO - Epoch [4/5][121/415] lr: 3.4e-06, eta: 2:34:40.960696, loss: 1.1116
2023-04-12 01:03:44 - training - INFO - Epoch [4/5][131/415] lr: 3.4e-06, eta: 2:23:03.654240, loss: 0.6927
2023-04-12 01:03:48 - training - INFO - Epoch [4/5][141/415] lr: 3.3e-06, eta: 2:13:04.739476, loss: 1.4122
2023-04-12 01:03:52 - training - INFO - Epoch [4/5][151/415] lr: 3.3e-06, eta: 2:04:24.606292, loss: 1.1296
2023-04-12 01:03:55 - training - INFO - Epoch [4/5][161/415] lr: 3.2e-06, eta: 1:56:48.958902, loss: 0.8241
2023-04-12 01:03:59 - training - INFO - Epoch [4/5][171/415] lr: 3.2e-06, eta: 1:50:05.897536, loss: 1.1724
2023-04-12 01:04:03 - training - INFO - Epoch [4/5][181/415] lr: 3.1e-06, eta: 1:44:07.158236, loss: 0.6886
2023-04-12 01:04:06 - training - INFO - Epoch [4/5][191/415] lr: 3.1e-06, eta: 1:38:45.603900, loss: 0.6933
2023-04-12 01:04:10 - training - INFO - Epoch [4/5][201/415] lr: 3.0e-06, eta: 1:33:55.496548, loss: 1.0925
2023-04-12 01:04:14 - training - INFO - Epoch [4/5][211/415] lr: 3.0e-06, eta: 1:29:32.506544, loss: 0.5646
2023-04-12 01:04:18 - training - INFO - Epoch [4/5][221/415] lr: 2.9e-06, eta: 1:25:32.999232, loss: 1.0435
2023-04-12 01:04:21 - training - INFO - Epoch [4/5][231/415] lr: 2.9e-06, eta: 1:21:53.880136, loss: 0.9580
2023-04-12 01:04:25 - training - INFO - Epoch [4/5][241/415] lr: 2.8e-06, eta: 1:18:32.637230, loss: 0.5138
2023-04-12 01:04:29 - training - INFO - Epoch [4/5][251/415] lr: 2.8e-06, eta: 1:15:27.197184, loss: 0.8010
2023-04-12 01:04:32 - training - INFO - Epoch [4/5][261/415] lr: 2.7e-06, eta: 1:12:35.628052, loss: 0.9071
2023-04-12 01:04:36 - training - INFO - Epoch [4/5][271/415] lr: 2.7e-06, eta: 1:09:56.533352, loss: 1.0883
2023-04-12 01:04:40 - training - INFO - Epoch [4/5][281/415] lr: 2.6e-06, eta: 1:07:28.422924, loss: 1.0928
2023-04-12 01:04:44 - training - INFO - Epoch [4/5][291/415] lr: 2.6e-06, eta: 1:05:10.249696, loss: 1.1149
2023-04-12 01:04:47 - training - INFO - Epoch [4/5][301/415] lr: 2.5e-06, eta: 1:03:01.004256, loss: 0.3941
2023-04-12 01:04:51 - training - INFO - Epoch [4/5][311/415] lr: 2.5e-06, eta: 1:00:59.837832, loss: 0.5705
2023-04-12 01:04:55 - training - INFO - Epoch [4/5][321/415] lr: 2.5e-06, eta: 0:59:05.986378, loss: 1.0415
2023-04-12 01:04:58 - training - INFO - Epoch [4/5][331/415] lr: 2.4e-06, eta: 0:57:18.768624, loss: 0.8224
2023-04-12 01:05:02 - training - INFO - Epoch [4/5][341/415] lr: 2.4e-06, eta: 0:55:37.632678, loss: 0.9941
2023-04-12 01:05:06 - training - INFO - Epoch [4/5][351/415] lr: 2.3e-06, eta: 0:54:02.063028, loss: 0.7448
2023-04-12 01:05:09 - training - INFO - Epoch [4/5][361/415] lr: 2.3e-06, eta: 0:52:31.571222, loss: 0.8009
2023-04-12 01:05:13 - training - INFO - Epoch [4/5][371/415] lr: 2.2e-06, eta: 0:51:05.783976, loss: 0.8902
2023-04-12 01:05:17 - training - INFO - Epoch [4/5][381/415] lr: 2.2e-06, eta: 0:49:44.284226, loss: 0.4835
2023-04-12 01:05:21 - training - INFO - Epoch [4/5][391/415] lr: 2.1e-06, eta: 0:48:26.777660, loss: 1.0551
2023-04-12 01:05:24 - training - INFO - Epoch [4/5][401/415] lr: 2.1e-06, eta: 0:47:12.957072, loss: 1.5551
2023-04-12 01:05:28 - training - INFO - Epoch [4/5][411/415] lr: 2.0e-06, eta: 0:46:02.577792, loss: 0.4539
2023-04-12 01:05:47 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 0.9048, Validation Metrics: {'exact_match': 77.21518987341773, 'f1': 80.9972502636132}, Test Metrics: {'exact_match': 74.41441441441441, 'f1': 78.74035474035476}
2023-04-12 01:05:47 - training - INFO - Epoch [5/5][1/415] lr: 2.0e-06, eta: 16 days, 20:01:39.140932, loss: 0.7641
2023-04-12 01:05:51 - training - INFO - Epoch [5/5][11/415] lr: 1.9e-06, eta: 1 day, 12:44:45.235776, loss: 0.6383
2023-04-12 01:05:54 - training - INFO - Epoch [5/5][21/415] lr: 1.9e-06, eta: 19:15:19.022578, loss: 0.8057
2023-04-12 01:05:58 - training - INFO - Epoch [5/5][31/415] lr: 1.9e-06, eta: 13:02:54.245276, loss: 0.8353
2023-04-12 01:06:02 - training - INFO - Epoch [5/5][41/415] lr: 1.8e-06, eta: 9:52:07.306446, loss: 0.6549
2023-04-12 01:06:06 - training - INFO - Epoch [5/5][51/415] lr: 1.8e-06, eta: 7:56:07.822888, loss: 0.8755
2023-04-12 01:06:09 - training - INFO - Epoch [5/5][61/415] lr: 1.7e-06, eta: 6:38:08.920020, loss: 1.1744
2023-04-12 01:06:13 - training - INFO - Epoch [5/5][71/415] lr: 1.7e-06, eta: 5:42:07.144344, loss: 0.9787
2023-04-12 01:06:17 - training - INFO - Epoch [5/5][81/415] lr: 1.6e-06, eta: 4:59:54.444230, loss: 1.0725
2023-04-12 01:06:20 - training - INFO - Epoch [5/5][91/415] lr: 1.6e-06, eta: 4:26:57.435136, loss: 0.7075
2023-04-12 01:06:24 - training - INFO - Epoch [5/5][101/415] lr: 1.5e-06, eta: 4:00:31.284294, loss: 0.8496
2023-04-12 01:06:28 - training - INFO - Epoch [5/5][111/415] lr: 1.5e-06, eta: 3:38:50.153096, loss: 0.7202
2023-04-12 01:06:31 - training - INFO - Epoch [5/5][121/415] lr: 1.4e-06, eta: 3:20:43.611872, loss: 0.6340
2023-04-12 01:06:35 - training - INFO - Epoch [5/5][131/415] lr: 1.4e-06, eta: 3:05:22.351056, loss: 1.1558
2023-04-12 01:06:39 - training - INFO - Epoch [5/5][141/415] lr: 1.3e-06, eta: 2:52:11.236534, loss: 1.1823
2023-04-12 01:06:43 - training - INFO - Epoch [5/5][151/415] lr: 1.3e-06, eta: 2:40:44.415560, loss: 0.8828
2023-04-12 01:06:46 - training - INFO - Epoch [5/5][161/415] lr: 1.2e-06, eta: 2:30:42.467148, loss: 0.7730
2023-04-12 01:06:50 - training - INFO - Epoch [5/5][171/415] lr: 1.2e-06, eta: 2:21:50.483968, loss: 1.1585
2023-04-12 01:06:54 - training - INFO - Epoch [5/5][181/415] lr: 1.1e-06, eta: 2:13:56.906794, loss: 0.9664
2023-04-12 01:06:57 - training - INFO - Epoch [5/5][191/415] lr: 1.1e-06, eta: 2:06:52.505472, loss: 1.0373
2023-04-12 01:07:01 - training - INFO - Epoch [5/5][201/415] lr: 1.0e-06, eta: 2:00:30.013810, loss: 0.7050
2023-04-12 01:07:05 - training - INFO - Epoch [5/5][211/415] lr: 9.8e-07, eta: 1:54:43.360560, loss: 0.6260
2023-04-12 01:07:09 - training - INFO - Epoch [5/5][221/415] lr: 9.3e-07, eta: 1:49:27.737526, loss: 0.8207
2023-04-12 01:07:12 - training - INFO - Epoch [5/5][231/415] lr: 8.9e-07, eta: 1:44:39.126104, loss: 0.5218
2023-04-12 01:07:16 - training - INFO - Epoch [5/5][241/415] lr: 8.4e-07, eta: 1:40:14.151836, loss: 0.6337
2023-04-12 01:07:20 - training - INFO - Epoch [5/5][251/415] lr: 7.9e-07, eta: 1:36:10.003296, loss: 0.9395
2023-04-12 01:07:23 - training - INFO - Epoch [5/5][261/415] lr: 7.4e-07, eta: 1:32:24.273320, loss: 0.7855
2023-04-12 01:07:27 - training - INFO - Epoch [5/5][271/415] lr: 6.9e-07, eta: 1:28:54.938532, loss: 0.7074
2023-04-12 01:07:31 - training - INFO - Epoch [5/5][281/415] lr: 6.5e-07, eta: 1:25:40.233384, loss: 0.6606
2023-04-12 01:07:35 - training - INFO - Epoch [5/5][291/415] lr: 6.0e-07, eta: 1:22:38.667248, loss: 0.6216
2023-04-12 01:07:38 - training - INFO - Epoch [5/5][301/415] lr: 5.5e-07, eta: 1:19:48.955576, loss: 0.7773
2023-04-12 01:07:42 - training - INFO - Epoch [5/5][311/415] lr: 5.0e-07, eta: 1:17:09.894948, loss: 0.6413
2023-04-12 01:07:46 - training - INFO - Epoch [5/5][321/415] lr: 4.5e-07, eta: 1:14:40.493022, loss: 0.6919
2023-04-12 01:07:49 - training - INFO - Epoch [5/5][331/415] lr: 4.0e-07, eta: 1:12:19.914352, loss: 0.7793
2023-04-12 01:07:53 - training - INFO - Epoch [5/5][341/415] lr: 3.6e-07, eta: 1:10:07.339452, loss: 0.7662
2023-04-12 01:07:57 - training - INFO - Epoch [5/5][351/415] lr: 3.1e-07, eta: 1:08:02.119956, loss: 0.6941
2023-04-12 01:08:01 - training - INFO - Epoch [5/5][361/415] lr: 2.6e-07, eta: 1:06:03.643854, loss: 0.8508
2023-04-12 01:08:04 - training - INFO - Epoch [5/5][371/415] lr: 2.1e-07, eta: 1:04:11.375688, loss: 0.3949
2023-04-12 01:08:08 - training - INFO - Epoch [5/5][381/415] lr: 1.6e-07, eta: 1:02:24.766564, loss: 0.4401
2023-04-12 01:08:12 - training - INFO - Epoch [5/5][391/415] lr: 1.2e-07, eta: 1:00:43.421568, loss: 1.0770
2023-04-12 01:08:15 - training - INFO - Epoch [5/5][401/415] lr: 6.7e-08, eta: 0:59:06.941508, loss: 1.3386
2023-04-12 01:08:19 - training - INFO - Epoch [5/5][411/415] lr: 1.9e-08, eta: 0:57:34.971520, loss: 1.1248
2023-04-12 01:08:38 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 0.8352, Validation Metrics: {'exact_match': 77.39602169981917, 'f1': 80.70411029549464}, Test Metrics: {'exact_match': 74.95495495495496, 'f1': 79.384036015615}
2023-04-12 01:08:46 - training - INFO - Final Test - Train Loss: 0.8352, Test Metrics: {'exact_match': 74.95495495495496, 'f1': 79.384036015615}
