2023-04-12 01:47:58 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-44a167365c0b341b/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'bert-base-uncased'}, 'data': {'task_type': 'list', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 1e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/bert_list_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 535.24it/s]
Map:   0%|          | 0/6878 [00:00<?, ? examples/s]Map:  15%|█▍        | 1000/6878 [00:00<00:03, 1670.76 examples/s]Map:  29%|██▉       | 2000/6878 [00:01<00:02, 1676.07 examples/s]Map:  44%|████▎     | 3000/6878 [00:01<00:02, 1689.08 examples/s]Map:  58%|█████▊    | 4000/6878 [00:02<00:01, 1705.72 examples/s]Map:  73%|███████▎  | 5000/6878 [00:02<00:01, 1687.08 examples/s]Map:  87%|████████▋ | 6000/6878 [00:03<00:00, 1708.23 examples/s]Map: 100%|██████████| 6878/6878 [00:04<00:00, 1575.70 examples/s]                                                                 Map:   0%|          | 0/859 [00:00<?, ? examples/s]Map: 100%|██████████| 859/859 [00:00<00:00, 1232.87 examples/s]                                                               Map:   0%|          | 0/861 [00:00<?, ? examples/s]Map: 100%|██████████| 861/861 [00:00<00:00, 1322.42 examples/s]                                                               Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-12 01:48:38 - training - INFO - First Test - Val Metrics:{'exact_match': 0.0, 'f1': 2.4782061021370154} Test Metrics: {'exact_match': 0.0, 'f1': 2.0979048341590265}
2023-04-12 01:48:39 - training - INFO - Epoch [1/5][1/649] lr: 1.0e-05, eta: 1 day, 0:46:00.966940, loss: 6.0574
2023-04-12 01:48:42 - training - INFO - Epoch [1/5][11/649] lr: 1.0e-05, eta: 2:32:47.630010, loss: 5.5405
2023-04-12 01:48:46 - training - INFO - Epoch [1/5][21/649] lr: 9.9e-06, eta: 1:29:14.428872, loss: 4.8531
2023-04-12 01:48:50 - training - INFO - Epoch [1/5][31/649] lr: 9.9e-06, eta: 1:06:39.353756, loss: 4.5081
2023-04-12 01:48:54 - training - INFO - Epoch [1/5][41/649] lr: 9.9e-06, eta: 0:55:05.669328, loss: 3.4879
2023-04-12 01:48:57 - training - INFO - Epoch [1/5][51/649] lr: 9.8e-06, eta: 0:48:00.821912, loss: 3.7132
2023-04-12 01:49:01 - training - INFO - Epoch [1/5][61/649] lr: 9.8e-06, eta: 0:43:14.208576, loss: 3.7074
2023-04-12 01:49:05 - training - INFO - Epoch [1/5][71/649] lr: 9.8e-06, eta: 0:39:47.213010, loss: 2.9918
2023-04-12 01:49:08 - training - INFO - Epoch [1/5][81/649] lr: 9.8e-06, eta: 0:37:10.778200, loss: 3.5039
2023-04-12 01:49:12 - training - INFO - Epoch [1/5][91/649] lr: 9.7e-06, eta: 0:35:08.231374, loss: 3.0922
2023-04-12 01:49:16 - training - INFO - Epoch [1/5][101/649] lr: 9.7e-06, eta: 0:33:29.044296, loss: 3.7349
2023-04-12 01:49:20 - training - INFO - Epoch [1/5][111/649] lr: 9.7e-06, eta: 0:32:06.820808, loss: 2.7013
2023-04-12 01:49:23 - training - INFO - Epoch [1/5][121/649] lr: 9.6e-06, eta: 0:30:57.864668, loss: 3.6956
2023-04-12 01:49:27 - training - INFO - Epoch [1/5][131/649] lr: 9.6e-06, eta: 0:29:58.764732, loss: 3.0967
2023-04-12 01:49:31 - training - INFO - Epoch [1/5][141/649] lr: 9.6e-06, eta: 0:29:07.449568, loss: 3.5043
2023-04-12 01:49:34 - training - INFO - Epoch [1/5][151/649] lr: 9.5e-06, eta: 0:28:22.516816, loss: 2.5330
2023-04-12 01:49:38 - training - INFO - Epoch [1/5][161/649] lr: 9.5e-06, eta: 0:27:42.868128, loss: 2.2544
2023-04-12 01:49:42 - training - INFO - Epoch [1/5][171/649] lr: 9.5e-06, eta: 0:27:07.280306, loss: 3.7477
2023-04-12 01:49:46 - training - INFO - Epoch [1/5][181/649] lr: 9.4e-06, eta: 0:26:35.109208, loss: 2.3279
2023-04-12 01:49:49 - training - INFO - Epoch [1/5][191/649] lr: 9.4e-06, eta: 0:26:06.075930, loss: 2.7836
2023-04-12 01:49:53 - training - INFO - Epoch [1/5][201/649] lr: 9.4e-06, eta: 0:25:39.414724, loss: 3.2384
2023-04-12 01:49:57 - training - INFO - Epoch [1/5][211/649] lr: 9.3e-06, eta: 0:25:14.897438, loss: 2.0668
2023-04-12 01:50:00 - training - INFO - Epoch [1/5][221/649] lr: 9.3e-06, eta: 0:24:52.301664, loss: 2.3383
2023-04-12 01:50:04 - training - INFO - Epoch [1/5][231/649] lr: 9.3e-06, eta: 0:24:31.428772, loss: 2.8618
2023-04-12 01:50:08 - training - INFO - Epoch [1/5][241/649] lr: 9.3e-06, eta: 0:24:11.848220, loss: 2.4194
2023-04-12 01:50:11 - training - INFO - Epoch [1/5][251/649] lr: 9.2e-06, eta: 0:23:53.608038, loss: 2.8747
2023-04-12 01:50:15 - training - INFO - Epoch [1/5][261/649] lr: 9.2e-06, eta: 0:23:36.468992, loss: 3.2121
2023-04-12 01:50:19 - training - INFO - Epoch [1/5][271/649] lr: 9.2e-06, eta: 0:23:20.337640, loss: 1.9667
2023-04-12 01:50:23 - training - INFO - Epoch [1/5][281/649] lr: 9.1e-06, eta: 0:23:05.068308, loss: 1.6951
2023-04-12 01:50:26 - training - INFO - Epoch [1/5][291/649] lr: 9.1e-06, eta: 0:22:50.567380, loss: 2.3011
2023-04-12 01:50:30 - training - INFO - Epoch [1/5][301/649] lr: 9.1e-06, eta: 0:22:36.798336, loss: 2.7339
2023-04-12 01:50:34 - training - INFO - Epoch [1/5][311/649] lr: 9.0e-06, eta: 0:22:24.012588, loss: 2.3527
2023-04-12 01:50:38 - training - INFO - Epoch [1/5][321/649] lr: 9.0e-06, eta: 0:22:12.136388, loss: 2.0646
2023-04-12 01:50:41 - training - INFO - Epoch [1/5][331/649] lr: 9.0e-06, eta: 0:22:00.575262, loss: 2.8586
2023-04-12 01:50:45 - training - INFO - Epoch [1/5][341/649] lr: 8.9e-06, eta: 0:21:49.352616, loss: 2.2501
2023-04-12 01:50:49 - training - INFO - Epoch [1/5][351/649] lr: 8.9e-06, eta: 0:21:38.769320, loss: 1.8596
2023-04-12 01:50:53 - training - INFO - Epoch [1/5][361/649] lr: 8.9e-06, eta: 0:21:28.597156, loss: 2.7205
2023-04-12 01:50:56 - training - INFO - Epoch [1/5][371/649] lr: 8.9e-06, eta: 0:21:18.763308, loss: 2.0503
2023-04-12 01:51:00 - training - INFO - Epoch [1/5][381/649] lr: 8.8e-06, eta: 0:21:08.952480, loss: 2.1750
2023-04-12 01:51:04 - training - INFO - Epoch [1/5][391/649] lr: 8.8e-06, eta: 0:20:59.775578, loss: 2.3062
2023-04-12 01:51:08 - training - INFO - Epoch [1/5][401/649] lr: 8.8e-06, eta: 0:20:50.799732, loss: 2.7654
2023-04-12 01:51:11 - training - INFO - Epoch [1/5][411/649] lr: 8.7e-06, eta: 0:20:41.697262, loss: 2.9726
2023-04-12 01:51:15 - training - INFO - Epoch [1/5][421/649] lr: 8.7e-06, eta: 0:20:32.828496, loss: 2.1773
2023-04-12 01:51:19 - training - INFO - Epoch [1/5][431/649] lr: 8.7e-06, eta: 0:20:24.208188, loss: 2.6747
2023-04-12 01:51:22 - training - INFO - Epoch [1/5][441/649] lr: 8.6e-06, eta: 0:20:15.786360, loss: 2.1219
2023-04-12 01:51:26 - training - INFO - Epoch [1/5][451/649] lr: 8.6e-06, eta: 0:20:07.619886, loss: 2.6822
2023-04-12 01:51:30 - training - INFO - Epoch [1/5][461/649] lr: 8.6e-06, eta: 0:19:59.608896, loss: 2.5144
2023-04-12 01:51:34 - training - INFO - Epoch [1/5][471/649] lr: 8.5e-06, eta: 0:19:51.829682, loss: 2.0347
2023-04-12 01:51:37 - training - INFO - Epoch [1/5][481/649] lr: 8.5e-06, eta: 0:19:44.163936, loss: 2.0692
2023-04-12 01:51:41 - training - INFO - Epoch [1/5][491/649] lr: 8.5e-06, eta: 0:19:36.674040, loss: 2.2239
2023-04-12 01:51:45 - training - INFO - Epoch [1/5][501/649] lr: 8.5e-06, eta: 0:19:29.336392, loss: 2.7508
2023-04-12 01:51:48 - training - INFO - Epoch [1/5][511/649] lr: 8.4e-06, eta: 0:19:22.138646, loss: 2.2797
2023-04-12 01:51:52 - training - INFO - Epoch [1/5][521/649] lr: 8.4e-06, eta: 0:19:15.041376, loss: 1.3746
2023-04-12 01:51:56 - training - INFO - Epoch [1/5][531/649] lr: 8.4e-06, eta: 0:19:08.073566, loss: 1.9887
2023-04-12 01:52:00 - training - INFO - Epoch [1/5][541/649] lr: 8.3e-06, eta: 0:19:01.277280, loss: 2.6239
2023-04-12 01:52:03 - training - INFO - Epoch [1/5][551/649] lr: 8.3e-06, eta: 0:18:54.580794, loss: 2.8537
2023-04-12 01:52:07 - training - INFO - Epoch [1/5][561/649] lr: 8.3e-06, eta: 0:18:47.983208, loss: 2.4160
2023-04-12 01:52:11 - training - INFO - Epoch [1/5][571/649] lr: 8.2e-06, eta: 0:18:41.475600, loss: 2.3254
2023-04-12 01:52:14 - training - INFO - Epoch [1/5][581/649] lr: 8.2e-06, eta: 0:18:35.054496, loss: 1.3333
2023-04-12 01:52:18 - training - INFO - Epoch [1/5][591/649] lr: 8.2e-06, eta: 0:18:28.735040, loss: 3.1025
2023-04-12 01:52:22 - training - INFO - Epoch [1/5][601/649] lr: 8.1e-06, eta: 0:18:22.508340, loss: 2.2700
2023-04-12 01:52:26 - training - INFO - Epoch [1/5][611/649] lr: 8.1e-06, eta: 0:18:16.362990, loss: 1.7182
2023-04-12 01:52:29 - training - INFO - Epoch [1/5][621/649] lr: 8.1e-06, eta: 0:18:10.282496, loss: 2.0926
2023-04-12 01:52:33 - training - INFO - Epoch [1/5][631/649] lr: 8.1e-06, eta: 0:18:04.263674, loss: 2.7177
2023-04-12 01:52:37 - training - INFO - Epoch [1/5][641/649] lr: 8.0e-06, eta: 0:17:58.347648, loss: 2.4144
2023-04-12 01:53:07 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 2.7653, Validation Metrics: {'exact_match': 26.542491268917345, 'f1': 34.21223708459208}, Test Metrics: {'exact_match': 29.965156794425088, 'f1': 36.98247402458272}
2023-04-12 01:53:07 - training - INFO - Epoch [2/5][1/649] lr: 8.0e-06, eta: 11 days, 2:33:07.361756, loss: 1.6363
2023-04-12 01:53:11 - training - INFO - Epoch [2/5][11/649] lr: 8.0e-06, eta: 1 day, 0:27:34.203930, loss: 2.1217
2023-04-12 01:53:14 - training - INFO - Epoch [2/5][21/649] lr: 7.9e-06, eta: 12:55:49.836840, loss: 1.6939
2023-04-12 01:53:18 - training - INFO - Epoch [2/5][31/649] lr: 7.9e-06, eta: 8:50:20.104152, loss: 1.9455
2023-04-12 01:53:22 - training - INFO - Epoch [2/5][41/649] lr: 7.9e-06, eta: 6:44:33.991008, loss: 1.7880
2023-04-12 01:53:26 - training - INFO - Epoch [2/5][51/649] lr: 7.8e-06, eta: 5:28:05.538678, loss: 1.3228
2023-04-12 01:53:29 - training - INFO - Epoch [2/5][61/649] lr: 7.8e-06, eta: 4:36:40.449456, loss: 1.7119
2023-04-12 01:53:33 - training - INFO - Epoch [2/5][71/649] lr: 7.8e-06, eta: 3:59:43.276182, loss: 3.1467
2023-04-12 01:53:37 - training - INFO - Epoch [2/5][81/649] lr: 7.8e-06, eta: 3:31:53.056412, loss: 2.5435
2023-04-12 01:53:40 - training - INFO - Epoch [2/5][91/649] lr: 7.7e-06, eta: 3:10:09.046204, loss: 1.5946
2023-04-12 01:53:44 - training - INFO - Epoch [2/5][101/649] lr: 7.7e-06, eta: 2:52:42.246720, loss: 1.8837
2023-04-12 01:53:48 - training - INFO - Epoch [2/5][111/649] lr: 7.7e-06, eta: 2:38:23.403704, loss: 1.8302
2023-04-12 01:53:52 - training - INFO - Epoch [2/5][121/649] lr: 7.6e-06, eta: 2:26:26.037568, loss: 1.3965
2023-04-12 01:53:55 - training - INFO - Epoch [2/5][131/649] lr: 7.6e-06, eta: 2:16:17.747022, loss: 1.9185
2023-04-12 01:53:59 - training - INFO - Epoch [2/5][141/649] lr: 7.6e-06, eta: 2:07:35.143776, loss: 1.8564
2023-04-12 01:54:03 - training - INFO - Epoch [2/5][151/649] lr: 7.5e-06, eta: 2:00:01.235496, loss: 1.6855
2023-04-12 01:54:06 - training - INFO - Epoch [2/5][161/649] lr: 7.5e-06, eta: 1:53:23.223816, loss: 2.1774
2023-04-12 01:54:10 - training - INFO - Epoch [2/5][171/649] lr: 7.5e-06, eta: 1:47:31.966342, loss: 1.6660
2023-04-12 01:54:14 - training - INFO - Epoch [2/5][181/649] lr: 7.4e-06, eta: 1:42:18.586120, loss: 2.0489
2023-04-12 01:54:18 - training - INFO - Epoch [2/5][191/649] lr: 7.4e-06, eta: 1:37:37.602540, loss: 1.6862
2023-04-12 01:54:21 - training - INFO - Epoch [2/5][201/649] lr: 7.4e-06, eta: 1:33:24.165332, loss: 1.9183
2023-04-12 01:54:25 - training - INFO - Epoch [2/5][211/649] lr: 7.3e-06, eta: 1:29:34.509518, loss: 1.8185
2023-04-12 01:54:29 - training - INFO - Epoch [2/5][221/649] lr: 7.3e-06, eta: 1:26:05.267184, loss: 2.5734
2023-04-12 01:54:32 - training - INFO - Epoch [2/5][231/649] lr: 7.3e-06, eta: 1:22:53.847472, loss: 1.8335
2023-04-12 01:54:36 - training - INFO - Epoch [2/5][241/649] lr: 7.3e-06, eta: 1:19:58.196076, loss: 1.8724
2023-04-12 01:54:40 - training - INFO - Epoch [2/5][251/649] lr: 7.2e-06, eta: 1:17:17.014386, loss: 1.3780
2023-04-12 01:54:44 - training - INFO - Epoch [2/5][261/649] lr: 7.2e-06, eta: 1:14:47.637600, loss: 1.8678
2023-04-12 01:54:48 - training - INFO - Epoch [2/5][271/649] lr: 7.2e-06, eta: 1:12:28.663098, loss: 1.7892
2023-04-12 01:54:51 - training - INFO - Epoch [2/5][281/649] lr: 7.1e-06, eta: 1:10:18.874608, loss: 1.8245
2023-04-12 01:54:55 - training - INFO - Epoch [2/5][291/649] lr: 7.1e-06, eta: 1:08:17.735628, loss: 2.6305
2023-04-12 01:54:59 - training - INFO - Epoch [2/5][301/649] lr: 7.1e-06, eta: 1:06:24.477312, loss: 2.5264
2023-04-12 01:55:02 - training - INFO - Epoch [2/5][311/649] lr: 7.0e-06, eta: 1:04:38.272692, loss: 2.3643
2023-04-12 01:55:06 - training - INFO - Epoch [2/5][321/649] lr: 7.0e-06, eta: 1:02:58.445432, loss: 2.0023
2023-04-12 01:55:10 - training - INFO - Epoch [2/5][331/649] lr: 7.0e-06, eta: 1:01:25.012346, loss: 1.6503
2023-04-12 01:55:14 - training - INFO - Epoch [2/5][341/649] lr: 6.9e-06, eta: 0:59:56.876976, loss: 2.0864
2023-04-12 01:55:17 - training - INFO - Epoch [2/5][351/649] lr: 6.9e-06, eta: 0:58:33.634340, loss: 1.8645
2023-04-12 01:55:21 - training - INFO - Epoch [2/5][361/649] lr: 6.9e-06, eta: 0:57:14.682496, loss: 1.4252
2023-04-12 01:55:25 - training - INFO - Epoch [2/5][371/649] lr: 6.9e-06, eta: 0:55:59.769228, loss: 2.4410
2023-04-12 01:55:29 - training - INFO - Epoch [2/5][381/649] lr: 6.8e-06, eta: 0:54:48.636688, loss: 1.8005
2023-04-12 01:55:33 - training - INFO - Epoch [2/5][391/649] lr: 6.8e-06, eta: 0:53:40.921656, loss: 1.9677
2023-04-12 01:55:36 - training - INFO - Epoch [2/5][401/649] lr: 6.8e-06, eta: 0:52:36.416244, loss: 1.4484
2023-04-12 01:55:40 - training - INFO - Epoch [2/5][411/649] lr: 6.7e-06, eta: 0:51:34.835692, loss: 2.2276
2023-04-12 01:55:44 - training - INFO - Epoch [2/5][421/649] lr: 6.7e-06, eta: 0:50:36.037216, loss: 2.1620
2023-04-12 01:55:48 - training - INFO - Epoch [2/5][431/649] lr: 6.7e-06, eta: 0:49:39.781182, loss: 1.6386
2023-04-12 01:55:51 - training - INFO - Epoch [2/5][441/649] lr: 6.6e-06, eta: 0:48:45.898292, loss: 1.5418
2023-04-12 01:55:55 - training - INFO - Epoch [2/5][451/649] lr: 6.6e-06, eta: 0:47:54.369410, loss: 1.8929
2023-04-12 01:55:59 - training - INFO - Epoch [2/5][461/649] lr: 6.6e-06, eta: 0:47:04.766112, loss: 1.8517
2023-04-12 01:56:03 - training - INFO - Epoch [2/5][471/649] lr: 6.5e-06, eta: 0:46:17.104106, loss: 1.7988
2023-04-12 01:56:07 - training - INFO - Epoch [2/5][481/649] lr: 6.5e-06, eta: 0:45:31.326756, loss: 1.9273
2023-04-12 01:56:10 - training - INFO - Epoch [2/5][491/649] lr: 6.5e-06, eta: 0:44:47.306382, loss: 1.6194
2023-04-12 01:56:14 - training - INFO - Epoch [2/5][501/649] lr: 6.5e-06, eta: 0:44:04.815376, loss: 2.4269
2023-04-12 01:56:18 - training - INFO - Epoch [2/5][511/649] lr: 6.4e-06, eta: 0:43:23.817856, loss: 1.8112
2023-04-12 01:56:22 - training - INFO - Epoch [2/5][521/649] lr: 6.4e-06, eta: 0:42:44.275536, loss: 1.3827
2023-04-12 01:56:26 - training - INFO - Epoch [2/5][531/649] lr: 6.4e-06, eta: 0:42:06.036502, loss: 1.6939
2023-04-12 01:56:29 - training - INFO - Epoch [2/5][541/649] lr: 6.3e-06, eta: 0:41:29.088784, loss: 1.8390
2023-04-12 01:56:33 - training - INFO - Epoch [2/5][551/649] lr: 6.3e-06, eta: 0:40:53.380002, loss: 2.5841
2023-04-12 01:56:37 - training - INFO - Epoch [2/5][561/649] lr: 6.3e-06, eta: 0:40:18.802012, loss: 1.3646
2023-04-12 01:56:41 - training - INFO - Epoch [2/5][571/649] lr: 6.2e-06, eta: 0:39:44.983384, loss: 1.6442
2023-04-12 01:56:44 - training - INFO - Epoch [2/5][581/649] lr: 6.2e-06, eta: 0:39:12.434544, loss: 2.1520
2023-04-12 01:56:48 - training - INFO - Epoch [2/5][591/649] lr: 6.2e-06, eta: 0:38:40.938924, loss: 1.9062
2023-04-12 01:56:52 - training - INFO - Epoch [2/5][601/649] lr: 6.1e-06, eta: 0:38:10.375576, loss: 2.0062
2023-04-12 01:56:56 - training - INFO - Epoch [2/5][611/649] lr: 6.1e-06, eta: 0:37:40.432950, loss: 1.5456
2023-04-12 01:56:59 - training - INFO - Epoch [2/5][621/649] lr: 6.1e-06, eta: 0:37:11.265920, loss: 1.4390
2023-04-12 01:57:03 - training - INFO - Epoch [2/5][631/649] lr: 6.1e-06, eta: 0:36:42.974640, loss: 2.6471
2023-04-12 01:57:07 - training - INFO - Epoch [2/5][641/649] lr: 6.0e-06, eta: 0:36:15.530028, loss: 1.7643
2023-04-12 01:57:37 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.9971, Validation Metrics: {'exact_match': 29.569266589057044, 'f1': 37.455318710455835}, Test Metrics: {'exact_match': 33.56562137049942, 'f1': 41.128818426643186}
2023-04-12 01:57:37 - training - INFO - Epoch [3/5][1/649] lr: 6.0e-06, eta: 21 days, 5:59:12.401392, loss: 2.0460
2023-04-12 01:57:41 - training - INFO - Epoch [3/5][11/649] lr: 6.0e-06, eta: 1 day, 22:31:19.064214, loss: 1.3594
2023-04-12 01:57:45 - training - INFO - Epoch [3/5][21/649] lr: 5.9e-06, eta: 1 day, 0:27:05.607072, loss: 1.6859
2023-04-12 01:57:48 - training - INFO - Epoch [3/5][31/649] lr: 5.9e-06, eta: 16:37:16.420020, loss: 1.5446
2023-04-12 01:57:52 - training - INFO - Epoch [3/5][41/649] lr: 5.9e-06, eta: 12:36:34.377732, loss: 1.3193
2023-04-12 01:57:56 - training - INFO - Epoch [3/5][51/649] lr: 5.8e-06, eta: 10:10:12.093768, loss: 1.5677
2023-04-12 01:58:00 - training - INFO - Epoch [3/5][61/649] lr: 5.8e-06, eta: 8:31:47.909696, loss: 2.0882
2023-04-12 01:58:03 - training - INFO - Epoch [3/5][71/649] lr: 5.8e-06, eta: 7:21:09.287340, loss: 1.8998
2023-04-12 01:58:07 - training - INFO - Epoch [3/5][81/649] lr: 5.8e-06, eta: 6:27:56.367828, loss: 1.4546
2023-04-12 01:58:11 - training - INFO - Epoch [3/5][91/649] lr: 5.7e-06, eta: 5:46:23.226228, loss: 1.4141
2023-04-12 01:58:15 - training - INFO - Epoch [3/5][101/649] lr: 5.7e-06, eta: 5:13:01.894440, loss: 2.1014
2023-04-12 01:58:18 - training - INFO - Epoch [3/5][111/649] lr: 5.7e-06, eta: 4:45:40.585624, loss: 1.9699
2023-04-12 01:58:22 - training - INFO - Epoch [3/5][121/649] lr: 5.6e-06, eta: 4:22:50.729876, loss: 1.6351
2023-04-12 01:58:26 - training - INFO - Epoch [3/5][131/649] lr: 5.6e-06, eta: 4:03:30.059676, loss: 1.7325
2023-04-12 01:58:30 - training - INFO - Epoch [3/5][141/649] lr: 5.6e-06, eta: 3:46:53.523200, loss: 2.4299
2023-04-12 01:58:33 - training - INFO - Epoch [3/5][151/649] lr: 5.5e-06, eta: 3:32:28.421686, loss: 2.0545
2023-04-12 01:58:37 - training - INFO - Epoch [3/5][161/649] lr: 5.5e-06, eta: 3:19:49.762404, loss: 1.4039
2023-04-12 01:58:41 - training - INFO - Epoch [3/5][171/649] lr: 5.5e-06, eta: 3:08:38.858398, loss: 1.6315
2023-04-12 01:58:45 - training - INFO - Epoch [3/5][181/649] lr: 5.4e-06, eta: 2:58:41.668296, loss: 1.6714
2023-04-12 01:58:48 - training - INFO - Epoch [3/5][191/649] lr: 5.4e-06, eta: 2:49:46.555920, loss: 1.4876
2023-04-12 01:58:52 - training - INFO - Epoch [3/5][201/649] lr: 5.4e-06, eta: 2:41:44.323748, loss: 1.4016
2023-04-12 01:58:56 - training - INFO - Epoch [3/5][211/649] lr: 5.3e-06, eta: 2:34:27.346932, loss: 1.8253
2023-04-12 01:58:59 - training - INFO - Epoch [3/5][221/649] lr: 5.3e-06, eta: 2:27:49.582512, loss: 1.5088
2023-04-12 01:59:03 - training - INFO - Epoch [3/5][231/649] lr: 5.3e-06, eta: 2:21:45.978184, loss: 1.9199
2023-04-12 01:59:07 - training - INFO - Epoch [3/5][241/649] lr: 5.3e-06, eta: 2:16:12.315912, loss: 2.3622
2023-04-12 01:59:11 - training - INFO - Epoch [3/5][251/649] lr: 5.2e-06, eta: 2:11:05.091294, loss: 2.2547
2023-04-12 01:59:14 - training - INFO - Epoch [3/5][261/649] lr: 5.2e-06, eta: 2:06:21.785992, loss: 1.4177
2023-04-12 01:59:18 - training - INFO - Epoch [3/5][271/649] lr: 5.2e-06, eta: 2:01:59.005078, loss: 1.4735
2023-04-12 01:59:22 - training - INFO - Epoch [3/5][281/649] lr: 5.1e-06, eta: 1:57:54.747888, loss: 1.2763
2023-04-12 01:59:26 - training - INFO - Epoch [3/5][291/649] lr: 5.1e-06, eta: 1:54:07.357230, loss: 1.1635
2023-04-12 01:59:30 - training - INFO - Epoch [3/5][301/649] lr: 5.1e-06, eta: 1:50:34.483584, loss: 1.8441
2023-04-12 01:59:33 - training - INFO - Epoch [3/5][311/649] lr: 5.0e-06, eta: 1:47:14.394030, loss: 1.5412
2023-04-12 01:59:37 - training - INFO - Epoch [3/5][321/649] lr: 5.0e-06, eta: 1:44:06.547048, loss: 1.4105
2023-04-12 01:59:41 - training - INFO - Epoch [3/5][331/649] lr: 5.0e-06, eta: 1:41:09.786236, loss: 1.5000
2023-04-12 01:59:44 - training - INFO - Epoch [3/5][341/649] lr: 4.9e-06, eta: 1:38:23.219256, loss: 1.8180
2023-04-12 01:59:48 - training - INFO - Epoch [3/5][351/649] lr: 4.9e-06, eta: 1:35:46.557920, loss: 1.4554
2023-04-12 01:59:52 - training - INFO - Epoch [3/5][361/649] lr: 4.9e-06, eta: 1:33:18.397728, loss: 1.2569
2023-04-12 01:59:56 - training - INFO - Epoch [3/5][371/649] lr: 4.9e-06, eta: 1:30:57.901314, loss: 1.5396
2023-04-12 02:00:00 - training - INFO - Epoch [3/5][381/649] lr: 4.8e-06, eta: 1:28:44.625648, loss: 1.4689
2023-04-12 02:00:03 - training - INFO - Epoch [3/5][391/649] lr: 4.8e-06, eta: 1:26:37.995908, loss: 1.2777
2023-04-12 02:00:07 - training - INFO - Epoch [3/5][401/649] lr: 4.8e-06, eta: 1:24:37.489896, loss: 1.5380
2023-04-12 02:00:11 - training - INFO - Epoch [3/5][411/649] lr: 4.7e-06, eta: 1:22:42.784606, loss: 1.3518
2023-04-12 02:00:15 - training - INFO - Epoch [3/5][421/649] lr: 4.7e-06, eta: 1:20:53.221912, loss: 1.3920
2023-04-12 02:00:19 - training - INFO - Epoch [3/5][431/649] lr: 4.7e-06, eta: 1:19:08.549022, loss: 1.4784
2023-04-12 02:00:22 - training - INFO - Epoch [3/5][441/649] lr: 4.6e-06, eta: 1:17:28.602988, loss: 1.7641
2023-04-12 02:00:26 - training - INFO - Epoch [3/5][451/649] lr: 4.6e-06, eta: 1:15:52.496102, loss: 1.6259
2023-04-12 02:00:30 - training - INFO - Epoch [3/5][461/649] lr: 4.6e-06, eta: 1:14:20.226912, loss: 1.9289
2023-04-12 02:00:34 - training - INFO - Epoch [3/5][471/649] lr: 4.5e-06, eta: 1:12:51.962700, loss: 1.8357
2023-04-12 02:00:37 - training - INFO - Epoch [3/5][481/649] lr: 4.5e-06, eta: 1:11:27.392420, loss: 1.9211
2023-04-12 02:00:41 - training - INFO - Epoch [3/5][491/649] lr: 4.5e-06, eta: 1:10:06.167676, loss: 1.7423
2023-04-12 02:00:45 - training - INFO - Epoch [3/5][501/649] lr: 4.5e-06, eta: 1:08:47.782736, loss: 1.2199
2023-04-12 02:00:49 - training - INFO - Epoch [3/5][511/649] lr: 4.4e-06, eta: 1:07:32.515244, loss: 1.5535
2023-04-12 02:00:53 - training - INFO - Epoch [3/5][521/649] lr: 4.4e-06, eta: 1:06:20.028228, loss: 2.0214
2023-04-12 02:00:56 - training - INFO - Epoch [3/5][531/649] lr: 4.4e-06, eta: 1:05:10.162932, loss: 1.7123
2023-04-12 02:01:00 - training - INFO - Epoch [3/5][541/649] lr: 4.3e-06, eta: 1:04:02.627360, loss: 1.2177
2023-04-12 02:01:04 - training - INFO - Epoch [3/5][551/649] lr: 4.3e-06, eta: 1:02:57.252012, loss: 1.7900
2023-04-12 02:01:08 - training - INFO - Epoch [3/5][561/649] lr: 4.3e-06, eta: 1:01:53.931320, loss: 1.5872
2023-04-12 02:01:11 - training - INFO - Epoch [3/5][571/649] lr: 4.2e-06, eta: 1:00:52.713414, loss: 1.7154
2023-04-12 02:01:15 - training - INFO - Epoch [3/5][581/649] lr: 4.2e-06, eta: 0:59:53.421648, loss: 1.4012
2023-04-12 02:01:19 - training - INFO - Epoch [3/5][591/649] lr: 4.2e-06, eta: 0:58:56.048938, loss: 1.8102
2023-04-12 02:01:22 - training - INFO - Epoch [3/5][601/649] lr: 4.1e-06, eta: 0:58:00.543092, loss: 1.4126
2023-04-12 02:01:26 - training - INFO - Epoch [3/5][611/649] lr: 4.1e-06, eta: 0:57:06.973602, loss: 1.4578
2023-04-12 02:01:30 - training - INFO - Epoch [3/5][621/649] lr: 4.1e-06, eta: 0:56:14.965184, loss: 1.0737
2023-04-12 02:01:34 - training - INFO - Epoch [3/5][631/649] lr: 4.1e-06, eta: 0:55:24.200274, loss: 2.3824
2023-04-12 02:01:37 - training - INFO - Epoch [3/5][641/649] lr: 4.0e-06, eta: 0:54:34.889352, loss: 1.5715
2023-04-12 02:02:08 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 1.7059, Validation Metrics: {'exact_match': 32.71245634458673, 'f1': 40.044749037544584}, Test Metrics: {'exact_match': 34.959349593495936, 'f1': 42.30984588149724}
2023-04-12 02:02:08 - training - INFO - Epoch [4/5][1/649] lr: 4.0e-06, eta: 31 days, 9:53:07.526840, loss: 1.7168
2023-04-12 02:02:12 - training - INFO - Epoch [4/5][11/649] lr: 4.0e-06, eta: 2 days, 20:37:31.770042, loss: 0.8810
2023-04-12 02:02:15 - training - INFO - Epoch [4/5][21/649] lr: 3.9e-06, eta: 1 day, 11:59:36.029024, loss: 1.6418
2023-04-12 02:02:19 - training - INFO - Epoch [4/5][31/649] lr: 3.9e-06, eta: 1 day, 0:24:49.015520, loss: 1.2420
2023-04-12 02:02:23 - training - INFO - Epoch [4/5][41/649] lr: 3.9e-06, eta: 18:28:55.747452, loss: 1.3123
2023-04-12 02:02:26 - training - INFO - Epoch [4/5][51/649] lr: 3.8e-06, eta: 14:52:39.371530, loss: 1.2298
2023-04-12 02:02:30 - training - INFO - Epoch [4/5][61/649] lr: 3.8e-06, eta: 12:27:16.358864, loss: 2.1049
2023-04-12 02:02:34 - training - INFO - Epoch [4/5][71/649] lr: 3.8e-06, eta: 10:42:49.051440, loss: 0.9081
2023-04-12 02:02:38 - training - INFO - Epoch [4/5][81/649] lr: 3.8e-06, eta: 9:24:06.516648, loss: 1.5271
2023-04-12 02:02:41 - training - INFO - Epoch [4/5][91/649] lr: 3.7e-06, eta: 8:22:40.796802, loss: 1.4911
2023-04-12 02:02:45 - training - INFO - Epoch [4/5][101/649] lr: 3.7e-06, eta: 7:33:23.667504, loss: 1.2176
2023-04-12 02:02:49 - training - INFO - Epoch [4/5][111/649] lr: 3.7e-06, eta: 6:52:58.829970, loss: 1.3435
2023-04-12 02:02:53 - training - INFO - Epoch [4/5][121/649] lr: 3.6e-06, eta: 6:19:14.072616, loss: 1.7308
2023-04-12 02:02:56 - training - INFO - Epoch [4/5][131/649] lr: 3.6e-06, eta: 5:50:37.972248, loss: 1.7397
2023-04-12 02:03:00 - training - INFO - Epoch [4/5][141/649] lr: 3.6e-06, eta: 5:26:04.735488, loss: 1.1772
2023-04-12 02:03:04 - training - INFO - Epoch [4/5][151/649] lr: 3.5e-06, eta: 5:04:45.997912, loss: 1.7773
2023-04-12 02:03:07 - training - INFO - Epoch [4/5][161/649] lr: 3.5e-06, eta: 4:46:05.642688, loss: 1.6489
2023-04-12 02:03:11 - training - INFO - Epoch [4/5][171/649] lr: 3.5e-06, eta: 4:29:35.962838, loss: 1.4417
2023-04-12 02:03:15 - training - INFO - Epoch [4/5][181/649] lr: 3.4e-06, eta: 4:14:55.169344, loss: 1.4912
2023-04-12 02:03:19 - training - INFO - Epoch [4/5][191/649] lr: 3.4e-06, eta: 4:01:46.219032, loss: 1.3184
2023-04-12 02:03:22 - training - INFO - Epoch [4/5][201/649] lr: 3.4e-06, eta: 3:49:55.438440, loss: 1.3908
2023-04-12 02:03:26 - training - INFO - Epoch [4/5][211/649] lr: 3.3e-06, eta: 3:39:11.673976, loss: 2.1839
2023-04-12 02:03:30 - training - INFO - Epoch [4/5][221/649] lr: 3.3e-06, eta: 3:29:25.941696, loss: 1.9949
2023-04-12 02:03:33 - training - INFO - Epoch [4/5][231/649] lr: 3.3e-06, eta: 3:20:30.628148, loss: 1.9251
2023-04-12 02:03:37 - training - INFO - Epoch [4/5][241/649] lr: 3.3e-06, eta: 3:12:19.448444, loss: 1.5156
2023-04-12 02:03:41 - training - INFO - Epoch [4/5][251/649] lr: 3.2e-06, eta: 3:04:46.979604, loss: 1.5777
2023-04-12 02:03:44 - training - INFO - Epoch [4/5][261/649] lr: 3.2e-06, eta: 2:57:48.951824, loss: 1.5619
2023-04-12 02:03:48 - training - INFO - Epoch [4/5][271/649] lr: 3.2e-06, eta: 2:51:21.519490, loss: 1.5013
2023-04-12 02:03:52 - training - INFO - Epoch [4/5][281/649] lr: 3.1e-06, eta: 2:45:21.367560, loss: 1.2967
2023-04-12 02:03:56 - training - INFO - Epoch [4/5][291/649] lr: 3.1e-06, eta: 2:39:45.803850, loss: 2.1161
2023-04-12 02:03:59 - training - INFO - Epoch [4/5][301/649] lr: 3.1e-06, eta: 2:34:32.242816, loss: 1.3183
2023-04-12 02:04:03 - training - INFO - Epoch [4/5][311/649] lr: 3.0e-06, eta: 2:29:38.606262, loss: 1.4959
2023-04-12 02:04:07 - training - INFO - Epoch [4/5][321/649] lr: 3.0e-06, eta: 2:25:03.063776, loss: 1.1325
2023-04-12 02:04:10 - training - INFO - Epoch [4/5][331/649] lr: 3.0e-06, eta: 2:20:43.897800, loss: 1.4356
2023-04-12 02:04:14 - training - INFO - Epoch [4/5][341/649] lr: 2.9e-06, eta: 2:16:39.711168, loss: 1.5134
2023-04-12 02:04:18 - training - INFO - Epoch [4/5][351/649] lr: 2.9e-06, eta: 2:12:49.219376, loss: 1.3519
2023-04-12 02:04:22 - training - INFO - Epoch [4/5][361/649] lr: 2.9e-06, eta: 2:09:11.297960, loss: 1.9874
2023-04-12 02:04:25 - training - INFO - Epoch [4/5][371/649] lr: 2.9e-06, eta: 2:05:44.928264, loss: 1.4165
2023-04-12 02:04:29 - training - INFO - Epoch [4/5][381/649] lr: 2.8e-06, eta: 2:02:29.230208, loss: 1.1927
2023-04-12 02:04:33 - training - INFO - Epoch [4/5][391/649] lr: 2.8e-06, eta: 1:59:23.328804, loss: 2.5949
2023-04-12 02:04:36 - training - INFO - Epoch [4/5][401/649] lr: 2.8e-06, eta: 1:56:26.530584, loss: 2.1973
2023-04-12 02:04:40 - training - INFO - Epoch [4/5][411/649] lr: 2.7e-06, eta: 1:53:38.241248, loss: 1.3404
2023-04-12 02:04:44 - training - INFO - Epoch [4/5][421/649] lr: 2.7e-06, eta: 1:50:57.692960, loss: 1.1754
2023-04-12 02:04:48 - training - INFO - Epoch [4/5][431/649] lr: 2.7e-06, eta: 1:48:24.414672, loss: 2.0621
2023-04-12 02:04:51 - training - INFO - Epoch [4/5][441/649] lr: 2.6e-06, eta: 1:45:57.924192, loss: 1.5248
2023-04-12 02:04:55 - training - INFO - Epoch [4/5][451/649] lr: 2.6e-06, eta: 1:43:37.837450, loss: 1.3160
2023-04-12 02:04:59 - training - INFO - Epoch [4/5][461/649] lr: 2.6e-06, eta: 1:41:23.594016, loss: 1.3393
2023-04-12 02:05:02 - training - INFO - Epoch [4/5][471/649] lr: 2.5e-06, eta: 1:39:14.929156, loss: 1.1204
2023-04-12 02:05:06 - training - INFO - Epoch [4/5][481/649] lr: 2.5e-06, eta: 1:37:11.448504, loss: 1.4963
2023-04-12 02:05:10 - training - INFO - Epoch [4/5][491/649] lr: 2.5e-06, eta: 1:35:12.914124, loss: 1.1995
2023-04-12 02:05:14 - training - INFO - Epoch [4/5][501/649] lr: 2.5e-06, eta: 1:33:18.909736, loss: 1.2071
2023-04-12 02:05:17 - training - INFO - Epoch [4/5][511/649] lr: 2.4e-06, eta: 1:31:29.169362, loss: 1.5297
2023-04-12 02:05:21 - training - INFO - Epoch [4/5][521/649] lr: 2.4e-06, eta: 1:29:43.541988, loss: 1.6715
2023-04-12 02:05:25 - training - INFO - Epoch [4/5][531/649] lr: 2.4e-06, eta: 1:28:01.747968, loss: 1.4535
2023-04-12 02:05:28 - training - INFO - Epoch [4/5][541/649] lr: 2.3e-06, eta: 1:26:23.603152, loss: 1.6148
2023-04-12 02:05:32 - training - INFO - Epoch [4/5][551/649] lr: 2.3e-06, eta: 1:24:48.850158, loss: 1.7064
2023-04-12 02:05:36 - training - INFO - Epoch [4/5][561/649] lr: 2.3e-06, eta: 1:23:17.342284, loss: 1.5094
2023-04-12 02:05:40 - training - INFO - Epoch [4/5][571/649] lr: 2.2e-06, eta: 1:21:48.937222, loss: 1.3760
2023-04-12 02:05:43 - training - INFO - Epoch [4/5][581/649] lr: 2.2e-06, eta: 1:20:23.430408, loss: 1.5541
2023-04-12 02:05:47 - training - INFO - Epoch [4/5][591/649] lr: 2.2e-06, eta: 1:19:00.694230, loss: 1.5142
2023-04-12 02:05:51 - training - INFO - Epoch [4/5][601/649] lr: 2.1e-06, eta: 1:17:40.599952, loss: 1.6484
2023-04-12 02:05:54 - training - INFO - Epoch [4/5][611/649] lr: 2.1e-06, eta: 1:16:22.999326, loss: 1.6793
2023-04-12 02:05:58 - training - INFO - Epoch [4/5][621/649] lr: 2.1e-06, eta: 1:15:07.756480, loss: 2.3517
2023-04-12 02:06:02 - training - INFO - Epoch [4/5][631/649] lr: 2.1e-06, eta: 1:13:54.807840, loss: 1.3159
2023-04-12 02:06:06 - training - INFO - Epoch [4/5][641/649] lr: 2.0e-06, eta: 1:12:44.012352, loss: 1.3069
2023-04-12 02:06:36 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 1.5407, Validation Metrics: {'exact_match': 32.82887077997672, 'f1': 39.454050151717986}, Test Metrics: {'exact_match': 36.35307781649245, 'f1': 43.08672312281392}
2023-04-12 02:06:36 - training - INFO - Epoch [5/5][1/649] lr: 2.0e-06, eta: 41 days, 11:25:13.799244, loss: 1.6595
2023-04-12 02:06:40 - training - INFO - Epoch [5/5][11/649] lr: 2.0e-06, eta: 3 days, 18:30:59.104494, loss: 1.6392
2023-04-12 02:06:43 - training - INFO - Epoch [5/5][21/649] lr: 1.9e-06, eta: 1 day, 23:25:30.569568, loss: 1.5136
2023-04-12 02:06:47 - training - INFO - Epoch [5/5][31/649] lr: 1.9e-06, eta: 1 day, 8:08:01.990528, loss: 1.7949
2023-04-12 02:06:51 - training - INFO - Epoch [5/5][41/649] lr: 1.9e-06, eta: 1 day, 0:18:05.367384, loss: 1.1734
2023-04-12 02:06:54 - training - INFO - Epoch [5/5][51/649] lr: 1.8e-06, eta: 19:32:24.256750, loss: 1.7453
2023-04-12 02:06:58 - training - INFO - Epoch [5/5][61/649] lr: 1.8e-06, eta: 16:20:22.231696, loss: 1.1049
2023-04-12 02:07:02 - training - INFO - Epoch [5/5][71/649] lr: 1.8e-06, eta: 14:02:24.781968, loss: 1.7661
2023-04-12 02:07:06 - training - INFO - Epoch [5/5][81/649] lr: 1.8e-06, eta: 12:18:30.206360, loss: 1.3224
2023-04-12 02:07:09 - training - INFO - Epoch [5/5][91/649] lr: 1.7e-06, eta: 10:57:25.154060, loss: 1.3454
2023-04-12 02:07:13 - training - INFO - Epoch [5/5][101/649] lr: 1.7e-06, eta: 9:52:22.596168, loss: 1.7971
2023-04-12 02:07:17 - training - INFO - Epoch [5/5][111/649] lr: 1.7e-06, eta: 8:59:02.522724, loss: 1.7670
2023-04-12 02:07:21 - training - INFO - Epoch [5/5][121/649] lr: 1.6e-06, eta: 8:14:30.877280, loss: 1.7409
2023-04-12 02:07:24 - training - INFO - Epoch [5/5][131/649] lr: 1.6e-06, eta: 7:36:46.575576, loss: 1.2651
2023-04-12 02:07:28 - training - INFO - Epoch [5/5][141/649] lr: 1.6e-06, eta: 7:04:22.888000, loss: 1.2370
2023-04-12 02:07:32 - training - INFO - Epoch [5/5][151/649] lr: 1.5e-06, eta: 6:36:16.053392, loss: 1.1716
2023-04-12 02:07:35 - training - INFO - Epoch [5/5][161/649] lr: 1.5e-06, eta: 6:11:38.297628, loss: 1.7875
2023-04-12 02:07:39 - training - INFO - Epoch [5/5][171/649] lr: 1.5e-06, eta: 5:49:53.228238, loss: 1.6734
2023-04-12 02:07:43 - training - INFO - Epoch [5/5][181/649] lr: 1.4e-06, eta: 5:30:31.966736, loss: 1.3434
2023-04-12 02:07:47 - training - INFO - Epoch [5/5][191/649] lr: 1.4e-06, eta: 5:13:11.704830, loss: 1.2330
2023-04-12 02:07:50 - training - INFO - Epoch [5/5][201/649] lr: 1.4e-06, eta: 4:57:34.594176, loss: 1.5333
2023-04-12 02:07:54 - training - INFO - Epoch [5/5][211/649] lr: 1.3e-06, eta: 4:43:25.861264, loss: 1.7535
2023-04-12 02:07:58 - training - INFO - Epoch [5/5][221/649] lr: 1.3e-06, eta: 4:30:33.588000, loss: 1.1339
2023-04-12 02:08:01 - training - INFO - Epoch [5/5][231/649] lr: 1.3e-06, eta: 4:18:47.868796, loss: 1.2484
2023-04-12 02:08:05 - training - INFO - Epoch [5/5][241/649] lr: 1.3e-06, eta: 4:08:00.380088, loss: 1.3936
2023-04-12 02:08:09 - training - INFO - Epoch [5/5][251/649] lr: 1.2e-06, eta: 3:58:04.191366, loss: 2.0620
2023-04-12 02:08:13 - training - INFO - Epoch [5/5][261/649] lr: 1.2e-06, eta: 3:48:54.271792, loss: 0.8741
2023-04-12 02:08:16 - training - INFO - Epoch [5/5][271/649] lr: 1.2e-06, eta: 3:40:23.896948, loss: 1.9360
2023-04-12 02:08:20 - training - INFO - Epoch [5/5][281/649] lr: 1.1e-06, eta: 3:32:29.515584, loss: 0.9524
2023-04-12 02:08:24 - training - INFO - Epoch [5/5][291/649] lr: 1.1e-06, eta: 3:25:07.584002, loss: 1.9232
2023-04-12 02:08:27 - training - INFO - Epoch [5/5][301/649] lr: 1.1e-06, eta: 3:18:14.728576, loss: 1.3619
2023-04-12 02:08:31 - training - INFO - Epoch [5/5][311/649] lr: 1.0e-06, eta: 3:11:48.198372, loss: 1.7603
2023-04-12 02:08:35 - training - INFO - Epoch [5/5][321/649] lr: 1.0e-06, eta: 3:05:45.521912, loss: 1.2279
2023-04-12 02:08:39 - training - INFO - Epoch [5/5][331/649] lr: 9.8e-07, eta: 3:00:04.505888, loss: 1.0406
2023-04-12 02:08:42 - training - INFO - Epoch [5/5][341/649] lr: 9.5e-07, eta: 2:54:43.239624, loss: 1.2487
2023-04-12 02:08:46 - training - INFO - Epoch [5/5][351/649] lr: 9.2e-07, eta: 2:49:40.081994, loss: 0.9329
2023-04-12 02:08:50 - training - INFO - Epoch [5/5][361/649] lr: 8.9e-07, eta: 2:44:53.478364, loss: 0.9266
2023-04-12 02:08:53 - training - INFO - Epoch [5/5][371/649] lr: 8.6e-07, eta: 2:40:22.154874, loss: 1.2689
2023-04-12 02:08:57 - training - INFO - Epoch [5/5][381/649] lr: 8.3e-07, eta: 2:36:04.873312, loss: 1.3321
2023-04-12 02:09:01 - training - INFO - Epoch [5/5][391/649] lr: 8.0e-07, eta: 2:32:00.582026, loss: 1.6593
2023-04-12 02:09:05 - training - INFO - Epoch [5/5][401/649] lr: 7.6e-07, eta: 2:28:08.293476, loss: 1.9438
2023-04-12 02:09:08 - training - INFO - Epoch [5/5][411/649] lr: 7.3e-07, eta: 2:24:27.208030, loss: 1.3073
2023-04-12 02:09:12 - training - INFO - Epoch [5/5][421/649] lr: 7.0e-07, eta: 2:20:56.419992, loss: 1.6289
2023-04-12 02:09:16 - training - INFO - Epoch [5/5][431/649] lr: 6.7e-07, eta: 2:17:35.164470, loss: 1.2856
2023-04-12 02:09:19 - training - INFO - Epoch [5/5][441/649] lr: 6.4e-07, eta: 2:14:22.882372, loss: 1.9540
2023-04-12 02:09:23 - training - INFO - Epoch [5/5][451/649] lr: 6.1e-07, eta: 2:11:18.937506, loss: 1.7740
2023-04-12 02:09:27 - training - INFO - Epoch [5/5][461/649] lr: 5.8e-07, eta: 2:08:22.807392, loss: 1.4472
2023-04-12 02:09:30 - training - INFO - Epoch [5/5][471/649] lr: 5.5e-07, eta: 2:05:34.017560, loss: 1.1857
2023-04-12 02:09:34 - training - INFO - Epoch [5/5][481/649] lr: 5.2e-07, eta: 2:02:52.107632, loss: 1.7034
2023-04-12 02:09:38 - training - INFO - Epoch [5/5][491/649] lr: 4.9e-07, eta: 2:00:16.661466, loss: 1.3070
2023-04-12 02:09:42 - training - INFO - Epoch [5/5][501/649] lr: 4.6e-07, eta: 1:57:47.254320, loss: 1.6623
2023-04-12 02:09:45 - training - INFO - Epoch [5/5][511/649] lr: 4.3e-07, eta: 1:55:23.540590, loss: 1.2531
2023-04-12 02:09:49 - training - INFO - Epoch [5/5][521/649] lr: 3.9e-07, eta: 1:53:05.206152, loss: 1.5417
2023-04-12 02:09:53 - training - INFO - Epoch [5/5][531/649] lr: 3.6e-07, eta: 1:50:51.927152, loss: 1.7528
2023-04-12 02:09:56 - training - INFO - Epoch [5/5][541/649] lr: 3.3e-07, eta: 1:48:43.443264, loss: 1.3104
2023-04-12 02:10:00 - training - INFO - Epoch [5/5][551/649] lr: 3.0e-07, eta: 1:46:39.491934, loss: 1.0922
2023-04-12 02:10:04 - training - INFO - Epoch [5/5][561/649] lr: 2.7e-07, eta: 1:44:39.816532, loss: 1.2715
2023-04-12 02:10:08 - training - INFO - Epoch [5/5][571/649] lr: 2.4e-07, eta: 1:42:44.246522, loss: 1.1822
2023-04-12 02:10:11 - training - INFO - Epoch [5/5][581/649] lr: 2.1e-07, eta: 1:40:52.578696, loss: 1.3070
2023-04-12 02:10:15 - training - INFO - Epoch [5/5][591/649] lr: 1.8e-07, eta: 1:39:04.787490, loss: 1.9169
2023-04-12 02:10:19 - training - INFO - Epoch [5/5][601/649] lr: 1.5e-07, eta: 1:37:20.432072, loss: 1.4558
2023-04-12 02:10:23 - training - INFO - Epoch [5/5][611/649] lr: 1.2e-07, eta: 1:35:39.106704, loss: 1.3981
2023-04-12 02:10:26 - training - INFO - Epoch [5/5][621/649] lr: 8.6e-08, eta: 1:34:00.957120, loss: 0.9395
2023-04-12 02:10:30 - training - INFO - Epoch [5/5][631/649] lr: 5.5e-08, eta: 1:32:25.755226, loss: 1.5319
2023-04-12 02:10:34 - training - INFO - Epoch [5/5][641/649] lr: 2.5e-08, eta: 1:30:53.445228, loss: 1.0528
2023-04-12 02:11:04 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 1.4406, Validation Metrics: {'exact_match': 31.78114086146682, 'f1': 38.47536094543858}, Test Metrics: {'exact_match': 36.469221835075494, 'f1': 43.574726537646605}
2023-04-12 02:11:17 - training - INFO - Final Test - Train Loss: 1.4406, Test Metrics: {'exact_match': 36.469221835075494, 'f1': 43.574726537646605}
