2023-04-12 05:55:59 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1380cc367820a3f3/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'deepset/roberta-base-squad2'}, 'data': {'task_type': 'factoid', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 4.54e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/roberta_factoid_squad2'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 597.82it/s]
Map:   0%|          | 0/4429 [00:00<?, ? examples/s]Map:  23%|██▎       | 1000/4429 [00:00<00:02, 1505.01 examples/s]Map:  45%|████▌     | 2000/4429 [00:01<00:01, 1796.43 examples/s]Map:  68%|██████▊   | 3000/4429 [00:01<00:00, 1876.91 examples/s]Map:  90%|█████████ | 4000/4429 [00:02<00:00, 1940.46 examples/s]Map: 100%|██████████| 4429/4429 [00:02<00:00, 1948.16 examples/s]                                                                 Map:   0%|          | 0/553 [00:00<?, ? examples/s]Map: 100%|██████████| 553/553 [00:00<00:00, 1519.07 examples/s]                                                               Map:   0%|          | 0/555 [00:00<?, ? examples/s]Map: 100%|██████████| 555/555 [00:00<00:00, 1518.27 examples/s]                                                               2023-04-12 05:56:26 - training - INFO - First Test - Val Metrics:{'exact_match': 46.835443037974684, 'f1': 58.833915162172495} Test Metrics: {'exact_match': 47.747747747747745, 'f1': 61.90147040611443}
2023-04-12 05:56:26 - training - INFO - Epoch [1/5][1/402] lr: 4.5e-05, eta: 9:22:59.671548, loss: 2.2338
2023-04-12 05:56:30 - training - INFO - Epoch [1/5][11/402] lr: 4.5e-05, eta: 1:02:05.282427, loss: 1.6794
2023-04-12 05:56:33 - training - INFO - Epoch [1/5][21/402] lr: 4.5e-05, eta: 0:38:10.504554, loss: 2.4063
2023-04-12 05:56:37 - training - INFO - Epoch [1/5][31/402] lr: 4.5e-05, eta: 0:29:38.932995, loss: 2.2490
2023-04-12 05:56:41 - training - INFO - Epoch [1/5][41/402] lr: 4.4e-05, eta: 0:25:15.470385, loss: 1.6737
2023-04-12 05:56:44 - training - INFO - Epoch [1/5][51/402] lr: 4.4e-05, eta: 0:22:33.721893, loss: 1.1996
2023-04-12 05:56:48 - training - INFO - Epoch [1/5][61/402] lr: 4.4e-05, eta: 0:20:43.931709, loss: 1.6307
2023-04-12 05:56:52 - training - INFO - Epoch [1/5][71/402] lr: 4.4e-05, eta: 0:19:24.101918, loss: 2.5565
2023-04-12 05:56:55 - training - INFO - Epoch [1/5][81/402] lr: 4.4e-05, eta: 0:18:23.094792, loss: 1.7689
2023-04-12 05:56:59 - training - INFO - Epoch [1/5][91/402] lr: 4.3e-05, eta: 0:17:34.544232, loss: 1.9582
2023-04-12 05:57:03 - training - INFO - Epoch [1/5][101/402] lr: 4.3e-05, eta: 0:16:54.854944, loss: 1.0696
2023-04-12 05:57:06 - training - INFO - Epoch [1/5][111/402] lr: 4.3e-05, eta: 0:16:21.697545, loss: 2.5585
2023-04-12 05:57:10 - training - INFO - Epoch [1/5][121/402] lr: 4.3e-05, eta: 0:15:53.472750, loss: 2.1904
2023-04-12 05:57:14 - training - INFO - Epoch [1/5][131/402] lr: 4.2e-05, eta: 0:15:28.893045, loss: 1.4930
2023-04-12 05:57:18 - training - INFO - Epoch [1/5][141/402] lr: 4.2e-05, eta: 0:15:07.347168, loss: 1.1304
2023-04-12 05:57:21 - training - INFO - Epoch [1/5][151/402] lr: 4.2e-05, eta: 0:14:48.202315, loss: 1.8961
2023-04-12 05:57:25 - training - INFO - Epoch [1/5][161/402] lr: 4.2e-05, eta: 0:14:30.919678, loss: 1.2269
2023-04-12 05:57:29 - training - INFO - Epoch [1/5][171/402] lr: 4.2e-05, eta: 0:14:15.214077, loss: 1.1959
2023-04-12 05:57:32 - training - INFO - Epoch [1/5][181/402] lr: 4.1e-05, eta: 0:14:00.844341, loss: 1.7206
2023-04-12 05:57:36 - training - INFO - Epoch [1/5][191/402] lr: 4.1e-05, eta: 0:13:47.630448, loss: 0.9164
2023-04-12 05:57:40 - training - INFO - Epoch [1/5][201/402] lr: 4.1e-05, eta: 0:13:35.394087, loss: 1.0673
2023-04-12 05:57:43 - training - INFO - Epoch [1/5][211/402] lr: 4.1e-05, eta: 0:13:23.919130, loss: 1.3871
2023-04-12 05:57:47 - training - INFO - Epoch [1/5][221/402] lr: 4.0e-05, eta: 0:13:13.172829, loss: 1.9289
2023-04-12 05:57:51 - training - INFO - Epoch [1/5][231/402] lr: 4.0e-05, eta: 0:13:03.037524, loss: 1.8861
2023-04-12 05:57:54 - training - INFO - Epoch [1/5][241/402] lr: 4.0e-05, eta: 0:12:53.443949, loss: 1.4696
2023-04-12 05:57:58 - training - INFO - Epoch [1/5][251/402] lr: 4.0e-05, eta: 0:12:44.350583, loss: 1.6026
2023-04-12 05:58:02 - training - INFO - Epoch [1/5][261/402] lr: 4.0e-05, eta: 0:12:35.653701, loss: 1.7436
2023-04-12 05:58:06 - training - INFO - Epoch [1/5][271/402] lr: 3.9e-05, eta: 0:12:27.343945, loss: 1.3928
2023-04-12 05:58:09 - training - INFO - Epoch [1/5][281/402] lr: 3.9e-05, eta: 0:12:19.310026, loss: 0.6514
2023-04-12 05:58:13 - training - INFO - Epoch [1/5][291/402] lr: 3.9e-05, eta: 0:12:11.570301, loss: 0.8118
2023-04-12 05:58:17 - training - INFO - Epoch [1/5][301/402] lr: 3.9e-05, eta: 0:12:04.111845, loss: 0.8210
2023-04-12 05:58:20 - training - INFO - Epoch [1/5][311/402] lr: 3.8e-05, eta: 0:11:56.903244, loss: 0.7770
2023-04-12 05:58:24 - training - INFO - Epoch [1/5][321/402] lr: 3.8e-05, eta: 0:11:49.935681, loss: 1.0890
2023-04-12 05:58:28 - training - INFO - Epoch [1/5][331/402] lr: 3.8e-05, eta: 0:11:43.155126, loss: 1.4072
2023-04-12 05:58:31 - training - INFO - Epoch [1/5][341/402] lr: 3.8e-05, eta: 0:11:36.570502, loss: 1.4310
2023-04-12 05:58:35 - training - INFO - Epoch [1/5][351/402] lr: 3.7e-05, eta: 0:11:30.162249, loss: 2.0882
2023-04-12 05:58:39 - training - INFO - Epoch [1/5][361/402] lr: 3.7e-05, eta: 0:11:23.894717, loss: 1.3900
2023-04-12 05:58:43 - training - INFO - Epoch [1/5][371/402] lr: 3.7e-05, eta: 0:11:17.775670, loss: 1.0703
2023-04-12 05:58:46 - training - INFO - Epoch [1/5][381/402] lr: 3.7e-05, eta: 0:11:11.781681, loss: 1.8016
2023-04-12 05:58:50 - training - INFO - Epoch [1/5][391/402] lr: 3.7e-05, eta: 0:11:05.896319, loss: 1.0095
2023-04-12 05:58:54 - training - INFO - Epoch [1/5][401/402] lr: 3.6e-05, eta: 0:11:00.129257, loss: 1.1996
2023-04-12 05:59:10 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 1.5117, Validation Metrics: {'exact_match': 73.77938517179024, 'f1': 78.74237238708542}, Test Metrics: {'exact_match': 76.93693693693693, 'f1': 82.50490009932736}
2023-04-12 05:59:10 - training - INFO - Epoch [2/5][1/402] lr: 3.6e-05, eta: 4 days, 5:10:54.441880, loss: 1.3047
2023-04-12 05:59:14 - training - INFO - Epoch [2/5][11/402] lr: 3.6e-05, eta: 9:20:19.569806, loss: 1.1297
2023-04-12 05:59:18 - training - INFO - Epoch [2/5][21/402] lr: 3.6e-05, eta: 4:57:52.461828, loss: 1.3932
2023-04-12 05:59:22 - training - INFO - Epoch [2/5][31/402] lr: 3.6e-05, eta: 3:24:42.505180, loss: 1.2029
2023-04-12 05:59:25 - training - INFO - Epoch [2/5][41/402] lr: 3.5e-05, eta: 2:36:57.679744, loss: 0.7325
2023-04-12 05:59:29 - training - INFO - Epoch [2/5][51/402] lr: 3.5e-05, eta: 2:07:54.897717, loss: 0.8029
2023-04-12 05:59:33 - training - INFO - Epoch [2/5][61/402] lr: 3.5e-05, eta: 1:48:22.255749, loss: 1.0251
2023-04-12 05:59:36 - training - INFO - Epoch [2/5][71/402] lr: 3.5e-05, eta: 1:34:18.924964, loss: 0.8943
2023-04-12 05:59:40 - training - INFO - Epoch [2/5][81/402] lr: 3.4e-05, eta: 1:23:43.081278, loss: 1.6139
2023-04-12 05:59:44 - training - INFO - Epoch [2/5][91/402] lr: 3.4e-05, eta: 1:15:25.996042, loss: 0.9083
2023-04-12 05:59:47 - training - INFO - Epoch [2/5][101/402] lr: 3.4e-05, eta: 1:08:46.574578, loss: 0.6073
2023-04-12 05:59:51 - training - INFO - Epoch [2/5][111/402] lr: 3.4e-05, eta: 1:03:18.592488, loss: 1.1122
2023-04-12 05:59:55 - training - INFO - Epoch [2/5][121/402] lr: 3.4e-05, eta: 0:58:44.192071, loss: 0.8417
2023-04-12 05:59:59 - training - INFO - Epoch [2/5][131/402] lr: 3.3e-05, eta: 0:54:51.057226, loss: 0.8916
2023-04-12 06:00:02 - training - INFO - Epoch [2/5][141/402] lr: 3.3e-05, eta: 0:51:30.539151, loss: 0.9353
2023-04-12 06:00:06 - training - INFO - Epoch [2/5][151/402] lr: 3.3e-05, eta: 0:48:36.094324, loss: 1.1165
2023-04-12 06:00:10 - training - INFO - Epoch [2/5][161/402] lr: 3.3e-05, eta: 0:46:02.859005, loss: 1.1277
2023-04-12 06:00:13 - training - INFO - Epoch [2/5][171/402] lr: 3.2e-05, eta: 0:43:47.092416, loss: 0.3489
2023-04-12 06:00:17 - training - INFO - Epoch [2/5][181/402] lr: 3.2e-05, eta: 0:41:45.933019, loss: 1.1006
2023-04-12 06:00:21 - training - INFO - Epoch [2/5][191/402] lr: 3.2e-05, eta: 0:39:57.023630, loss: 0.9150
2023-04-12 06:00:25 - training - INFO - Epoch [2/5][201/402] lr: 3.2e-05, eta: 0:38:18.669165, loss: 0.9935
2023-04-12 06:00:28 - training - INFO - Epoch [2/5][211/402] lr: 3.2e-05, eta: 0:36:49.249357, loss: 0.5604
2023-04-12 06:00:32 - training - INFO - Epoch [2/5][221/402] lr: 3.1e-05, eta: 0:35:27.582562, loss: 1.0335
2023-04-12 06:00:36 - training - INFO - Epoch [2/5][231/402] lr: 3.1e-05, eta: 0:34:12.677802, loss: 0.6923
2023-04-12 06:00:39 - training - INFO - Epoch [2/5][241/402] lr: 3.1e-05, eta: 0:33:03.703530, loss: 1.2882
2023-04-12 06:00:43 - training - INFO - Epoch [2/5][251/402] lr: 3.1e-05, eta: 0:31:59.899248, loss: 1.2028
2023-04-12 06:00:47 - training - INFO - Epoch [2/5][261/402] lr: 3.0e-05, eta: 0:31:00.687642, loss: 1.5428
2023-04-12 06:00:50 - training - INFO - Epoch [2/5][271/402] lr: 3.0e-05, eta: 0:30:05.582832, loss: 0.4231
2023-04-12 06:00:54 - training - INFO - Epoch [2/5][281/402] lr: 3.0e-05, eta: 0:29:14.148305, loss: 1.0051
2023-04-12 06:00:58 - training - INFO - Epoch [2/5][291/402] lr: 3.0e-05, eta: 0:28:25.964823, loss: 1.0594
2023-04-12 06:01:02 - training - INFO - Epoch [2/5][301/402] lr: 3.0e-05, eta: 0:27:40.760057, loss: 0.5650
2023-04-12 06:01:05 - training - INFO - Epoch [2/5][311/402] lr: 2.9e-05, eta: 0:26:58.227841, loss: 0.8702
2023-04-12 06:01:09 - training - INFO - Epoch [2/5][321/402] lr: 2.9e-05, eta: 0:26:18.118839, loss: 1.4949
2023-04-12 06:01:13 - training - INFO - Epoch [2/5][331/402] lr: 2.9e-05, eta: 0:25:40.207144, loss: 0.9363
2023-04-12 06:01:16 - training - INFO - Epoch [2/5][341/402] lr: 2.9e-05, eta: 0:25:04.293066, loss: 0.8021
2023-04-12 06:01:20 - training - INFO - Epoch [2/5][351/402] lr: 2.8e-05, eta: 0:24:30.217413, loss: 0.7324
2023-04-12 06:01:24 - training - INFO - Epoch [2/5][361/402] lr: 2.8e-05, eta: 0:23:57.820815, loss: 1.0020
2023-04-12 06:01:28 - training - INFO - Epoch [2/5][371/402] lr: 2.8e-05, eta: 0:23:26.981521, loss: 1.4123
2023-04-12 06:01:31 - training - INFO - Epoch [2/5][381/402] lr: 2.8e-05, eta: 0:22:57.541044, loss: 0.7189
2023-04-12 06:01:35 - training - INFO - Epoch [2/5][391/402] lr: 2.7e-05, eta: 0:22:29.446214, loss: 1.2253
2023-04-12 06:01:39 - training - INFO - Epoch [2/5][401/402] lr: 2.7e-05, eta: 0:22:02.552948, loss: 1.2674
2023-04-12 06:01:55 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 0.9858, Validation Metrics: {'exact_match': 76.13019891500905, 'f1': 79.24868559071808}, Test Metrics: {'exact_match': 79.27927927927928, 'f1': 82.44637692934907}
2023-04-12 06:01:56 - training - INFO - Epoch [3/5][1/402] lr: 2.7e-05, eta: 8 days, 1:19:58.592835, loss: 0.8733
2023-04-12 06:01:59 - training - INFO - Epoch [3/5][11/402] lr: 2.7e-05, eta: 17:40:30.718725, loss: 0.5774
2023-04-12 06:02:03 - training - INFO - Epoch [3/5][21/402] lr: 2.7e-05, eta: 9:18:34.544583, loss: 0.7464
2023-04-12 06:02:07 - training - INFO - Epoch [3/5][31/402] lr: 2.7e-05, eta: 6:20:26.053165, loss: 0.7612
2023-04-12 06:02:10 - training - INFO - Epoch [3/5][41/402] lr: 2.6e-05, eta: 4:49:09.666290, loss: 0.9741
2023-04-12 06:02:14 - training - INFO - Epoch [3/5][51/402] lr: 2.6e-05, eta: 3:53:39.324912, loss: 0.6898
2023-04-12 06:02:18 - training - INFO - Epoch [3/5][61/402] lr: 2.6e-05, eta: 3:16:19.681938, loss: 0.6482
2023-04-12 06:02:21 - training - INFO - Epoch [3/5][71/402] lr: 2.6e-05, eta: 2:49:29.981318, loss: 0.5372
2023-04-12 06:02:25 - training - INFO - Epoch [3/5][81/402] lr: 2.5e-05, eta: 2:29:16.894836, loss: 0.9464
2023-04-12 06:02:29 - training - INFO - Epoch [3/5][91/402] lr: 2.5e-05, eta: 2:13:29.593203, loss: 0.5108
2023-04-12 06:02:33 - training - INFO - Epoch [3/5][101/402] lr: 2.5e-05, eta: 2:00:49.139241, loss: 0.5239
2023-04-12 06:02:36 - training - INFO - Epoch [3/5][111/402] lr: 2.5e-05, eta: 1:50:25.007118, loss: 0.8021
2023-04-12 06:02:40 - training - INFO - Epoch [3/5][121/402] lr: 2.5e-05, eta: 1:41:43.447783, loss: 0.7328
2023-04-12 06:02:44 - training - INFO - Epoch [3/5][131/402] lr: 2.4e-05, eta: 1:34:20.929065, loss: 0.6546
2023-04-12 06:02:47 - training - INFO - Epoch [3/5][141/402] lr: 2.4e-05, eta: 1:28:00.698766, loss: 0.8174
2023-04-12 06:02:51 - training - INFO - Epoch [3/5][151/402] lr: 2.4e-05, eta: 1:22:30.256740, loss: 0.6289
2023-04-12 06:02:55 - training - INFO - Epoch [3/5][161/402] lr: 2.4e-05, eta: 1:17:40.421141, loss: 0.6640
2023-04-12 06:02:59 - training - INFO - Epoch [3/5][171/402] lr: 2.3e-05, eta: 1:13:24.039039, loss: 0.3436
2023-04-12 06:03:02 - training - INFO - Epoch [3/5][181/402] lr: 2.3e-05, eta: 1:09:35.599684, loss: 0.7279
2023-04-12 06:03:06 - training - INFO - Epoch [3/5][191/402] lr: 2.3e-05, eta: 1:06:10.671453, loss: 0.4761
2023-04-12 06:03:10 - training - INFO - Epoch [3/5][201/402] lr: 2.3e-05, eta: 1:03:05.810076, loss: 0.4438
2023-04-12 06:03:13 - training - INFO - Epoch [3/5][211/402] lr: 2.2e-05, eta: 1:00:18.125413, loss: 0.3117
2023-04-12 06:03:17 - training - INFO - Epoch [3/5][221/402] lr: 2.2e-05, eta: 0:57:45.237541, loss: 1.2218
2023-04-12 06:03:21 - training - INFO - Epoch [3/5][231/402] lr: 2.2e-05, eta: 0:55:25.280115, loss: 0.5108
2023-04-12 06:03:25 - training - INFO - Epoch [3/5][241/402] lr: 2.2e-05, eta: 0:53:16.614842, loss: 1.2917
2023-04-12 06:03:28 - training - INFO - Epoch [3/5][251/402] lr: 2.2e-05, eta: 0:51:17.896441, loss: 0.7468
2023-04-12 06:03:32 - training - INFO - Epoch [3/5][261/402] lr: 2.1e-05, eta: 0:49:28.030263, loss: 0.6009
2023-04-12 06:03:36 - training - INFO - Epoch [3/5][271/402] lr: 2.1e-05, eta: 0:47:46.019815, loss: 0.2837
2023-04-12 06:03:39 - training - INFO - Epoch [3/5][281/402] lr: 2.1e-05, eta: 0:46:10.992224, loss: 1.3460
2023-04-12 06:03:43 - training - INFO - Epoch [3/5][291/402] lr: 2.1e-05, eta: 0:44:42.219303, loss: 0.7649
2023-04-12 06:03:47 - training - INFO - Epoch [3/5][301/402] lr: 2.0e-05, eta: 0:43:19.117269, loss: 0.9899
2023-04-12 06:03:51 - training - INFO - Epoch [3/5][311/402] lr: 2.0e-05, eta: 0:42:01.079839, loss: 0.5551
2023-04-12 06:03:54 - training - INFO - Epoch [3/5][321/402] lr: 2.0e-05, eta: 0:40:47.683599, loss: 0.9990
2023-04-12 06:03:58 - training - INFO - Epoch [3/5][331/402] lr: 2.0e-05, eta: 0:39:38.496585, loss: 0.5939
2023-04-12 06:04:02 - training - INFO - Epoch [3/5][341/402] lr: 2.0e-05, eta: 0:38:33.133860, loss: 0.9686
2023-04-12 06:04:05 - training - INFO - Epoch [3/5][351/402] lr: 1.9e-05, eta: 0:37:31.311111, loss: 0.6174
2023-04-12 06:04:09 - training - INFO - Epoch [3/5][361/402] lr: 1.9e-05, eta: 0:36:32.723121, loss: 0.8664
2023-04-12 06:04:13 - training - INFO - Epoch [3/5][371/402] lr: 1.9e-05, eta: 0:35:37.095378, loss: 1.0211
2023-04-12 06:04:17 - training - INFO - Epoch [3/5][381/402] lr: 1.9e-05, eta: 0:34:44.162148, loss: 0.7433
2023-04-12 06:04:20 - training - INFO - Epoch [3/5][391/402] lr: 1.8e-05, eta: 0:33:53.748944, loss: 0.6651
2023-04-12 06:04:24 - training - INFO - Epoch [3/5][401/402] lr: 1.8e-05, eta: 0:33:05.679772, loss: 1.0290
2023-04-12 06:04:41 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 0.7929, Validation Metrics: {'exact_match': 76.67269439421338, 'f1': 79.46328848989528}, Test Metrics: {'exact_match': 80.36036036036036, 'f1': 83.81329176066019}
2023-04-12 06:04:41 - training - INFO - Epoch [4/5][1/402] lr: 1.8e-05, eta: 11 days, 21:39:27.217332, loss: 0.5543
2023-04-12 06:04:45 - training - INFO - Epoch [4/5][11/402] lr: 1.8e-05, eta: 1 day, 2:01:36.364382, loss: 0.5063
2023-04-12 06:04:48 - training - INFO - Epoch [4/5][21/402] lr: 1.8e-05, eta: 13:39:44.801523, loss: 0.7617
2023-04-12 06:04:52 - training - INFO - Epoch [4/5][31/402] lr: 1.7e-05, eta: 9:16:27.948459, loss: 0.5052
2023-04-12 06:04:56 - training - INFO - Epoch [4/5][41/402] lr: 1.7e-05, eta: 7:01:35.195618, loss: 0.6659
2023-04-12 06:05:00 - training - INFO - Epoch [4/5][51/402] lr: 1.7e-05, eta: 5:39:34.514853, loss: 0.9433
2023-04-12 06:05:03 - training - INFO - Epoch [4/5][61/402] lr: 1.7e-05, eta: 4:44:26.022853, loss: 0.6702
2023-04-12 06:05:07 - training - INFO - Epoch [4/5][71/402] lr: 1.7e-05, eta: 4:04:48.545480, loss: 0.3200
2023-04-12 06:05:11 - training - INFO - Epoch [4/5][81/402] lr: 1.6e-05, eta: 3:34:57.049017, loss: 0.9621
2023-04-12 06:05:14 - training - INFO - Epoch [4/5][91/402] lr: 1.6e-05, eta: 3:11:38.494480, loss: 0.3655
2023-04-12 06:05:18 - training - INFO - Epoch [4/5][101/402] lr: 1.6e-05, eta: 2:52:56.165237, loss: 0.6926
2023-04-12 06:05:22 - training - INFO - Epoch [4/5][111/402] lr: 1.6e-05, eta: 2:37:35.356476, loss: 0.6636
2023-04-12 06:05:25 - training - INFO - Epoch [4/5][121/402] lr: 1.5e-05, eta: 2:24:46.158476, loss: 0.8258
2023-04-12 06:05:29 - training - INFO - Epoch [4/5][131/402] lr: 1.5e-05, eta: 2:13:53.901254, loss: 1.0320
2023-04-12 06:05:33 - training - INFO - Epoch [4/5][141/402] lr: 1.5e-05, eta: 2:04:33.588990, loss: 0.5079
2023-04-12 06:05:37 - training - INFO - Epoch [4/5][151/402] lr: 1.5e-05, eta: 1:56:26.997589, loss: 0.6613
2023-04-12 06:05:40 - training - INFO - Epoch [4/5][161/402] lr: 1.5e-05, eta: 1:49:20.433202, loss: 0.6669
2023-04-12 06:05:44 - training - INFO - Epoch [4/5][171/402] lr: 1.4e-05, eta: 1:43:03.266022, loss: 0.7142
2023-04-12 06:05:48 - training - INFO - Epoch [4/5][181/402] lr: 1.4e-05, eta: 1:37:27.430056, loss: 0.6432
2023-04-12 06:05:51 - training - INFO - Epoch [4/5][191/402] lr: 1.4e-05, eta: 1:32:26.382022, loss: 0.6073
2023-04-12 06:05:55 - training - INFO - Epoch [4/5][201/402] lr: 1.4e-05, eta: 1:27:54.899280, loss: 0.4871
2023-04-12 06:05:59 - training - INFO - Epoch [4/5][211/402] lr: 1.3e-05, eta: 1:23:48.748298, loss: 0.6677
2023-04-12 06:06:03 - training - INFO - Epoch [4/5][221/402] lr: 1.3e-05, eta: 1:20:04.567024, loss: 0.2918
2023-04-12 06:06:06 - training - INFO - Epoch [4/5][231/402] lr: 1.3e-05, eta: 1:16:39.485307, loss: 0.5314
2023-04-12 06:06:10 - training - INFO - Epoch [4/5][241/402] lr: 1.3e-05, eta: 1:13:31.082874, loss: 0.9117
2023-04-12 06:06:14 - training - INFO - Epoch [4/5][251/402] lr: 1.2e-05, eta: 1:10:37.409892, loss: 0.7445
2023-04-12 06:06:17 - training - INFO - Epoch [4/5][261/402] lr: 1.2e-05, eta: 1:07:56.789574, loss: 0.6285
2023-04-12 06:06:21 - training - INFO - Epoch [4/5][271/402] lr: 1.2e-05, eta: 1:05:27.708878, loss: 0.5284
2023-04-12 06:06:25 - training - INFO - Epoch [4/5][281/402] lr: 1.2e-05, eta: 1:03:08.998031, loss: 0.5157
2023-04-12 06:06:29 - training - INFO - Epoch [4/5][291/402] lr: 1.2e-05, eta: 1:00:59.561910, loss: 0.6207
2023-04-12 06:06:32 - training - INFO - Epoch [4/5][301/402] lr: 1.1e-05, eta: 0:58:58.474246, loss: 0.7034
2023-04-12 06:06:36 - training - INFO - Epoch [4/5][311/402] lr: 1.1e-05, eta: 0:57:04.934247, loss: 0.4870
2023-04-12 06:06:40 - training - INFO - Epoch [4/5][321/402] lr: 1.1e-05, eta: 0:55:18.216156, loss: 0.8348
2023-04-12 06:06:43 - training - INFO - Epoch [4/5][331/402] lr: 1.1e-05, eta: 0:53:37.751451, loss: 0.8484
2023-04-12 06:06:47 - training - INFO - Epoch [4/5][341/402] lr: 1.0e-05, eta: 0:52:02.954357, loss: 0.6656
2023-04-12 06:06:51 - training - INFO - Epoch [4/5][351/402] lr: 1.0e-05, eta: 0:50:33.352098, loss: 0.5944
2023-04-12 06:06:55 - training - INFO - Epoch [4/5][361/402] lr: 1.0e-05, eta: 0:49:08.514238, loss: 0.6346
2023-04-12 06:06:58 - training - INFO - Epoch [4/5][371/402] lr: 9.8e-06, eta: 0:47:48.068071, loss: 0.5376
2023-04-12 06:07:02 - training - INFO - Epoch [4/5][381/402] lr: 9.6e-06, eta: 0:46:31.630332, loss: 0.3132
2023-04-12 06:07:06 - training - INFO - Epoch [4/5][391/402] lr: 9.3e-06, eta: 0:45:18.904887, loss: 0.8790
2023-04-12 06:07:09 - training - INFO - Epoch [4/5][401/402] lr: 9.1e-06, eta: 0:44:09.607878, loss: 0.5773
2023-04-12 06:07:26 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 0.6817, Validation Metrics: {'exact_match': 76.85352622061482, 'f1': 79.72085895524891}, Test Metrics: {'exact_match': 77.47747747747748, 'f1': 81.49161408604135}
2023-04-12 06:07:26 - training - INFO - Epoch [5/5][1/402] lr: 9.1e-06, eta: 15 days, 17:58:36.503195, loss: 0.6243
2023-04-12 06:07:30 - training - INFO - Epoch [5/5][11/402] lr: 8.8e-06, eta: 1 day, 10:22:38.913588, loss: 0.5256
2023-04-12 06:07:34 - training - INFO - Epoch [5/5][21/402] lr: 8.6e-06, eta: 18:00:52.836750, loss: 0.7015
2023-04-12 06:07:38 - training - INFO - Epoch [5/5][31/402] lr: 8.4e-06, eta: 12:12:28.531676, loss: 0.5256
2023-04-12 06:07:41 - training - INFO - Epoch [5/5][41/402] lr: 8.2e-06, eta: 9:13:59.903873, loss: 0.5678
2023-04-12 06:07:45 - training - INFO - Epoch [5/5][51/402] lr: 7.9e-06, eta: 7:25:29.187618, loss: 0.4580
2023-04-12 06:07:49 - training - INFO - Epoch [5/5][61/402] lr: 7.7e-06, eta: 6:12:32.129888, loss: 0.3856
2023-04-12 06:07:52 - training - INFO - Epoch [5/5][71/402] lr: 7.5e-06, eta: 5:20:06.851755, loss: 0.3514
2023-04-12 06:07:56 - training - INFO - Epoch [5/5][81/402] lr: 7.3e-06, eta: 4:40:37.222488, loss: 0.3057
2023-04-12 06:08:00 - training - INFO - Epoch [5/5][91/402] lr: 7.0e-06, eta: 4:09:47.641389, loss: 0.4902
2023-04-12 06:08:04 - training - INFO - Epoch [5/5][101/402] lr: 6.8e-06, eta: 3:45:03.574942, loss: 0.8936
2023-04-12 06:08:07 - training - INFO - Epoch [5/5][111/402] lr: 6.6e-06, eta: 3:24:46.283130, loss: 0.5742
2023-04-12 06:08:11 - training - INFO - Epoch [5/5][121/402] lr: 6.3e-06, eta: 3:07:49.566210, loss: 0.7505
2023-04-12 06:08:15 - training - INFO - Epoch [5/5][131/402] lr: 6.1e-06, eta: 2:53:27.476602, loss: 0.6134
2023-04-12 06:08:18 - training - INFO - Epoch [5/5][141/402] lr: 5.9e-06, eta: 2:41:07.105329, loss: 0.5657
2023-04-12 06:08:22 - training - INFO - Epoch [5/5][151/402] lr: 5.7e-06, eta: 2:30:24.303574, loss: 0.4719
2023-04-12 06:08:26 - training - INFO - Epoch [5/5][161/402] lr: 5.4e-06, eta: 2:21:00.905664, loss: 0.4744
2023-04-12 06:08:30 - training - INFO - Epoch [5/5][171/402] lr: 5.2e-06, eta: 2:12:42.923331, loss: 0.5833
2023-04-12 06:08:33 - training - INFO - Epoch [5/5][181/402] lr: 5.0e-06, eta: 2:05:19.585990, loss: 0.1897
2023-04-12 06:08:37 - training - INFO - Epoch [5/5][191/402] lr: 4.8e-06, eta: 1:58:42.290862, loss: 0.4875
2023-04-12 06:08:41 - training - INFO - Epoch [5/5][201/402] lr: 4.5e-06, eta: 1:52:44.183856, loss: 0.5644
2023-04-12 06:08:44 - training - INFO - Epoch [5/5][211/402] lr: 4.3e-06, eta: 1:47:19.651827, loss: 0.4612
2023-04-12 06:08:48 - training - INFO - Epoch [5/5][221/402] lr: 4.1e-06, eta: 1:42:24.095086, loss: 0.5816
2023-04-12 06:08:52 - training - INFO - Epoch [5/5][231/402] lr: 3.9e-06, eta: 1:37:53.838156, loss: 0.4742
2023-04-12 06:08:56 - training - INFO - Epoch [5/5][241/402] lr: 3.6e-06, eta: 1:33:45.720730, loss: 1.2338
2023-04-12 06:08:59 - training - INFO - Epoch [5/5][251/402] lr: 3.4e-06, eta: 1:29:57.060545, loss: 1.0573
2023-04-12 06:09:03 - training - INFO - Epoch [5/5][261/402] lr: 3.2e-06, eta: 1:26:25.631088, loss: 0.7187
2023-04-12 06:09:07 - training - INFO - Epoch [5/5][271/402] lr: 3.0e-06, eta: 1:23:09.577058, loss: 0.6879
2023-04-12 06:09:10 - training - INFO - Epoch [5/5][281/402] lr: 2.7e-06, eta: 1:20:07.202673, loss: 0.5197
2023-04-12 06:09:14 - training - INFO - Epoch [5/5][291/402] lr: 2.5e-06, eta: 1:17:17.107359, loss: 0.5099
2023-04-12 06:09:18 - training - INFO - Epoch [5/5][301/402] lr: 2.3e-06, eta: 1:14:38.051684, loss: 0.7744
2023-04-12 06:09:22 - training - INFO - Epoch [5/5][311/402] lr: 2.1e-06, eta: 1:12:08.980642, loss: 0.6997
2023-04-12 06:09:25 - training - INFO - Epoch [5/5][321/402] lr: 1.8e-06, eta: 1:09:48.981795, loss: 0.6065
2023-04-12 06:09:29 - training - INFO - Epoch [5/5][331/402] lr: 1.6e-06, eta: 1:07:37.231303, loss: 0.8925
2023-04-12 06:09:33 - training - INFO - Epoch [5/5][341/402] lr: 1.4e-06, eta: 1:05:32.971796, loss: 0.5210
2023-04-12 06:09:36 - training - INFO - Epoch [5/5][351/402] lr: 1.2e-06, eta: 1:03:35.567280, loss: 0.5637
2023-04-12 06:09:40 - training - INFO - Epoch [5/5][361/402] lr: 9.3e-07, eta: 1:01:44.468606, loss: 0.3639
2023-04-12 06:09:44 - training - INFO - Epoch [5/5][371/402] lr: 7.0e-07, eta: 0:59:59.157133, loss: 0.4876
2023-04-12 06:09:48 - training - INFO - Epoch [5/5][381/402] lr: 4.7e-07, eta: 0:58:19.163676, loss: 0.3341
2023-04-12 06:09:51 - training - INFO - Epoch [5/5][391/402] lr: 2.5e-07, eta: 0:56:44.114257, loss: 0.4161
2023-04-12 06:09:55 - training - INFO - Epoch [5/5][401/402] lr: 2.3e-08, eta: 0:55:13.606780, loss: 0.3566
2023-04-12 06:10:11 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 0.6058, Validation Metrics: {'exact_match': 77.39602169981917, 'f1': 80.49540272867578}, Test Metrics: {'exact_match': 79.09909909909909, 'f1': 82.87517946960673}
2023-04-12 06:10:20 - training - INFO - Final Test - Train Loss: 0.6058, Test Metrics: {'exact_match': 79.09909909909909, 'f1': 82.87517946960673}
