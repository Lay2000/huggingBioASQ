2023-04-12 05:26:29 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1380cc367820a3f3/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'roberta-base'}, 'data': {'task_type': 'factoid', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 4.54e-06, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/roberta_factoid_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 579.24it/s]
Map:   0%|          | 0/4429 [00:00<?, ? examples/s]Map:  23%|██▎       | 1000/4429 [00:00<00:02, 1474.93 examples/s]Map:  45%|████▌     | 2000/4429 [00:01<00:01, 1786.97 examples/s]Map:  68%|██████▊   | 3000/4429 [00:01<00:00, 1875.94 examples/s]Map:  90%|█████████ | 4000/4429 [00:02<00:00, 1941.83 examples/s]Map: 100%|██████████| 4429/4429 [00:02<00:00, 1943.96 examples/s]                                                                 Map:   0%|          | 0/553 [00:00<?, ? examples/s]Map: 100%|██████████| 553/553 [00:00<00:00, 1485.28 examples/s]                                                               Map:   0%|          | 0/555 [00:00<?, ? examples/s]Map: 100%|██████████| 555/555 [00:00<00:00, 1525.73 examples/s]                                                               Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-12 05:26:58 - training - INFO - First Test - Val Metrics:{'exact_match': 0.0, 'f1': 2.418336176912124} Test Metrics: {'exact_match': 0.0, 'f1': 2.905020243161542}
2023-04-12 05:26:59 - training - INFO - Epoch [1/5][1/402] lr: 4.5e-06, eta: 9:14:39.157324, loss: 5.9920
2023-04-12 05:27:02 - training - INFO - Epoch [1/5][11/402] lr: 4.5e-06, eta: 1:01:21.428365, loss: 5.8542
2023-04-12 05:27:06 - training - INFO - Epoch [1/5][21/402] lr: 4.5e-06, eta: 0:37:49.548450, loss: 5.5820
2023-04-12 05:27:10 - training - INFO - Epoch [1/5][31/402] lr: 4.5e-06, eta: 0:29:26.025957, loss: 5.5284
2023-04-12 05:27:14 - training - INFO - Epoch [1/5][41/402] lr: 4.4e-06, eta: 0:25:06.724087, loss: 4.6771
2023-04-12 05:27:17 - training - INFO - Epoch [1/5][51/402] lr: 4.4e-06, eta: 0:22:27.709722, loss: 4.8395
2023-04-12 05:27:21 - training - INFO - Epoch [1/5][61/402] lr: 4.4e-06, eta: 0:20:39.640011, loss: 4.5553
2023-04-12 05:27:25 - training - INFO - Epoch [1/5][71/402] lr: 4.4e-06, eta: 0:19:21.069322, loss: 4.6468
2023-04-12 05:27:28 - training - INFO - Epoch [1/5][81/402] lr: 4.4e-06, eta: 0:18:21.011472, loss: 3.9938
2023-04-12 05:27:32 - training - INFO - Epoch [1/5][91/402] lr: 4.3e-06, eta: 0:17:33.245069, loss: 3.7214
2023-04-12 05:27:36 - training - INFO - Epoch [1/5][101/402] lr: 4.3e-06, eta: 0:16:54.339514, loss: 3.7793
2023-04-12 05:27:40 - training - INFO - Epoch [1/5][111/402] lr: 4.3e-06, eta: 0:16:21.680454, loss: 3.4051
2023-04-12 05:27:43 - training - INFO - Epoch [1/5][121/402] lr: 4.3e-06, eta: 0:15:53.797658, loss: 2.9328
2023-04-12 05:27:47 - training - INFO - Epoch [1/5][131/402] lr: 4.2e-06, eta: 0:15:29.637129, loss: 2.4624
2023-04-12 05:27:51 - training - INFO - Epoch [1/5][141/402] lr: 4.2e-06, eta: 0:15:08.401284, loss: 3.7440
2023-04-12 05:27:54 - training - INFO - Epoch [1/5][151/402] lr: 4.2e-06, eta: 0:14:49.410665, loss: 3.4837
2023-04-12 05:27:58 - training - INFO - Epoch [1/5][161/402] lr: 4.2e-06, eta: 0:14:32.334163, loss: 2.4889
2023-04-12 05:28:02 - training - INFO - Epoch [1/5][171/402] lr: 4.2e-06, eta: 0:14:16.821363, loss: 2.9079
2023-04-12 05:28:06 - training - INFO - Epoch [1/5][181/402] lr: 4.1e-06, eta: 0:14:02.644077, loss: 3.1937
2023-04-12 05:28:09 - training - INFO - Epoch [1/5][191/402] lr: 4.1e-06, eta: 0:13:49.505837, loss: 2.7171
2023-04-12 05:28:13 - training - INFO - Epoch [1/5][201/402] lr: 4.1e-06, eta: 0:13:37.331526, loss: 2.3660
2023-04-12 05:28:17 - training - INFO - Epoch [1/5][211/402] lr: 4.1e-06, eta: 0:13:25.984382, loss: 2.6164
2023-04-12 05:28:20 - training - INFO - Epoch [1/5][221/402] lr: 4.0e-06, eta: 0:13:15.317840, loss: 2.1478
2023-04-12 05:28:24 - training - INFO - Epoch [1/5][231/402] lr: 4.0e-06, eta: 0:13:05.239926, loss: 2.1617
2023-04-12 05:28:28 - training - INFO - Epoch [1/5][241/402] lr: 4.0e-06, eta: 0:12:55.681734, loss: 1.9931
2023-04-12 05:28:32 - training - INFO - Epoch [1/5][251/402] lr: 4.0e-06, eta: 0:12:46.591549, loss: 2.5341
2023-04-12 05:28:35 - training - INFO - Epoch [1/5][261/402] lr: 4.0e-06, eta: 0:12:37.922154, loss: 2.4214
2023-04-12 05:28:39 - training - INFO - Epoch [1/5][271/402] lr: 3.9e-06, eta: 0:12:29.618557, loss: 3.3359
2023-04-12 05:28:43 - training - INFO - Epoch [1/5][281/402] lr: 3.9e-06, eta: 0:12:21.638989, loss: 2.6834
2023-04-12 05:28:46 - training - INFO - Epoch [1/5][291/402] lr: 3.9e-06, eta: 0:12:13.983777, loss: 2.6331
2023-04-12 05:28:50 - training - INFO - Epoch [1/5][301/402] lr: 3.9e-06, eta: 0:12:06.571096, loss: 2.8004
2023-04-12 05:28:54 - training - INFO - Epoch [1/5][311/402] lr: 3.8e-06, eta: 0:11:59.414366, loss: 2.3724
2023-04-12 05:28:58 - training - INFO - Epoch [1/5][321/402] lr: 3.8e-06, eta: 0:11:52.465803, loss: 2.7371
2023-04-12 05:29:01 - training - INFO - Epoch [1/5][331/402] lr: 3.8e-06, eta: 0:11:45.678663, loss: 2.1993
2023-04-12 05:29:05 - training - INFO - Epoch [1/5][341/402] lr: 3.8e-06, eta: 0:11:39.080678, loss: 2.8265
2023-04-12 05:29:09 - training - INFO - Epoch [1/5][351/402] lr: 3.7e-06, eta: 0:11:32.642454, loss: 2.7732
2023-04-12 05:29:12 - training - INFO - Epoch [1/5][361/402] lr: 3.7e-06, eta: 0:11:26.361621, loss: 2.9676
2023-04-12 05:29:16 - training - INFO - Epoch [1/5][371/402] lr: 3.7e-06, eta: 0:11:20.232531, loss: 2.0123
2023-04-12 05:29:20 - training - INFO - Epoch [1/5][381/402] lr: 3.7e-06, eta: 0:11:14.217036, loss: 2.0937
2023-04-12 05:29:24 - training - INFO - Epoch [1/5][391/402] lr: 3.7e-06, eta: 0:11:08.308629, loss: 2.3005
2023-04-12 05:29:27 - training - INFO - Epoch [1/5][401/402] lr: 3.6e-06, eta: 0:11:02.489660, loss: 1.8557
2023-04-12 05:29:44 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 3.1774, Validation Metrics: {'exact_match': 45.93128390596745, 'f1': 56.99593998936699}, Test Metrics: {'exact_match': 49.909909909909906, 'f1': 61.003449196947635}
2023-04-12 05:29:44 - training - INFO - Epoch [2/5][1/402] lr: 3.6e-06, eta: 4 days, 5:32:15.064867, loss: 2.0372
2023-04-12 05:29:48 - training - INFO - Epoch [2/5][11/402] lr: 3.6e-06, eta: 9:22:16.839142, loss: 1.7621
2023-04-12 05:29:52 - training - INFO - Epoch [2/5][21/402] lr: 3.6e-06, eta: 4:58:53.987565, loss: 2.2802
2023-04-12 05:29:55 - training - INFO - Epoch [2/5][31/402] lr: 3.6e-06, eta: 3:25:24.072096, loss: 2.1657
2023-04-12 05:29:59 - training - INFO - Epoch [2/5][41/402] lr: 3.5e-06, eta: 2:37:28.870673, loss: 1.9501
2023-04-12 05:30:03 - training - INFO - Epoch [2/5][51/402] lr: 3.5e-06, eta: 2:08:19.822074, loss: 2.0551
2023-04-12 05:30:06 - training - INFO - Epoch [2/5][61/402] lr: 3.5e-06, eta: 1:48:43.059375, loss: 1.8198
2023-04-12 05:30:10 - training - INFO - Epoch [2/5][71/402] lr: 3.5e-06, eta: 1:34:36.802544, loss: 1.8892
2023-04-12 05:30:14 - training - INFO - Epoch [2/5][81/402] lr: 3.4e-06, eta: 1:23:58.484343, loss: 1.9534
2023-04-12 05:30:18 - training - INFO - Epoch [2/5][91/402] lr: 3.4e-06, eta: 1:15:39.599833, loss: 1.7429
2023-04-12 05:30:21 - training - INFO - Epoch [2/5][101/402] lr: 3.4e-06, eta: 1:08:58.799814, loss: 2.2022
2023-04-12 05:30:25 - training - INFO - Epoch [2/5][111/402] lr: 3.4e-06, eta: 1:03:29.540223, loss: 2.2780
2023-04-12 05:30:29 - training - INFO - Epoch [2/5][121/402] lr: 3.4e-06, eta: 0:58:54.080986, loss: 3.0182
2023-04-12 05:30:32 - training - INFO - Epoch [2/5][131/402] lr: 3.3e-06, eta: 0:55:00.129038, loss: 1.7923
2023-04-12 05:30:36 - training - INFO - Epoch [2/5][141/402] lr: 3.3e-06, eta: 0:51:38.796393, loss: 2.2156
2023-04-12 05:30:40 - training - INFO - Epoch [2/5][151/402] lr: 3.3e-06, eta: 0:48:43.699493, loss: 2.6645
2023-04-12 05:30:43 - training - INFO - Epoch [2/5][161/402] lr: 3.3e-06, eta: 0:46:09.850074, loss: 2.4538
2023-04-12 05:30:47 - training - INFO - Epoch [2/5][171/402] lr: 3.2e-06, eta: 0:43:53.563857, loss: 1.4554
2023-04-12 05:30:51 - training - INFO - Epoch [2/5][181/402] lr: 3.2e-06, eta: 0:41:51.924823, loss: 2.1235
2023-04-12 05:30:55 - training - INFO - Epoch [2/5][191/402] lr: 3.2e-06, eta: 0:40:02.626150, loss: 1.5725
2023-04-12 05:30:58 - training - INFO - Epoch [2/5][201/402] lr: 3.2e-06, eta: 0:38:23.842905, loss: 1.6995
2023-04-12 05:31:02 - training - INFO - Epoch [2/5][211/402] lr: 3.2e-06, eta: 0:36:54.074275, loss: 1.3923
2023-04-12 05:31:06 - training - INFO - Epoch [2/5][221/402] lr: 3.1e-06, eta: 0:35:32.140934, loss: 1.4092
2023-04-12 05:31:09 - training - INFO - Epoch [2/5][231/402] lr: 3.1e-06, eta: 0:34:17.006109, loss: 2.0202
2023-04-12 05:31:13 - training - INFO - Epoch [2/5][241/402] lr: 3.1e-06, eta: 0:33:07.781075, loss: 1.7354
2023-04-12 05:31:17 - training - INFO - Epoch [2/5][251/402] lr: 3.1e-06, eta: 0:32:03.723314, loss: 1.4797
2023-04-12 05:31:20 - training - INFO - Epoch [2/5][261/402] lr: 3.0e-06, eta: 0:31:04.294080, loss: 2.4741
2023-04-12 05:31:24 - training - INFO - Epoch [2/5][271/402] lr: 3.0e-06, eta: 0:30:08.986055, loss: 1.3805
2023-04-12 05:31:28 - training - INFO - Epoch [2/5][281/402] lr: 3.0e-06, eta: 0:29:17.348684, loss: 2.6866
2023-04-12 05:31:32 - training - INFO - Epoch [2/5][291/402] lr: 3.0e-06, eta: 0:28:29.029800, loss: 2.0963
2023-04-12 05:31:35 - training - INFO - Epoch [2/5][301/402] lr: 3.0e-06, eta: 0:27:43.684156, loss: 1.8563
2023-04-12 05:31:39 - training - INFO - Epoch [2/5][311/402] lr: 2.9e-06, eta: 0:27:01.020997, loss: 1.5593
2023-04-12 05:31:43 - training - INFO - Epoch [2/5][321/402] lr: 2.9e-06, eta: 0:26:20.784081, loss: 2.0239
2023-04-12 05:31:46 - training - INFO - Epoch [2/5][331/402] lr: 2.9e-06, eta: 0:25:42.744113, loss: 2.2479
2023-04-12 05:31:50 - training - INFO - Epoch [2/5][341/402] lr: 2.9e-06, eta: 0:25:06.724799, loss: 1.2572
2023-04-12 05:31:54 - training - INFO - Epoch [2/5][351/402] lr: 2.8e-06, eta: 0:24:32.551626, loss: 1.7702
2023-04-12 05:31:57 - training - INFO - Epoch [2/5][361/402] lr: 2.8e-06, eta: 0:24:00.068402, loss: 1.9582
2023-04-12 05:32:01 - training - INFO - Epoch [2/5][371/402] lr: 2.8e-06, eta: 0:23:29.123694, loss: 1.8891
2023-04-12 05:32:05 - training - INFO - Epoch [2/5][381/402] lr: 2.8e-06, eta: 0:22:59.603358, loss: 1.7718
2023-04-12 05:32:09 - training - INFO - Epoch [2/5][391/402] lr: 2.7e-06, eta: 0:22:31.421394, loss: 2.5266
2023-04-12 05:32:12 - training - INFO - Epoch [2/5][401/402] lr: 2.7e-06, eta: 0:22:04.437087, loss: 2.4771
2023-04-12 05:32:29 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.8629, Validation Metrics: {'exact_match': 58.40867992766727, 'f1': 67.98342922657527}, Test Metrics: {'exact_match': 60.9009009009009, 'f1': 70.71131104103242}
2023-04-12 05:32:29 - training - INFO - Epoch [3/5][1/402] lr: 2.7e-06, eta: 8 days, 1:33:13.546099, loss: 2.1425
2023-04-12 05:32:33 - training - INFO - Epoch [3/5][11/402] lr: 2.7e-06, eta: 17:41:40.879627, loss: 2.1979
2023-04-12 05:32:36 - training - INFO - Epoch [3/5][21/402] lr: 2.7e-06, eta: 9:19:11.205831, loss: 1.0501
2023-04-12 05:32:40 - training - INFO - Epoch [3/5][31/402] lr: 2.7e-06, eta: 6:20:50.565059, loss: 1.8702
2023-04-12 05:32:44 - training - INFO - Epoch [3/5][41/402] lr: 2.6e-06, eta: 4:49:27.794873, loss: 2.3246
2023-04-12 05:32:48 - training - INFO - Epoch [3/5][51/402] lr: 2.6e-06, eta: 3:53:53.627571, loss: 2.0223
2023-04-12 05:32:51 - training - INFO - Epoch [3/5][61/402] lr: 2.6e-06, eta: 3:16:31.401275, loss: 2.0955
2023-04-12 05:32:55 - training - INFO - Epoch [3/5][71/402] lr: 2.6e-06, eta: 2:49:39.713159, loss: 1.1539
2023-04-12 05:32:59 - training - INFO - Epoch [3/5][81/402] lr: 2.5e-06, eta: 2:29:25.077654, loss: 1.7636
2023-04-12 05:33:02 - training - INFO - Epoch [3/5][91/402] lr: 2.5e-06, eta: 2:13:36.547659, loss: 1.5077
2023-04-12 05:33:06 - training - INFO - Epoch [3/5][101/402] lr: 2.5e-06, eta: 2:00:55.068595, loss: 1.4319
2023-04-12 05:33:10 - training - INFO - Epoch [3/5][111/402] lr: 2.5e-06, eta: 1:50:30.212277, loss: 1.7279
2023-04-12 05:33:13 - training - INFO - Epoch [3/5][121/402] lr: 2.5e-06, eta: 1:41:48.000273, loss: 1.2982
2023-04-12 05:33:17 - training - INFO - Epoch [3/5][131/402] lr: 2.4e-06, eta: 1:34:24.938851, loss: 1.3027
2023-04-12 05:33:21 - training - INFO - Epoch [3/5][141/402] lr: 2.4e-06, eta: 1:28:04.244259, loss: 1.9142
2023-04-12 05:33:25 - training - INFO - Epoch [3/5][151/402] lr: 2.4e-06, eta: 1:22:33.500695, loss: 0.8268
2023-04-12 05:33:28 - training - INFO - Epoch [3/5][161/402] lr: 2.4e-06, eta: 1:17:43.386937, loss: 1.7481
2023-04-12 05:33:32 - training - INFO - Epoch [3/5][171/402] lr: 2.3e-06, eta: 1:13:26.731335, loss: 1.3708
2023-04-12 05:33:36 - training - INFO - Epoch [3/5][181/402] lr: 2.3e-06, eta: 1:09:38.032254, loss: 1.7789
2023-04-12 05:33:39 - training - INFO - Epoch [3/5][191/402] lr: 2.3e-06, eta: 1:06:12.917918, loss: 1.4389
2023-04-12 05:33:43 - training - INFO - Epoch [3/5][201/402] lr: 2.3e-06, eta: 1:03:07.861482, loss: 1.1969
2023-04-12 05:33:47 - training - INFO - Epoch [3/5][211/402] lr: 2.2e-06, eta: 1:00:19.965790, loss: 2.3011
2023-04-12 05:33:51 - training - INFO - Epoch [3/5][221/402] lr: 2.2e-06, eta: 0:57:46.978238, loss: 1.6133
2023-04-12 05:33:54 - training - INFO - Epoch [3/5][231/402] lr: 2.2e-06, eta: 0:55:26.947038, loss: 1.7128
2023-04-12 05:33:58 - training - INFO - Epoch [3/5][241/402] lr: 2.2e-06, eta: 0:53:18.212249, loss: 1.9225
2023-04-12 05:34:02 - training - INFO - Epoch [3/5][251/402] lr: 2.2e-06, eta: 0:51:19.416217, loss: 1.2702
2023-04-12 05:34:05 - training - INFO - Epoch [3/5][261/402] lr: 2.1e-06, eta: 0:49:29.483682, loss: 2.4183
2023-04-12 05:34:09 - training - INFO - Epoch [3/5][271/402] lr: 2.1e-06, eta: 0:47:47.369279, loss: 1.7268
2023-04-12 05:34:13 - training - INFO - Epoch [3/5][281/402] lr: 2.1e-06, eta: 0:46:12.247478, loss: 1.4832
2023-04-12 05:34:16 - training - INFO - Epoch [3/5][291/402] lr: 2.1e-06, eta: 0:44:43.407132, loss: 1.3301
2023-04-12 05:34:20 - training - INFO - Epoch [3/5][301/402] lr: 2.0e-06, eta: 0:43:20.224701, loss: 1.0646
2023-04-12 05:34:24 - training - INFO - Epoch [3/5][311/402] lr: 2.0e-06, eta: 0:42:02.167199, loss: 2.5230
2023-04-12 05:34:28 - training - INFO - Epoch [3/5][321/402] lr: 2.0e-06, eta: 0:40:48.744291, loss: 1.4480
2023-04-12 05:34:31 - training - INFO - Epoch [3/5][331/402] lr: 2.0e-06, eta: 0:39:39.547639, loss: 1.6136
2023-04-12 05:34:35 - training - INFO - Epoch [3/5][341/402] lr: 2.0e-06, eta: 0:38:34.197013, loss: 1.3438
2023-04-12 05:34:39 - training - INFO - Epoch [3/5][351/402] lr: 1.9e-06, eta: 0:37:32.303193, loss: 1.6999
2023-04-12 05:34:42 - training - INFO - Epoch [3/5][361/402] lr: 1.9e-06, eta: 0:36:33.643263, loss: 1.2833
2023-04-12 05:34:46 - training - INFO - Epoch [3/5][371/402] lr: 1.9e-06, eta: 0:35:37.967326, loss: 0.9258
2023-04-12 05:34:50 - training - INFO - Epoch [3/5][381/402] lr: 1.9e-06, eta: 0:34:44.997825, loss: 1.8421
2023-04-12 05:34:54 - training - INFO - Epoch [3/5][391/402] lr: 1.8e-06, eta: 0:33:54.566539, loss: 1.4480
2023-04-12 05:34:57 - training - INFO - Epoch [3/5][401/402] lr: 1.8e-06, eta: 0:33:06.464964, loss: 1.7116
2023-04-12 05:35:14 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 1.5894, Validation Metrics: {'exact_match': 64.55696202531645, 'f1': 72.5350652121929}, Test Metrics: {'exact_match': 67.38738738738739, 'f1': 75.35748618720757}
2023-04-12 05:35:14 - training - INFO - Epoch [4/5][1/402] lr: 1.8e-06, eta: 11 days, 21:46:58.818433, loss: 1.5156
2023-04-12 05:35:18 - training - INFO - Epoch [4/5][11/402] lr: 1.8e-06, eta: 1 day, 2:02:17.245931, loss: 1.5808
2023-04-12 05:35:22 - training - INFO - Epoch [4/5][21/402] lr: 1.8e-06, eta: 13:40:06.354327, loss: 1.1073
2023-04-12 05:35:25 - training - INFO - Epoch [4/5][31/402] lr: 1.7e-06, eta: 9:16:42.595038, loss: 1.4044
2023-04-12 05:35:29 - training - INFO - Epoch [4/5][41/402] lr: 1.7e-06, eta: 7:01:46.468143, loss: 0.9258
2023-04-12 05:35:33 - training - INFO - Epoch [4/5][51/402] lr: 1.7e-06, eta: 5:39:43.812267, loss: 1.7860
2023-04-12 05:35:37 - training - INFO - Epoch [4/5][61/402] lr: 1.7e-06, eta: 4:44:33.816904, loss: 1.0744
2023-04-12 05:35:40 - training - INFO - Epoch [4/5][71/402] lr: 1.7e-06, eta: 4:04:55.178799, loss: 2.1401
2023-04-12 05:35:44 - training - INFO - Epoch [4/5][81/402] lr: 1.6e-06, eta: 3:35:02.936325, loss: 0.7383
2023-04-12 05:35:48 - training - INFO - Epoch [4/5][91/402] lr: 1.6e-06, eta: 3:11:43.633562, loss: 1.1156
2023-04-12 05:35:51 - training - INFO - Epoch [4/5][101/402] lr: 1.6e-06, eta: 2:53:00.699112, loss: 1.6282
2023-04-12 05:35:55 - training - INFO - Epoch [4/5][111/402] lr: 1.6e-06, eta: 2:37:39.532377, loss: 1.4837
2023-04-12 05:35:59 - training - INFO - Epoch [4/5][121/402] lr: 1.5e-06, eta: 2:24:50.061150, loss: 1.6014
2023-04-12 05:36:03 - training - INFO - Epoch [4/5][131/402] lr: 1.5e-06, eta: 2:13:57.508934, loss: 1.7258
2023-04-12 05:36:06 - training - INFO - Epoch [4/5][141/402] lr: 1.5e-06, eta: 2:04:36.962535, loss: 1.3602
2023-04-12 05:36:10 - training - INFO - Epoch [4/5][151/402] lr: 1.5e-06, eta: 1:56:30.137440, loss: 1.9515
2023-04-12 05:36:14 - training - INFO - Epoch [4/5][161/402] lr: 1.5e-06, eta: 1:49:23.310246, loss: 1.0839
2023-04-12 05:36:17 - training - INFO - Epoch [4/5][171/402] lr: 1.4e-06, eta: 1:43:05.928894, loss: 1.8229
2023-04-12 05:36:21 - training - INFO - Epoch [4/5][181/402] lr: 1.4e-06, eta: 1:37:29.816901, loss: 0.8140
2023-04-12 05:36:25 - training - INFO - Epoch [4/5][191/402] lr: 1.4e-06, eta: 1:32:28.557546, loss: 1.8727
2023-04-12 05:36:29 - training - INFO - Epoch [4/5][201/402] lr: 1.4e-06, eta: 1:27:56.838528, loss: 1.4245
2023-04-12 05:36:32 - training - INFO - Epoch [4/5][211/402] lr: 1.3e-06, eta: 1:23:50.565288, loss: 1.0074
2023-04-12 05:36:36 - training - INFO - Epoch [4/5][221/402] lr: 1.3e-06, eta: 1:20:06.252262, loss: 1.4874
2023-04-12 05:36:40 - training - INFO - Epoch [4/5][231/402] lr: 1.3e-06, eta: 1:16:41.045490, loss: 1.9148
2023-04-12 05:36:43 - training - INFO - Epoch [4/5][241/402] lr: 1.3e-06, eta: 1:13:32.570603, loss: 1.3102
2023-04-12 05:36:47 - training - INFO - Epoch [4/5][251/402] lr: 1.2e-06, eta: 1:10:38.843477, loss: 1.5897
2023-04-12 05:36:51 - training - INFO - Epoch [4/5][261/402] lr: 1.2e-06, eta: 1:07:58.099575, loss: 1.5534
2023-04-12 05:36:55 - training - INFO - Epoch [4/5][271/402] lr: 1.2e-06, eta: 1:05:28.952263, loss: 1.4633
2023-04-12 05:36:58 - training - INFO - Epoch [4/5][281/402] lr: 1.2e-06, eta: 1:03:10.114965, loss: 1.6175
2023-04-12 05:37:02 - training - INFO - Epoch [4/5][291/402] lr: 1.2e-06, eta: 1:01:00.576120, loss: 1.3690
2023-04-12 05:37:06 - training - INFO - Epoch [4/5][301/402] lr: 1.1e-06, eta: 0:58:59.397106, loss: 1.9654
2023-04-12 05:37:09 - training - INFO - Epoch [4/5][311/402] lr: 1.1e-06, eta: 0:57:05.780349, loss: 1.3168
2023-04-12 05:37:13 - training - INFO - Epoch [4/5][321/402] lr: 1.1e-06, eta: 0:55:19.031943, loss: 2.1034
2023-04-12 05:37:17 - training - INFO - Epoch [4/5][331/402] lr: 1.1e-06, eta: 0:53:38.498606, loss: 1.8666
2023-04-12 05:37:20 - training - INFO - Epoch [4/5][341/402] lr: 1.0e-06, eta: 0:52:03.631971, loss: 0.7844
2023-04-12 05:37:24 - training - INFO - Epoch [4/5][351/402] lr: 1.0e-06, eta: 0:50:33.975882, loss: 1.5638
2023-04-12 05:37:28 - training - INFO - Epoch [4/5][361/402] lr: 1.0e-06, eta: 0:49:09.073249, loss: 1.3122
2023-04-12 05:37:32 - training - INFO - Epoch [4/5][371/402] lr: 9.8e-07, eta: 0:47:48.543381, loss: 1.7110
2023-04-12 05:37:35 - training - INFO - Epoch [4/5][381/402] lr: 9.6e-07, eta: 0:46:32.053872, loss: 1.2182
2023-04-12 05:37:39 - training - INFO - Epoch [4/5][391/402] lr: 9.3e-07, eta: 0:45:19.308018, loss: 1.0302
2023-04-12 05:37:43 - training - INFO - Epoch [4/5][401/402] lr: 9.1e-07, eta: 0:44:10.005301, loss: 1.0942
2023-04-12 05:37:59 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 1.4414, Validation Metrics: {'exact_match': 67.0886075949367, 'f1': 74.15636735790734}, Test Metrics: {'exact_match': 69.54954954954955, 'f1': 76.88004708848699}
2023-04-12 05:38:00 - training - INFO - Epoch [5/5][1/402] lr: 9.1e-07, eta: 15 days, 17:59:57.074140, loss: 1.2871
2023-04-12 05:38:03 - training - INFO - Epoch [5/5][11/402] lr: 8.8e-07, eta: 1 day, 10:22:45.902092, loss: 1.4122
2023-04-12 05:38:07 - training - INFO - Epoch [5/5][21/402] lr: 8.6e-07, eta: 18:00:56.826684, loss: 1.5399
2023-04-12 05:38:11 - training - INFO - Epoch [5/5][31/402] lr: 8.4e-07, eta: 12:12:31.614958, loss: 1.3468
2023-04-12 05:38:14 - training - INFO - Epoch [5/5][41/402] lr: 8.2e-07, eta: 9:14:02.203665, loss: 1.1514
2023-04-12 05:38:18 - training - INFO - Epoch [5/5][51/402] lr: 7.9e-07, eta: 7:25:30.784203, loss: 1.8444
2023-04-12 05:38:22 - training - INFO - Epoch [5/5][61/402] lr: 7.7e-07, eta: 6:12:33.233022, loss: 1.6111
2023-04-12 05:38:26 - training - INFO - Epoch [5/5][71/402] lr: 7.5e-07, eta: 5:20:07.796048, loss: 0.8039
2023-04-12 05:38:29 - training - INFO - Epoch [5/5][81/402] lr: 7.3e-07, eta: 4:40:38.061603, loss: 1.1636
2023-04-12 05:38:33 - training - INFO - Epoch [5/5][91/402] lr: 7.0e-07, eta: 4:09:48.387880, loss: 1.0639
2023-04-12 05:38:37 - training - INFO - Epoch [5/5][101/402] lr: 6.8e-07, eta: 3:45:04.271727, loss: 1.2219
2023-04-12 05:38:40 - training - INFO - Epoch [5/5][111/402] lr: 6.6e-07, eta: 3:24:46.926891, loss: 1.7661
2023-04-12 05:38:44 - training - INFO - Epoch [5/5][121/402] lr: 6.3e-07, eta: 3:07:50.123465, loss: 1.1450
2023-04-12 05:38:48 - training - INFO - Epoch [5/5][131/402] lr: 6.1e-07, eta: 2:53:28.008359, loss: 0.8379
2023-04-12 05:38:52 - training - INFO - Epoch [5/5][141/402] lr: 5.9e-07, eta: 2:41:07.660422, loss: 1.4896
2023-04-12 05:38:55 - training - INFO - Epoch [5/5][151/402] lr: 5.7e-07, eta: 2:30:24.931916, loss: 1.4962
2023-04-12 05:38:59 - training - INFO - Epoch [5/5][161/402] lr: 5.4e-07, eta: 2:21:01.582398, loss: 0.9820
2023-04-12 05:39:03 - training - INFO - Epoch [5/5][171/402] lr: 5.2e-07, eta: 2:12:43.653414, loss: 1.1387
2023-04-12 05:39:06 - training - INFO - Epoch [5/5][181/402] lr: 5.0e-07, eta: 2:05:20.297471, loss: 0.8553
2023-04-12 05:39:10 - training - INFO - Epoch [5/5][191/402] lr: 4.8e-07, eta: 1:58:43.016643, loss: 1.0682
2023-04-12 05:39:14 - training - INFO - Epoch [5/5][201/402] lr: 4.5e-07, eta: 1:52:44.867658, loss: 0.9669
2023-04-12 05:39:18 - training - INFO - Epoch [5/5][211/402] lr: 4.3e-07, eta: 1:47:20.310261, loss: 1.3015
2023-04-12 05:39:21 - training - INFO - Epoch [5/5][221/402] lr: 4.1e-07, eta: 1:42:24.780273, loss: 1.1222
2023-04-12 05:39:25 - training - INFO - Epoch [5/5][231/402] lr: 3.9e-07, eta: 1:37:54.528408, loss: 1.3804
2023-04-12 05:39:29 - training - INFO - Epoch [5/5][241/402] lr: 3.6e-07, eta: 1:33:46.400026, loss: 0.9216
2023-04-12 05:39:32 - training - INFO - Epoch [5/5][251/402] lr: 3.4e-07, eta: 1:29:57.720170, loss: 1.1818
2023-04-12 05:39:36 - training - INFO - Epoch [5/5][261/402] lr: 3.2e-07, eta: 1:26:26.285214, loss: 1.5494
2023-04-12 05:39:40 - training - INFO - Epoch [5/5][271/402] lr: 3.0e-07, eta: 1:23:10.230922, loss: 1.7245
2023-04-12 05:39:44 - training - INFO - Epoch [5/5][281/402] lr: 2.7e-07, eta: 1:20:07.819926, loss: 0.9812
2023-04-12 05:39:47 - training - INFO - Epoch [5/5][291/402] lr: 2.5e-07, eta: 1:17:17.683224, loss: 1.8296
2023-04-12 05:39:51 - training - INFO - Epoch [5/5][301/402] lr: 2.3e-07, eta: 1:14:38.617363, loss: 1.1913
2023-04-12 05:39:55 - training - INFO - Epoch [5/5][311/402] lr: 2.1e-07, eta: 1:12:09.520924, loss: 1.5761
2023-04-12 05:39:58 - training - INFO - Epoch [5/5][321/402] lr: 1.8e-07, eta: 1:09:49.500318, loss: 1.4870
2023-04-12 05:40:02 - training - INFO - Epoch [5/5][331/402] lr: 1.6e-07, eta: 1:07:37.736682, loss: 1.7231
2023-04-12 05:40:06 - training - INFO - Epoch [5/5][341/402] lr: 1.4e-07, eta: 1:05:33.485848, loss: 1.7421
2023-04-12 05:40:10 - training - INFO - Epoch [5/5][351/402] lr: 1.2e-07, eta: 1:03:36.079911, loss: 1.3390
2023-04-12 05:40:13 - training - INFO - Epoch [5/5][361/402] lr: 9.3e-08, eta: 1:01:44.963306, loss: 1.2764
2023-04-12 05:40:17 - training - INFO - Epoch [5/5][371/402] lr: 7.0e-08, eta: 0:59:59.620970, loss: 1.0224
2023-04-12 05:40:21 - training - INFO - Epoch [5/5][381/402] lr: 4.7e-08, eta: 0:58:19.616538, loss: 1.3596
2023-04-12 05:40:24 - training - INFO - Epoch [5/5][391/402] lr: 2.5e-08, eta: 0:56:44.540054, loss: 0.9620
2023-04-12 05:40:28 - training - INFO - Epoch [5/5][401/402] lr: 2.3e-09, eta: 0:55:14.000985, loss: 1.4481
2023-04-12 05:40:45 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 1.3710, Validation Metrics: {'exact_match': 67.45027124773961, 'f1': 74.20717249008679}, Test Metrics: {'exact_match': 69.72972972972973, 'f1': 77.10216248451545}
2023-04-12 05:40:53 - training - INFO - Final Test - Train Loss: 1.3710, Test Metrics: {'exact_match': 69.72972972972973, 'f1': 77.10216248451545}
