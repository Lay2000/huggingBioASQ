2023-04-12 07:32:15 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-44a167365c0b341b/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'roberta-base'}, 'data': {'task_type': 'list', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 1e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/roberta_list_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 369.98it/s]
Map:   0%|          | 0/6878 [00:00<?, ? examples/s]Map:  15%|█▍        | 1000/6878 [00:00<00:03, 1872.31 examples/s]Map:  29%|██▉       | 2000/6878 [00:01<00:02, 1980.50 examples/s]Map:  44%|████▎     | 3000/6878 [00:01<00:01, 2011.56 examples/s]Map:  58%|█████▊    | 4000/6878 [00:01<00:01, 2035.32 examples/s]Map:  73%|███████▎  | 5000/6878 [00:02<00:00, 2051.25 examples/s]Map:  87%|████████▋ | 6000/6878 [00:02<00:00, 2028.70 examples/s]Map: 100%|██████████| 6878/6878 [00:03<00:00, 2046.30 examples/s]                                                                 Map:   0%|          | 0/859 [00:00<?, ? examples/s]Map: 100%|██████████| 859/859 [00:00<00:00, 1543.78 examples/s]                                                               Map:   0%|          | 0/861 [00:00<?, ? examples/s]Map: 100%|██████████| 861/861 [00:00<00:00, 1180.21 examples/s]                                                               Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-12 07:32:53 - training - INFO - First Test - Val Metrics:{'exact_match': 0.0, 'f1': 1.7051426061741173} Test Metrics: {'exact_match': 0.0, 'f1': 1.619917679102585}
2023-04-12 07:32:53 - training - INFO - Epoch [1/5][1/631] lr: 1.0e-05, eta: 22:28:12.117226, loss: 5.9886
2023-04-12 07:32:57 - training - INFO - Epoch [1/5][11/631] lr: 1.0e-05, eta: 2:19:39.014664, loss: 5.6442
2023-04-12 07:33:00 - training - INFO - Epoch [1/5][21/631] lr: 9.9e-06, eta: 1:22:03.376104, loss: 4.7813
2023-04-12 07:33:04 - training - INFO - Epoch [1/5][31/631] lr: 9.9e-06, eta: 1:01:34.967232, loss: 4.3770
2023-04-12 07:33:08 - training - INFO - Epoch [1/5][41/631] lr: 9.9e-06, eta: 0:51:04.443804, loss: 4.4476
2023-04-12 07:33:11 - training - INFO - Epoch [1/5][51/631] lr: 9.8e-06, eta: 0:44:39.661472, loss: 3.4432
2023-04-12 07:33:15 - training - INFO - Epoch [1/5][61/631] lr: 9.8e-06, eta: 0:40:19.755520, loss: 3.8602
2023-04-12 07:33:19 - training - INFO - Epoch [1/5][71/631] lr: 9.8e-06, eta: 0:37:11.893884, loss: 2.8060
2023-04-12 07:33:22 - training - INFO - Epoch [1/5][81/631] lr: 9.7e-06, eta: 0:34:49.480798, loss: 2.7811
2023-04-12 07:33:26 - training - INFO - Epoch [1/5][91/631] lr: 9.7e-06, eta: 0:32:57.701696, loss: 3.3369
2023-04-12 07:33:30 - training - INFO - Epoch [1/5][101/631] lr: 9.7e-06, eta: 0:31:27.332298, loss: 2.2092
2023-04-12 07:33:33 - training - INFO - Epoch [1/5][111/631] lr: 9.6e-06, eta: 0:30:12.778100, loss: 2.5868
2023-04-12 07:33:37 - training - INFO - Epoch [1/5][121/631] lr: 9.6e-06, eta: 0:29:09.944452, loss: 2.8664
2023-04-12 07:33:41 - training - INFO - Epoch [1/5][131/631] lr: 9.6e-06, eta: 0:28:16.234176, loss: 3.1980
2023-04-12 07:33:44 - training - INFO - Epoch [1/5][141/631] lr: 9.6e-06, eta: 0:27:29.586312, loss: 2.9268
2023-04-12 07:33:48 - training - INFO - Epoch [1/5][151/631] lr: 9.5e-06, eta: 0:26:48.642000, loss: 2.6875
2023-04-12 07:33:52 - training - INFO - Epoch [1/5][161/631] lr: 9.5e-06, eta: 0:26:12.296106, loss: 2.3807
2023-04-12 07:33:55 - training - INFO - Epoch [1/5][171/631] lr: 9.5e-06, eta: 0:25:39.779808, loss: 3.3510
2023-04-12 07:33:59 - training - INFO - Epoch [1/5][181/631] lr: 9.4e-06, eta: 0:25:10.458912, loss: 2.9559
2023-04-12 07:34:03 - training - INFO - Epoch [1/5][191/631] lr: 9.4e-06, eta: 0:24:43.837680, loss: 2.9778
2023-04-12 07:34:07 - training - INFO - Epoch [1/5][201/631] lr: 9.4e-06, eta: 0:24:19.494596, loss: 2.9157
2023-04-12 07:34:10 - training - INFO - Epoch [1/5][211/631] lr: 9.3e-06, eta: 0:23:57.154816, loss: 2.0483
2023-04-12 07:34:14 - training - INFO - Epoch [1/5][221/631] lr: 9.3e-06, eta: 0:23:36.529332, loss: 2.3254
2023-04-12 07:34:18 - training - INFO - Epoch [1/5][231/631] lr: 9.3e-06, eta: 0:23:17.382524, loss: 2.4090
2023-04-12 07:34:21 - training - INFO - Epoch [1/5][241/631] lr: 9.2e-06, eta: 0:22:59.510912, loss: 2.2434
2023-04-12 07:34:25 - training - INFO - Epoch [1/5][251/631] lr: 9.2e-06, eta: 0:22:42.672960, loss: 3.2550
2023-04-12 07:34:29 - training - INFO - Epoch [1/5][261/631] lr: 9.2e-06, eta: 0:22:26.861812, loss: 2.2929
2023-04-12 07:34:32 - training - INFO - Epoch [1/5][271/631] lr: 9.1e-06, eta: 0:22:11.963864, loss: 2.0231
2023-04-12 07:34:36 - training - INFO - Epoch [1/5][281/631] lr: 9.1e-06, eta: 0:21:57.938802, loss: 2.3211
2023-04-12 07:34:40 - training - INFO - Epoch [1/5][291/631] lr: 9.1e-06, eta: 0:21:44.612144, loss: 1.7594
2023-04-12 07:34:44 - training - INFO - Epoch [1/5][301/631] lr: 9.0e-06, eta: 0:21:31.943012, loss: 2.5965
2023-04-12 07:34:47 - training - INFO - Epoch [1/5][311/631] lr: 9.0e-06, eta: 0:21:19.825596, loss: 2.1355
2023-04-12 07:34:51 - training - INFO - Epoch [1/5][321/631] lr: 9.0e-06, eta: 0:21:08.240506, loss: 2.3540
2023-04-12 07:34:55 - training - INFO - Epoch [1/5][331/631] lr: 9.0e-06, eta: 0:20:57.148784, loss: 2.4878
2023-04-12 07:34:58 - training - INFO - Epoch [1/5][341/631] lr: 8.9e-06, eta: 0:20:46.469742, loss: 2.5382
2023-04-12 07:35:02 - training - INFO - Epoch [1/5][351/631] lr: 8.9e-06, eta: 0:20:36.216304, loss: 2.3138
2023-04-12 07:35:06 - training - INFO - Epoch [1/5][361/631] lr: 8.9e-06, eta: 0:20:26.320128, loss: 2.9349
2023-04-12 07:35:09 - training - INFO - Epoch [1/5][371/631] lr: 8.8e-06, eta: 0:20:16.766688, loss: 1.7457
2023-04-12 07:35:13 - training - INFO - Epoch [1/5][381/631] lr: 8.8e-06, eta: 0:20:07.502782, loss: 1.5392
2023-04-12 07:35:17 - training - INFO - Epoch [1/5][391/631] lr: 8.8e-06, eta: 0:19:58.550556, loss: 2.0101
2023-04-12 07:35:21 - training - INFO - Epoch [1/5][401/631] lr: 8.7e-06, eta: 0:19:49.871208, loss: 2.8006
2023-04-12 07:35:24 - training - INFO - Epoch [1/5][411/631] lr: 8.7e-06, eta: 0:19:41.429200, loss: 2.1154
2023-04-12 07:35:28 - training - INFO - Epoch [1/5][421/631] lr: 8.7e-06, eta: 0:19:33.216814, loss: 1.8059
2023-04-12 07:35:32 - training - INFO - Epoch [1/5][431/631] lr: 8.6e-06, eta: 0:19:25.220964, loss: 1.6302
2023-04-12 07:35:35 - training - INFO - Epoch [1/5][441/631] lr: 8.6e-06, eta: 0:19:17.409726, loss: 2.5782
2023-04-12 07:35:39 - training - INFO - Epoch [1/5][451/631] lr: 8.6e-06, eta: 0:19:09.770544, loss: 2.7457
2023-04-12 07:35:43 - training - INFO - Epoch [1/5][461/631] lr: 8.5e-06, eta: 0:19:02.301798, loss: 2.5089
2023-04-12 07:35:46 - training - INFO - Epoch [1/5][471/631] lr: 8.5e-06, eta: 0:18:54.988448, loss: 1.4586
2023-04-12 07:35:50 - training - INFO - Epoch [1/5][481/631] lr: 8.5e-06, eta: 0:18:47.829024, loss: 1.9782
2023-04-12 07:35:54 - training - INFO - Epoch [1/5][491/631] lr: 8.4e-06, eta: 0:18:40.822056, loss: 1.9163
2023-04-12 07:35:58 - training - INFO - Epoch [1/5][501/631] lr: 8.4e-06, eta: 0:18:33.963420, loss: 1.9890
2023-04-12 07:36:01 - training - INFO - Epoch [1/5][511/631] lr: 8.4e-06, eta: 0:18:27.227880, loss: 1.8297
2023-04-12 07:36:05 - training - INFO - Epoch [1/5][521/631] lr: 8.3e-06, eta: 0:18:20.587926, loss: 1.8663
2023-04-12 07:36:09 - training - INFO - Epoch [1/5][531/631] lr: 8.3e-06, eta: 0:18:14.040064, loss: 2.6001
2023-04-12 07:36:12 - training - INFO - Epoch [1/5][541/631] lr: 8.3e-06, eta: 0:18:07.633120, loss: 2.5762
2023-04-12 07:36:16 - training - INFO - Epoch [1/5][551/631] lr: 8.3e-06, eta: 0:18:01.316208, loss: 1.9354
2023-04-12 07:36:20 - training - INFO - Epoch [1/5][561/631] lr: 8.2e-06, eta: 0:17:55.085894, loss: 1.5722
2023-04-12 07:36:23 - training - INFO - Epoch [1/5][571/631] lr: 8.2e-06, eta: 0:17:48.962040, loss: 3.1239
2023-04-12 07:36:27 - training - INFO - Epoch [1/5][581/631] lr: 8.2e-06, eta: 0:17:42.912708, loss: 2.2749
2023-04-12 07:36:31 - training - INFO - Epoch [1/5][591/631] lr: 8.1e-06, eta: 0:17:36.937208, loss: 1.9464
2023-04-12 07:36:35 - training - INFO - Epoch [1/5][601/631] lr: 8.1e-06, eta: 0:17:31.050174, loss: 2.0649
2023-04-12 07:36:38 - training - INFO - Epoch [1/5][611/631] lr: 8.1e-06, eta: 0:17:25.215120, loss: 1.3311
2023-04-12 07:36:42 - training - INFO - Epoch [1/5][621/631] lr: 8.0e-06, eta: 0:17:19.446800, loss: 2.2830
2023-04-12 07:36:46 - training - INFO - Epoch [1/5][631/631] lr: 8.0e-06, eta: 0:17:13.290264, loss: 1.9435
2023-04-12 07:37:11 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 2.6022, Validation Metrics: {'exact_match': 27.59022118742724, 'f1': 37.6143189975964}, Test Metrics: {'exact_match': 29.849012775842045, 'f1': 38.938216767177614}
2023-04-12 07:37:11 - training - INFO - Epoch [2/5][1/631] lr: 8.0e-06, eta: 10 days, 8:57:56.914154, loss: 1.8606
2023-04-12 07:37:15 - training - INFO - Epoch [2/5][11/631] lr: 8.0e-06, eta: 22:51:17.964384, loss: 1.8090
2023-04-12 07:37:19 - training - INFO - Epoch [2/5][21/631] lr: 7.9e-06, eta: 12:05:12.640906, loss: 2.2640
2023-04-12 07:37:23 - training - INFO - Epoch [2/5][31/631] lr: 7.9e-06, eta: 8:15:55.131560, loss: 2.1335
2023-04-12 07:37:26 - training - INFO - Epoch [2/5][41/631] lr: 7.9e-06, eta: 6:18:27.163440, loss: 2.1724
2023-04-12 07:37:30 - training - INFO - Epoch [2/5][51/631] lr: 7.8e-06, eta: 5:07:01.805440, loss: 1.5712
2023-04-12 07:37:34 - training - INFO - Epoch [2/5][61/631] lr: 7.8e-06, eta: 4:19:00.298774, loss: 1.2879
2023-04-12 07:37:37 - training - INFO - Epoch [2/5][71/631] lr: 7.8e-06, eta: 3:44:29.604384, loss: 1.6909
2023-04-12 07:37:41 - training - INFO - Epoch [2/5][81/631] lr: 7.7e-06, eta: 3:18:29.241616, loss: 2.0106
2023-04-12 07:37:45 - training - INFO - Epoch [2/5][91/631] lr: 7.7e-06, eta: 2:58:10.865904, loss: 1.8740
2023-04-12 07:37:48 - training - INFO - Epoch [2/5][101/631] lr: 7.7e-06, eta: 2:41:52.859142, loss: 1.3321
2023-04-12 07:37:52 - training - INFO - Epoch [2/5][111/631] lr: 7.6e-06, eta: 2:28:30.567264, loss: 1.6010
2023-04-12 07:37:56 - training - INFO - Epoch [2/5][121/631] lr: 7.6e-06, eta: 2:17:20.213538, loss: 1.8674
2023-04-12 07:38:00 - training - INFO - Epoch [2/5][131/631] lr: 7.6e-06, eta: 2:07:51.697488, loss: 2.6220
2023-04-12 07:38:03 - training - INFO - Epoch [2/5][141/631] lr: 7.6e-06, eta: 1:59:43.196878, loss: 1.7591
2023-04-12 07:38:07 - training - INFO - Epoch [2/5][151/631] lr: 7.5e-06, eta: 1:52:39.009012, loss: 1.9363
2023-04-12 07:38:11 - training - INFO - Epoch [2/5][161/631] lr: 7.5e-06, eta: 1:46:27.073254, loss: 2.2779
2023-04-12 07:38:14 - training - INFO - Epoch [2/5][171/631] lr: 7.5e-06, eta: 1:40:58.191400, loss: 1.8136
2023-04-12 07:38:18 - training - INFO - Epoch [2/5][181/631] lr: 7.4e-06, eta: 1:36:05.206064, loss: 2.0218
2023-04-12 07:38:22 - training - INFO - Epoch [2/5][191/631] lr: 7.4e-06, eta: 1:31:42.538548, loss: 2.0539
2023-04-12 07:38:26 - training - INFO - Epoch [2/5][201/631] lr: 7.4e-06, eta: 1:27:45.611344, loss: 2.1225
2023-04-12 07:38:29 - training - INFO - Epoch [2/5][211/631] lr: 7.3e-06, eta: 1:24:10.808832, loss: 2.2486
2023-04-12 07:38:33 - training - INFO - Epoch [2/5][221/631] lr: 7.3e-06, eta: 1:20:55.086378, loss: 1.9959
2023-04-12 07:38:37 - training - INFO - Epoch [2/5][231/631] lr: 7.3e-06, eta: 1:17:56.043256, loss: 2.1414
2023-04-12 07:38:40 - training - INFO - Epoch [2/5][241/631] lr: 7.2e-06, eta: 1:15:11.527650, loss: 1.7308
2023-04-12 07:38:44 - training - INFO - Epoch [2/5][251/631] lr: 7.2e-06, eta: 1:12:39.853608, loss: 1.3477
2023-04-12 07:38:48 - training - INFO - Epoch [2/5][261/631] lr: 7.2e-06, eta: 1:10:19.480940, loss: 1.6991
2023-04-12 07:38:51 - training - INFO - Epoch [2/5][271/631] lr: 7.1e-06, eta: 1:08:09.186108, loss: 1.9473
2023-04-12 07:38:55 - training - INFO - Epoch [2/5][281/631] lr: 7.1e-06, eta: 1:06:07.896132, loss: 2.0586
2023-04-12 07:38:59 - training - INFO - Epoch [2/5][291/631] lr: 7.1e-06, eta: 1:04:14.723472, loss: 2.1714
2023-04-12 07:39:03 - training - INFO - Epoch [2/5][301/631] lr: 7.0e-06, eta: 1:02:28.794642, loss: 2.0582
2023-04-12 07:39:06 - training - INFO - Epoch [2/5][311/631] lr: 7.0e-06, eta: 1:00:49.432176, loss: 2.2912
2023-04-12 07:39:10 - training - INFO - Epoch [2/5][321/631] lr: 7.0e-06, eta: 0:59:16.043686, loss: 2.1216
2023-04-12 07:39:14 - training - INFO - Epoch [2/5][331/631] lr: 7.0e-06, eta: 0:57:48.078152, loss: 1.7458
2023-04-12 07:39:17 - training - INFO - Epoch [2/5][341/631] lr: 6.9e-06, eta: 0:56:25.036578, loss: 1.7849
2023-04-12 07:39:21 - training - INFO - Epoch [2/5][351/631] lr: 6.9e-06, eta: 0:55:06.504840, loss: 1.4445
2023-04-12 07:39:25 - training - INFO - Epoch [2/5][361/631] lr: 6.9e-06, eta: 0:53:52.166256, loss: 2.2796
2023-04-12 07:39:29 - training - INFO - Epoch [2/5][371/631] lr: 6.8e-06, eta: 0:52:41.610624, loss: 2.6368
2023-04-12 07:39:32 - training - INFO - Epoch [2/5][381/631] lr: 6.8e-06, eta: 0:51:34.552344, loss: 1.9492
2023-04-12 07:39:36 - training - INFO - Epoch [2/5][391/631] lr: 6.8e-06, eta: 0:50:30.742584, loss: 2.1342
2023-04-12 07:39:40 - training - INFO - Epoch [2/5][401/631] lr: 6.7e-06, eta: 0:49:29.935632, loss: 1.8059
2023-04-12 07:39:43 - training - INFO - Epoch [2/5][411/631] lr: 6.7e-06, eta: 0:48:31.908104, loss: 1.9032
2023-04-12 07:39:47 - training - INFO - Epoch [2/5][421/631] lr: 6.7e-06, eta: 0:47:36.477732, loss: 1.1169
2023-04-12 07:39:51 - training - INFO - Epoch [2/5][431/631] lr: 6.6e-06, eta: 0:46:43.459080, loss: 2.1397
2023-04-12 07:39:55 - training - INFO - Epoch [2/5][441/631] lr: 6.6e-06, eta: 0:45:52.696212, loss: 1.9913
2023-04-12 07:39:58 - training - INFO - Epoch [2/5][451/631] lr: 6.6e-06, eta: 0:45:03.970256, loss: 1.4236
2023-04-12 07:40:02 - training - INFO - Epoch [2/5][461/631] lr: 6.5e-06, eta: 0:44:17.194572, loss: 2.1434
2023-04-12 07:40:06 - training - INFO - Epoch [2/5][471/631] lr: 6.5e-06, eta: 0:43:32.259364, loss: 1.6668
2023-04-12 07:40:09 - training - INFO - Epoch [2/5][481/631] lr: 6.5e-06, eta: 0:42:49.026782, loss: 1.5975
2023-04-12 07:40:13 - training - INFO - Epoch [2/5][491/631] lr: 6.4e-06, eta: 0:42:07.427376, loss: 1.5913
2023-04-12 07:40:17 - training - INFO - Epoch [2/5][501/631] lr: 6.4e-06, eta: 0:41:27.331454, loss: 1.7757
2023-04-12 07:40:20 - training - INFO - Epoch [2/5][511/631] lr: 6.4e-06, eta: 0:40:48.648060, loss: 1.5565
2023-04-12 07:40:24 - training - INFO - Epoch [2/5][521/631] lr: 6.3e-06, eta: 0:40:11.329542, loss: 1.8262
2023-04-12 07:40:28 - training - INFO - Epoch [2/5][531/631] lr: 6.3e-06, eta: 0:39:35.257920, loss: 1.6130
2023-04-12 07:40:32 - training - INFO - Epoch [2/5][541/631] lr: 6.3e-06, eta: 0:39:00.382164, loss: 1.9787
2023-04-12 07:40:35 - training - INFO - Epoch [2/5][551/631] lr: 6.3e-06, eta: 0:38:26.636220, loss: 1.4567
2023-04-12 07:40:39 - training - INFO - Epoch [2/5][561/631] lr: 6.2e-06, eta: 0:37:53.980814, loss: 2.2820
2023-04-12 07:40:43 - training - INFO - Epoch [2/5][571/631] lr: 6.2e-06, eta: 0:37:22.335768, loss: 2.5471
2023-04-12 07:40:46 - training - INFO - Epoch [2/5][581/631] lr: 6.2e-06, eta: 0:36:51.652872, loss: 1.3628
2023-04-12 07:40:50 - training - INFO - Epoch [2/5][591/631] lr: 6.1e-06, eta: 0:36:21.874260, loss: 1.8267
2023-04-12 07:40:54 - training - INFO - Epoch [2/5][601/631] lr: 6.1e-06, eta: 0:35:52.986244, loss: 1.1094
2023-04-12 07:40:58 - training - INFO - Epoch [2/5][611/631] lr: 6.1e-06, eta: 0:35:24.916704, loss: 1.1625
2023-04-12 07:41:01 - training - INFO - Epoch [2/5][621/631] lr: 6.0e-06, eta: 0:34:57.635064, loss: 1.5449
2023-04-12 07:41:05 - training - INFO - Epoch [2/5][631/631] lr: 6.0e-06, eta: 0:34:30.639120, loss: 1.5520
2023-04-12 07:41:30 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.8508, Validation Metrics: {'exact_match': 34.92433061699651, 'f1': 42.40926680994088}, Test Metrics: {'exact_match': 35.19163763066202, 'f1': 42.704138035148496}
2023-04-12 07:41:31 - training - INFO - Epoch [3/5][1/631] lr: 6.0e-06, eta: 19 days, 20:09:43.848184, loss: 0.8352
2023-04-12 07:41:34 - training - INFO - Epoch [3/5][11/631] lr: 6.0e-06, eta: 1 day, 19:26:37.516008, loss: 1.7324
2023-04-12 07:41:38 - training - INFO - Epoch [3/5][21/631] lr: 5.9e-06, eta: 22:50:15.256220, loss: 1.8002
2023-04-12 07:41:42 - training - INFO - Epoch [3/5][31/631] lr: 5.9e-06, eta: 15:31:30.225028, loss: 1.3767
2023-04-12 07:41:46 - training - INFO - Epoch [3/5][41/631] lr: 5.9e-06, eta: 11:46:44.627196, loss: 1.4233
2023-04-12 07:41:49 - training - INFO - Epoch [3/5][51/631] lr: 5.8e-06, eta: 9:30:06.092416, loss: 1.5791
2023-04-12 07:41:53 - training - INFO - Epoch [3/5][61/631] lr: 5.8e-06, eta: 7:58:14.402646, loss: 1.7374
2023-04-12 07:41:57 - training - INFO - Epoch [3/5][71/631] lr: 5.8e-06, eta: 6:52:14.278296, loss: 1.7633
2023-04-12 07:42:00 - training - INFO - Epoch [3/5][81/631] lr: 5.7e-06, eta: 6:02:31.227454, loss: 1.5328
2023-04-12 07:42:04 - training - INFO - Epoch [3/5][91/631] lr: 5.7e-06, eta: 5:23:43.008528, loss: 1.2090
2023-04-12 07:42:08 - training - INFO - Epoch [3/5][101/631] lr: 5.7e-06, eta: 4:52:35.344848, loss: 1.7387
2023-04-12 07:42:12 - training - INFO - Epoch [3/5][111/631] lr: 5.6e-06, eta: 4:27:03.250720, loss: 1.6574
2023-04-12 07:42:15 - training - INFO - Epoch [3/5][121/631] lr: 5.6e-06, eta: 4:05:43.768510, loss: 1.9818
2023-04-12 07:42:19 - training - INFO - Epoch [3/5][131/631] lr: 5.6e-06, eta: 3:47:39.020928, loss: 2.0039
2023-04-12 07:42:23 - training - INFO - Epoch [3/5][141/631] lr: 5.6e-06, eta: 3:32:07.543312, loss: 1.9360
2023-04-12 07:42:26 - training - INFO - Epoch [3/5][151/631] lr: 5.5e-06, eta: 3:18:39.006848, loss: 1.8742
2023-04-12 07:42:30 - training - INFO - Epoch [3/5][161/631] lr: 5.5e-06, eta: 3:06:50.416236, loss: 2.5345
2023-04-12 07:42:34 - training - INFO - Epoch [3/5][171/631] lr: 5.5e-06, eta: 2:56:24.322600, loss: 1.4871
2023-04-12 07:42:38 - training - INFO - Epoch [3/5][181/631] lr: 5.4e-06, eta: 2:47:07.016466, loss: 1.6261
2023-04-12 07:42:41 - training - INFO - Epoch [3/5][191/631] lr: 5.4e-06, eta: 2:38:47.804676, loss: 1.5967
2023-04-12 07:42:45 - training - INFO - Epoch [3/5][201/631] lr: 5.4e-06, eta: 2:31:17.757206, loss: 1.4860
2023-04-12 07:42:49 - training - INFO - Epoch [3/5][211/631] lr: 5.3e-06, eta: 2:24:29.994624, loss: 1.6843
2023-04-12 07:42:52 - training - INFO - Epoch [3/5][221/631] lr: 5.3e-06, eta: 2:18:18.824868, loss: 1.6474
2023-04-12 07:42:56 - training - INFO - Epoch [3/5][231/631] lr: 5.3e-06, eta: 2:12:39.481804, loss: 1.7934
2023-04-12 07:43:00 - training - INFO - Epoch [3/5][241/631] lr: 5.2e-06, eta: 2:07:27.947442, loss: 1.7706
2023-04-12 07:43:03 - training - INFO - Epoch [3/5][251/631] lr: 5.2e-06, eta: 2:02:40.969176, loss: 1.7624
2023-04-12 07:43:07 - training - INFO - Epoch [3/5][261/631] lr: 5.2e-06, eta: 1:58:15.714674, loss: 0.8738
2023-04-12 07:43:11 - training - INFO - Epoch [3/5][271/631] lr: 5.1e-06, eta: 1:54:09.837428, loss: 1.3579
2023-04-12 07:43:15 - training - INFO - Epoch [3/5][281/631] lr: 5.1e-06, eta: 1:50:21.149940, loss: 1.0147
2023-04-12 07:43:18 - training - INFO - Epoch [3/5][291/631] lr: 5.1e-06, eta: 1:46:48.033888, loss: 1.5091
2023-04-12 07:43:22 - training - INFO - Epoch [3/5][301/631] lr: 5.0e-06, eta: 1:43:28.680074, loss: 1.9627
2023-04-12 07:43:26 - training - INFO - Epoch [3/5][311/631] lr: 5.0e-06, eta: 1:40:21.931104, loss: 2.1811
2023-04-12 07:43:29 - training - INFO - Epoch [3/5][321/631] lr: 5.0e-06, eta: 1:37:26.584510, loss: 2.0120
2023-04-12 07:43:33 - training - INFO - Epoch [3/5][331/631] lr: 5.0e-06, eta: 1:34:41.611248, loss: 2.4618
2023-04-12 07:43:37 - training - INFO - Epoch [3/5][341/631] lr: 4.9e-06, eta: 1:32:06.147270, loss: 1.2258
2023-04-12 07:43:41 - training - INFO - Epoch [3/5][351/631] lr: 4.9e-06, eta: 1:29:39.288936, loss: 1.7373
2023-04-12 07:43:44 - training - INFO - Epoch [3/5][361/631] lr: 4.9e-06, eta: 1:27:20.345374, loss: 1.6283
2023-04-12 07:43:48 - training - INFO - Epoch [3/5][371/631] lr: 4.8e-06, eta: 1:25:08.698464, loss: 1.8989
2023-04-12 07:43:52 - training - INFO - Epoch [3/5][381/631] lr: 4.8e-06, eta: 1:23:03.765626, loss: 1.2602
2023-04-12 07:43:55 - training - INFO - Epoch [3/5][391/631] lr: 4.8e-06, eta: 1:21:05.082240, loss: 1.4308
2023-04-12 07:43:59 - training - INFO - Epoch [3/5][401/631] lr: 4.7e-06, eta: 1:19:12.073818, loss: 1.8928
2023-04-12 07:44:03 - training - INFO - Epoch [3/5][411/631] lr: 4.7e-06, eta: 1:17:24.373664, loss: 2.1512
2023-04-12 07:44:07 - training - INFO - Epoch [3/5][421/631] lr: 4.7e-06, eta: 1:15:41.638780, loss: 1.4057
2023-04-12 07:44:10 - training - INFO - Epoch [3/5][431/631] lr: 4.6e-06, eta: 1:14:03.508656, loss: 1.0935
2023-04-12 07:44:14 - training - INFO - Epoch [3/5][441/631] lr: 4.6e-06, eta: 1:12:29.649094, loss: 1.4689
2023-04-12 07:44:18 - training - INFO - Epoch [3/5][451/631] lr: 4.6e-06, eta: 1:10:59.795072, loss: 1.6944
2023-04-12 07:44:21 - training - INFO - Epoch [3/5][461/631] lr: 4.5e-06, eta: 1:09:33.660642, loss: 1.2726
2023-04-12 07:44:25 - training - INFO - Epoch [3/5][471/631] lr: 4.5e-06, eta: 1:08:11.038688, loss: 1.9258
2023-04-12 07:44:29 - training - INFO - Epoch [3/5][481/631] lr: 4.5e-06, eta: 1:06:51.708610, loss: 1.5837
2023-04-12 07:44:33 - training - INFO - Epoch [3/5][491/631] lr: 4.4e-06, eta: 1:05:35.415312, loss: 1.7438
2023-04-12 07:44:36 - training - INFO - Epoch [3/5][501/631] lr: 4.4e-06, eta: 1:04:22.045066, loss: 1.5878
2023-04-12 07:44:40 - training - INFO - Epoch [3/5][511/631] lr: 4.4e-06, eta: 1:03:11.398172, loss: 2.0331
2023-04-12 07:44:44 - training - INFO - Epoch [3/5][521/631] lr: 4.3e-06, eta: 1:02:03.319674, loss: 1.2860
2023-04-12 07:44:47 - training - INFO - Epoch [3/5][531/631] lr: 4.3e-06, eta: 1:00:57.677568, loss: 1.5464
2023-04-12 07:44:51 - training - INFO - Epoch [3/5][541/631] lr: 4.3e-06, eta: 0:59:54.297052, loss: 1.9320
2023-04-12 07:44:55 - training - INFO - Epoch [3/5][551/631] lr: 4.3e-06, eta: 0:58:53.096784, loss: 1.0610
2023-04-12 07:44:59 - training - INFO - Epoch [3/5][561/631] lr: 4.2e-06, eta: 0:57:53.934086, loss: 1.1856
2023-04-12 07:45:02 - training - INFO - Epoch [3/5][571/631] lr: 4.2e-06, eta: 0:56:56.722424, loss: 1.9895
2023-04-12 07:45:06 - training - INFO - Epoch [3/5][581/631] lr: 4.2e-06, eta: 0:56:01.358286, loss: 1.4363
2023-04-12 07:45:10 - training - INFO - Epoch [3/5][591/631] lr: 4.1e-06, eta: 0:55:07.762556, loss: 1.3826
2023-04-12 07:45:13 - training - INFO - Epoch [3/5][601/631] lr: 4.1e-06, eta: 0:54:15.811106, loss: 1.4444
2023-04-12 07:45:17 - training - INFO - Epoch [3/5][611/631] lr: 4.1e-06, eta: 0:53:25.429824, loss: 1.7257
2023-04-12 07:45:21 - training - INFO - Epoch [3/5][621/631] lr: 4.0e-06, eta: 0:52:36.558188, loss: 1.3140
2023-04-12 07:45:24 - training - INFO - Epoch [3/5][631/631] lr: 4.0e-06, eta: 0:51:48.664408, loss: 2.1112
2023-04-12 07:45:50 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 1.6215, Validation Metrics: {'exact_match': 35.0407450523865, 'f1': 43.30401312622805}, Test Metrics: {'exact_match': 34.494773519163765, 'f1': 42.338417164201154}
2023-04-12 07:45:50 - training - INFO - Epoch [4/5][1/631] lr: 4.0e-06, eta: 29 days, 7:33:10.803052, loss: 1.2660
2023-04-12 07:45:54 - training - INFO - Epoch [4/5][11/631] lr: 4.0e-06, eta: 2 days, 16:03:03.245688, loss: 1.4088
2023-04-12 07:45:58 - training - INFO - Epoch [4/5][21/631] lr: 3.9e-06, eta: 1 day, 9:35:50.411856, loss: 1.0615
2023-04-12 07:46:01 - training - INFO - Epoch [4/5][31/631] lr: 3.9e-06, eta: 22:47:26.280536, loss: 1.6634
2023-04-12 07:46:05 - training - INFO - Epoch [4/5][41/631] lr: 3.9e-06, eta: 17:15:17.966124, loss: 1.8899
2023-04-12 07:46:09 - training - INFO - Epoch [4/5][51/631] lr: 3.8e-06, eta: 13:53:23.680032, loss: 1.6857
2023-04-12 07:46:13 - training - INFO - Epoch [4/5][61/631] lr: 3.8e-06, eta: 11:37:39.880062, loss: 2.7442
2023-04-12 07:46:16 - training - INFO - Epoch [4/5][71/631] lr: 3.8e-06, eta: 10:00:08.972124, loss: 1.3872
2023-04-12 07:46:20 - training - INFO - Epoch [4/5][81/631] lr: 3.7e-06, eta: 8:46:41.823566, loss: 1.8582
2023-04-12 07:46:24 - training - INFO - Epoch [4/5][91/631] lr: 3.7e-06, eta: 7:49:22.216736, loss: 1.4937
2023-04-12 07:46:27 - training - INFO - Epoch [4/5][101/631] lr: 3.7e-06, eta: 7:03:23.071218, loss: 1.2780
2023-04-12 07:46:31 - training - INFO - Epoch [4/5][111/631] lr: 3.6e-06, eta: 6:25:40.494088, loss: 1.8151
2023-04-12 07:46:35 - training - INFO - Epoch [4/5][121/631] lr: 3.6e-06, eta: 5:54:11.364770, loss: 2.0228
2023-04-12 07:46:38 - training - INFO - Epoch [4/5][131/631] lr: 3.6e-06, eta: 5:27:30.151584, loss: 1.5151
2023-04-12 07:46:42 - training - INFO - Epoch [4/5][141/631] lr: 3.6e-06, eta: 5:04:35.307622, loss: 1.9877
2023-04-12 07:46:46 - training - INFO - Epoch [4/5][151/631] lr: 3.5e-06, eta: 4:44:42.041728, loss: 1.3258
2023-04-12 07:46:50 - training - INFO - Epoch [4/5][161/631] lr: 3.5e-06, eta: 4:27:16.573578, loss: 1.5288
2023-04-12 07:46:53 - training - INFO - Epoch [4/5][171/631] lr: 3.5e-06, eta: 4:11:52.936488, loss: 1.7321
2023-04-12 07:46:57 - training - INFO - Epoch [4/5][181/631] lr: 3.4e-06, eta: 3:58:10.926512, loss: 1.0983
2023-04-12 07:47:01 - training - INFO - Epoch [4/5][191/631] lr: 3.4e-06, eta: 3:45:54.730644, loss: 1.1944
2023-04-12 07:47:04 - training - INFO - Epoch [4/5][201/631] lr: 3.4e-06, eta: 3:34:51.409608, loss: 1.2982
2023-04-12 07:47:08 - training - INFO - Epoch [4/5][211/631] lr: 3.3e-06, eta: 3:24:50.546432, loss: 1.4833
2023-04-12 07:47:12 - training - INFO - Epoch [4/5][221/631] lr: 3.3e-06, eta: 3:15:43.839648, loss: 1.5042
2023-04-12 07:47:16 - training - INFO - Epoch [4/5][231/631] lr: 3.3e-06, eta: 3:07:24.051940, loss: 1.1067
2023-04-12 07:47:19 - training - INFO - Epoch [4/5][241/631] lr: 3.2e-06, eta: 2:59:45.381306, loss: 1.2486
2023-04-12 07:47:23 - training - INFO - Epoch [4/5][251/631] lr: 3.2e-06, eta: 2:52:43.011120, loss: 1.3397
2023-04-12 07:47:27 - training - INFO - Epoch [4/5][261/631] lr: 3.2e-06, eta: 2:46:12.695060, loss: 1.4319
2023-04-12 07:47:30 - training - INFO - Epoch [4/5][271/631] lr: 3.1e-06, eta: 2:40:10.889624, loss: 1.8629
2023-04-12 07:47:34 - training - INFO - Epoch [4/5][281/631] lr: 3.1e-06, eta: 2:34:34.581936, loss: 2.2992
2023-04-12 07:47:38 - training - INFO - Epoch [4/5][291/631] lr: 3.1e-06, eta: 2:29:21.212560, loss: 1.5101
2023-04-12 07:47:41 - training - INFO - Epoch [4/5][301/631] lr: 3.0e-06, eta: 2:24:28.411390, loss: 1.5005
2023-04-12 07:47:45 - training - INFO - Epoch [4/5][311/631] lr: 3.0e-06, eta: 2:19:54.191136, loss: 1.2766
2023-04-12 07:47:49 - training - INFO - Epoch [4/5][321/631] lr: 3.0e-06, eta: 2:15:36.827764, loss: 1.9428
2023-04-12 07:47:53 - training - INFO - Epoch [4/5][331/631] lr: 3.0e-06, eta: 2:11:34.805464, loss: 1.3676
2023-04-12 07:47:56 - training - INFO - Epoch [4/5][341/631] lr: 2.9e-06, eta: 2:07:46.703604, loss: 1.6618
2023-04-12 07:48:00 - training - INFO - Epoch [4/5][351/631] lr: 2.9e-06, eta: 2:04:11.411288, loss: 1.3457
2023-04-12 07:48:04 - training - INFO - Epoch [4/5][361/631] lr: 2.9e-06, eta: 2:00:47.879078, loss: 1.2375
2023-04-12 07:48:07 - training - INFO - Epoch [4/5][371/631] lr: 2.8e-06, eta: 1:57:35.104224, loss: 1.2399
2023-04-12 07:48:11 - training - INFO - Epoch [4/5][381/631] lr: 2.8e-06, eta: 1:54:32.260442, loss: 1.6304
2023-04-12 07:48:15 - training - INFO - Epoch [4/5][391/631] lr: 2.8e-06, eta: 1:51:38.556764, loss: 1.0736
2023-04-12 07:48:19 - training - INFO - Epoch [4/5][401/631] lr: 2.7e-06, eta: 1:48:53.361018, loss: 1.8528
2023-04-12 07:48:22 - training - INFO - Epoch [4/5][411/631] lr: 2.7e-06, eta: 1:46:15.969376, loss: 1.1775
2023-04-12 07:48:26 - training - INFO - Epoch [4/5][421/631] lr: 2.7e-06, eta: 1:43:45.897608, loss: 1.4829
2023-04-12 07:48:30 - training - INFO - Epoch [4/5][431/631] lr: 2.6e-06, eta: 1:41:22.640244, loss: 1.8853
2023-04-12 07:48:33 - training - INFO - Epoch [4/5][441/631] lr: 2.6e-06, eta: 1:39:05.714498, loss: 1.6801
2023-04-12 07:48:37 - training - INFO - Epoch [4/5][451/631] lr: 2.6e-06, eta: 1:36:54.684304, loss: 1.4246
2023-04-12 07:48:41 - training - INFO - Epoch [4/5][461/631] lr: 2.5e-06, eta: 1:34:49.175730, loss: 1.9188
2023-04-12 07:48:44 - training - INFO - Epoch [4/5][471/631] lr: 2.5e-06, eta: 1:32:48.849088, loss: 1.2976
2023-04-12 07:48:48 - training - INFO - Epoch [4/5][481/631] lr: 2.5e-06, eta: 1:30:53.371644, loss: 1.5233
2023-04-12 07:48:52 - training - INFO - Epoch [4/5][491/631] lr: 2.4e-06, eta: 1:29:02.449536, loss: 1.8170
2023-04-12 07:48:56 - training - INFO - Epoch [4/5][501/631] lr: 2.4e-06, eta: 1:27:15.800584, loss: 1.7113
2023-04-12 07:48:59 - training - INFO - Epoch [4/5][511/631] lr: 2.4e-06, eta: 1:25:33.177936, loss: 1.2843
2023-04-12 07:49:03 - training - INFO - Epoch [4/5][521/631] lr: 2.3e-06, eta: 1:23:54.335226, loss: 1.6118
2023-04-12 07:49:07 - training - INFO - Epoch [4/5][531/631] lr: 2.3e-06, eta: 1:22:19.081728, loss: 1.5830
2023-04-12 07:49:10 - training - INFO - Epoch [4/5][541/631] lr: 2.3e-06, eta: 1:20:47.229076, loss: 1.8546
2023-04-12 07:49:14 - training - INFO - Epoch [4/5][551/631] lr: 2.3e-06, eta: 1:19:18.557412, loss: 1.6731
2023-04-12 07:49:18 - training - INFO - Epoch [4/5][561/631] lr: 2.2e-06, eta: 1:17:52.932766, loss: 1.2733
2023-04-12 07:49:22 - training - INFO - Epoch [4/5][571/631] lr: 2.2e-06, eta: 1:16:30.163336, loss: 1.0992
2023-04-12 07:49:25 - training - INFO - Epoch [4/5][581/631] lr: 2.2e-06, eta: 1:15:10.113894, loss: 1.2082
2023-04-12 07:49:29 - training - INFO - Epoch [4/5][591/631] lr: 2.1e-06, eta: 1:13:52.653456, loss: 2.2919
2023-04-12 07:49:33 - training - INFO - Epoch [4/5][601/631] lr: 2.1e-06, eta: 1:12:37.642462, loss: 1.9595
2023-04-12 07:49:36 - training - INFO - Epoch [4/5][611/631] lr: 2.1e-06, eta: 1:11:24.986400, loss: 1.1363
2023-04-12 07:49:40 - training - INFO - Epoch [4/5][621/631] lr: 2.0e-06, eta: 1:10:14.528528, loss: 1.9965
2023-04-12 07:49:44 - training - INFO - Epoch [4/5][631/631] lr: 2.0e-06, eta: 1:09:05.728052, loss: 1.0526
2023-04-12 07:50:09 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 1.4877, Validation Metrics: {'exact_match': 35.9720605355064, 'f1': 43.18677093744158}, Test Metrics: {'exact_match': 34.61091753774681, 'f1': 42.75323806132538}
2023-04-12 07:50:10 - training - INFO - Epoch [5/5][1/631] lr: 2.0e-06, eta: 38 days, 18:49:13.349858, loss: 0.6500
2023-04-12 07:50:13 - training - INFO - Epoch [5/5][11/631] lr: 2.0e-06, eta: 3 days, 12:38:41.070240, loss: 2.2771
2023-04-12 07:50:17 - training - INFO - Epoch [5/5][21/631] lr: 1.9e-06, eta: 1 day, 20:20:59.567828, loss: 1.4135
2023-04-12 07:50:21 - training - INFO - Epoch [5/5][31/631] lr: 1.9e-06, eta: 1 day, 6:03:04.666700, loss: 1.9194
2023-04-12 07:50:24 - training - INFO - Epoch [5/5][41/631] lr: 1.9e-06, eta: 22:43:37.696872, loss: 1.3241
2023-04-12 07:50:28 - training - INFO - Epoch [5/5][51/631] lr: 1.8e-06, eta: 18:16:29.388640, loss: 1.2454
2023-04-12 07:50:32 - training - INFO - Epoch [5/5][61/631] lr: 1.8e-06, eta: 15:16:54.964732, loss: 1.1333
2023-04-12 07:50:36 - training - INFO - Epoch [5/5][71/631] lr: 1.8e-06, eta: 13:07:54.759360, loss: 1.0095
2023-04-12 07:50:39 - training - INFO - Epoch [5/5][81/631] lr: 1.7e-06, eta: 11:30:44.799232, loss: 1.3709
2023-04-12 07:50:43 - training - INFO - Epoch [5/5][91/631] lr: 1.7e-06, eta: 10:14:55.379672, loss: 0.9078
2023-04-12 07:50:47 - training - INFO - Epoch [5/5][101/631] lr: 1.7e-06, eta: 9:14:06.244074, loss: 1.1992
2023-04-12 07:50:50 - training - INFO - Epoch [5/5][111/631] lr: 1.6e-06, eta: 8:24:13.716332, loss: 1.5908
2023-04-12 07:50:54 - training - INFO - Epoch [5/5][121/631] lr: 1.6e-06, eta: 7:42:35.365740, loss: 1.7155
2023-04-12 07:50:58 - training - INFO - Epoch [5/5][131/631] lr: 1.6e-06, eta: 7:07:17.958864, loss: 1.5945
2023-04-12 07:51:02 - training - INFO - Epoch [5/5][141/631] lr: 1.6e-06, eta: 6:37:00.238772, loss: 1.4102
2023-04-12 07:51:05 - training - INFO - Epoch [5/5][151/631] lr: 1.5e-06, eta: 6:10:42.841632, loss: 1.3796
2023-04-12 07:51:09 - training - INFO - Epoch [5/5][161/631] lr: 1.5e-06, eta: 5:47:41.096196, loss: 1.5562
2023-04-12 07:51:13 - training - INFO - Epoch [5/5][171/631] lr: 1.5e-06, eta: 5:27:20.309032, loss: 1.6474
2023-04-12 07:51:16 - training - INFO - Epoch [5/5][181/631] lr: 1.4e-06, eta: 5:09:14.015734, loss: 1.2930
2023-04-12 07:51:20 - training - INFO - Epoch [5/5][191/631] lr: 1.4e-06, eta: 4:53:01.060848, loss: 1.7573
2023-04-12 07:51:24 - training - INFO - Epoch [5/5][201/631] lr: 1.4e-06, eta: 4:38:24.618910, loss: 1.2929
2023-04-12 07:51:28 - training - INFO - Epoch [5/5][211/631] lr: 1.3e-06, eta: 4:25:10.883328, loss: 1.5410
2023-04-12 07:51:31 - training - INFO - Epoch [5/5][221/631] lr: 1.3e-06, eta: 4:13:08.607972, loss: 1.2282
2023-04-12 07:51:35 - training - INFO - Epoch [5/5][231/631] lr: 1.3e-06, eta: 4:02:08.607456, loss: 1.4496
2023-04-12 07:51:39 - training - INFO - Epoch [5/5][241/631] lr: 1.2e-06, eta: 3:52:02.966698, loss: 1.4768
2023-04-12 07:51:42 - training - INFO - Epoch [5/5][251/631] lr: 1.2e-06, eta: 3:42:45.387024, loss: 1.4889
2023-04-12 07:51:46 - training - INFO - Epoch [5/5][261/631] lr: 1.2e-06, eta: 3:34:10.161638, loss: 1.0532
2023-04-12 07:51:50 - training - INFO - Epoch [5/5][271/631] lr: 1.1e-06, eta: 3:26:12.801252, loss: 1.5368
2023-04-12 07:51:54 - training - INFO - Epoch [5/5][281/631] lr: 1.1e-06, eta: 3:18:49.106052, loss: 1.1266
2023-04-12 07:51:57 - training - INFO - Epoch [5/5][291/631] lr: 1.1e-06, eta: 3:11:55.677168, loss: 1.5237
2023-04-12 07:52:01 - training - INFO - Epoch [5/5][301/631] lr: 1.0e-06, eta: 3:05:29.421298, loss: 1.6529
2023-04-12 07:52:05 - training - INFO - Epoch [5/5][311/631] lr: 1.0e-06, eta: 2:59:27.850416, loss: 1.5361
2023-04-12 07:52:08 - training - INFO - Epoch [5/5][321/631] lr: 9.8e-07, eta: 2:53:48.502188, loss: 0.9610
2023-04-12 07:52:12 - training - INFO - Epoch [5/5][331/631] lr: 9.5e-07, eta: 2:48:29.459688, loss: 1.0882
2023-04-12 07:52:16 - training - INFO - Epoch [5/5][341/631] lr: 9.2e-07, eta: 2:43:28.951152, loss: 1.3242
2023-04-12 07:52:20 - training - INFO - Epoch [5/5][351/631] lr: 8.9e-07, eta: 2:38:45.274924, loss: 0.9544
2023-04-12 07:52:23 - training - INFO - Epoch [5/5][361/631] lr: 8.6e-07, eta: 2:34:17.128298, loss: 2.4173
2023-04-12 07:52:27 - training - INFO - Epoch [5/5][371/631] lr: 8.2e-07, eta: 2:30:03.208224, loss: 1.6500
2023-04-12 07:52:31 - training - INFO - Epoch [5/5][381/631] lr: 7.9e-07, eta: 2:26:02.414110, loss: 1.1127
2023-04-12 07:52:34 - training - INFO - Epoch [5/5][391/631] lr: 7.6e-07, eta: 2:22:13.761552, loss: 1.7315
2023-04-12 07:52:38 - training - INFO - Epoch [5/5][401/631] lr: 7.3e-07, eta: 2:18:36.341928, loss: 1.4911
2023-04-12 07:52:42 - training - INFO - Epoch [5/5][411/631] lr: 7.0e-07, eta: 2:15:09.304784, loss: 1.2732
2023-04-12 07:52:46 - training - INFO - Epoch [5/5][421/631] lr: 6.7e-07, eta: 2:11:51.955408, loss: 1.5469
2023-04-12 07:52:49 - training - INFO - Epoch [5/5][431/631] lr: 6.3e-07, eta: 2:08:43.550604, loss: 1.3359
2023-04-12 07:52:53 - training - INFO - Epoch [5/5][441/631] lr: 6.0e-07, eta: 2:05:43.544002, loss: 0.9594
2023-04-12 07:52:57 - training - INFO - Epoch [5/5][451/631] lr: 5.7e-07, eta: 2:02:51.352768, loss: 2.3680
2023-04-12 07:53:00 - training - INFO - Epoch [5/5][461/631] lr: 5.4e-07, eta: 2:00:06.458082, loss: 1.1973
2023-04-12 07:53:04 - training - INFO - Epoch [5/5][471/631] lr: 5.1e-07, eta: 1:57:28.406772, loss: 1.6647
2023-04-12 07:53:08 - training - INFO - Epoch [5/5][481/631] lr: 4.8e-07, eta: 1:54:56.815562, loss: 2.2777
2023-04-12 07:53:12 - training - INFO - Epoch [5/5][491/631] lr: 4.4e-07, eta: 1:52:31.220688, loss: 1.2402
2023-04-12 07:53:15 - training - INFO - Epoch [5/5][501/631] lr: 4.1e-07, eta: 1:50:11.275894, loss: 1.9284
2023-04-12 07:53:19 - training - INFO - Epoch [5/5][511/631] lr: 3.8e-07, eta: 1:47:56.681588, loss: 1.5710
2023-04-12 07:53:23 - training - INFO - Epoch [5/5][521/631] lr: 3.5e-07, eta: 1:45:47.136630, loss: 1.5794
2023-04-12 07:53:26 - training - INFO - Epoch [5/5][531/631] lr: 3.2e-07, eta: 1:43:42.296448, loss: 1.7337
2023-04-12 07:53:30 - training - INFO - Epoch [5/5][541/631] lr: 2.9e-07, eta: 1:41:41.909866, loss: 1.6118
2023-04-12 07:53:34 - training - INFO - Epoch [5/5][551/631] lr: 2.5e-07, eta: 1:39:45.775740, loss: 1.7682
2023-04-12 07:53:38 - training - INFO - Epoch [5/5][561/631] lr: 2.2e-07, eta: 1:37:53.653862, loss: 1.2634
2023-04-12 07:53:41 - training - INFO - Epoch [5/5][571/631] lr: 1.9e-07, eta: 1:36:05.330360, loss: 1.1594
2023-04-12 07:53:45 - training - INFO - Epoch [5/5][581/631] lr: 1.6e-07, eta: 1:34:20.619822, loss: 1.5192
2023-04-12 07:53:49 - training - INFO - Epoch [5/5][591/631] lr: 1.3e-07, eta: 1:32:39.313516, loss: 1.7129
2023-04-12 07:53:52 - training - INFO - Epoch [5/5][601/631] lr: 9.5e-08, eta: 1:31:01.251402, loss: 1.7228
2023-04-12 07:53:56 - training - INFO - Epoch [5/5][611/631] lr: 6.3e-08, eta: 1:29:26.283072, loss: 1.8316
2023-04-12 07:54:00 - training - INFO - Epoch [5/5][621/631] lr: 3.2e-08, eta: 1:27:54.257464, loss: 1.5982
2023-04-12 07:54:03 - training - INFO - Epoch [5/5][631/631] lr: 0.0e+00, eta: 1:26:24.568592, loss: 1.2231
2023-04-12 07:54:29 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 1.4073, Validation Metrics: {'exact_match': 35.739231664726425, 'f1': 43.05536425024232}, Test Metrics: {'exact_match': 34.146341463414636, 'f1': 42.254702324388745}
2023-04-12 07:54:42 - training - INFO - Final Test - Train Loss: 1.4073, Test Metrics: {'exact_match': 34.146341463414636, 'f1': 42.254702324388745}
