2023-04-11 00:34:36 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-44a167365c0b341b/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'roberta-base'}, 'data': {'task_type': 'list', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 4.54e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/roberta_list_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 594.35it/s]
Map:   0%|          | 0/6878 [00:00<?, ? examples/s]Map:  15%|█▍        | 1000/6878 [00:00<00:03, 1910.09 examples/s]Map:  29%|██▉       | 2000/6878 [00:01<00:02, 2001.55 examples/s]Map:  44%|████▎     | 3000/6878 [00:01<00:01, 2026.89 examples/s]Map:  58%|█████▊    | 4000/6878 [00:01<00:01, 2042.84 examples/s]Map:  73%|███████▎  | 5000/6878 [00:02<00:00, 2053.28 examples/s]Map:  87%|████████▋ | 6000/6878 [00:02<00:00, 2028.45 examples/s]Map: 100%|██████████| 6878/6878 [00:03<00:00, 2039.29 examples/s]                                                                 Map:   0%|          | 0/859 [00:00<?, ? examples/s]Map: 100%|██████████| 859/859 [00:00<00:00, 1548.35 examples/s]                                                               Map:   0%|          | 0/861 [00:00<?, ? examples/s]Map: 100%|██████████| 861/861 [00:00<00:00, 1212.54 examples/s]                                                               Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-11 00:35:14 - training - INFO - First Test - Val Metrics:{'exact_match': 0.23282887077997672, 'f1': 3.122447171232795} Test Metrics: {'exact_match': 0.11614401858304298, 'f1': 3.069263284073827}
2023-04-11 00:35:14 - training - INFO - Epoch [1/5][1/631] lr: 4.5e-05, eta: 22:17:32.757270, loss: 5.9918
2023-04-11 00:35:18 - training - INFO - Epoch [1/5][11/631] lr: 4.5e-05, eta: 2:18:40.989000, loss: 4.0066
2023-04-11 00:35:21 - training - INFO - Epoch [1/5][21/631] lr: 4.5e-05, eta: 1:21:33.239560, loss: 2.7720
2023-04-11 00:35:25 - training - INFO - Epoch [1/5][31/631] lr: 4.5e-05, eta: 1:01:15.485968, loss: 2.5506
2023-04-11 00:35:29 - training - INFO - Epoch [1/5][41/631] lr: 4.5e-05, eta: 0:50:49.340904, loss: 4.0233
2023-04-11 00:35:33 - training - INFO - Epoch [1/5][51/631] lr: 4.5e-05, eta: 0:44:27.692448, loss: 2.6575
2023-04-11 00:35:36 - training - INFO - Epoch [1/5][61/631] lr: 4.5e-05, eta: 0:40:10.154838, loss: 3.2670
2023-04-11 00:35:40 - training - INFO - Epoch [1/5][71/631] lr: 4.4e-05, eta: 0:37:04.208556, loss: 2.5884
2023-04-11 00:35:44 - training - INFO - Epoch [1/5][81/631] lr: 4.4e-05, eta: 0:34:43.323576, loss: 1.9223
2023-04-11 00:35:47 - training - INFO - Epoch [1/5][91/631] lr: 4.4e-05, eta: 0:32:52.615456, loss: 2.8264
2023-04-11 00:35:51 - training - INFO - Epoch [1/5][101/631] lr: 4.4e-05, eta: 0:31:23.249100, loss: 2.8188
2023-04-11 00:35:55 - training - INFO - Epoch [1/5][111/631] lr: 4.4e-05, eta: 0:30:09.368820, loss: 2.6750
2023-04-11 00:35:58 - training - INFO - Epoch [1/5][121/631] lr: 4.4e-05, eta: 0:29:07.016642, loss: 2.4114
2023-04-11 00:36:02 - training - INFO - Epoch [1/5][131/631] lr: 4.4e-05, eta: 0:28:13.585152, loss: 2.8269
2023-04-11 00:36:06 - training - INFO - Epoch [1/5][141/631] lr: 4.3e-05, eta: 0:27:27.247448, loss: 2.4035
2023-04-11 00:36:09 - training - INFO - Epoch [1/5][151/631] lr: 4.3e-05, eta: 0:26:46.515168, loss: 1.5947
2023-04-11 00:36:13 - training - INFO - Epoch [1/5][161/631] lr: 4.3e-05, eta: 0:26:10.433838, loss: 2.7364
2023-04-11 00:36:17 - training - INFO - Epoch [1/5][171/631] lr: 4.3e-05, eta: 0:25:38.099816, loss: 2.7852
2023-04-11 00:36:21 - training - INFO - Epoch [1/5][181/631] lr: 4.3e-05, eta: 0:25:08.957042, loss: 2.2551
2023-04-11 00:36:24 - training - INFO - Epoch [1/5][191/631] lr: 4.3e-05, eta: 0:24:42.503880, loss: 2.5775
2023-04-11 00:36:28 - training - INFO - Epoch [1/5][201/631] lr: 4.3e-05, eta: 0:24:18.345490, loss: 2.3146
2023-04-11 00:36:32 - training - INFO - Epoch [1/5][211/631] lr: 4.2e-05, eta: 0:23:56.094976, loss: 2.8678
2023-04-11 00:36:35 - training - INFO - Epoch [1/5][221/631] lr: 4.2e-05, eta: 0:23:35.549376, loss: 2.9665
2023-04-11 00:36:39 - training - INFO - Epoch [1/5][231/631] lr: 4.2e-05, eta: 0:23:16.557956, loss: 2.4925
2023-04-11 00:36:43 - training - INFO - Epoch [1/5][241/631] lr: 4.2e-05, eta: 0:22:58.744530, loss: 2.7637
2023-04-11 00:36:46 - training - INFO - Epoch [1/5][251/631] lr: 4.2e-05, eta: 0:22:42.036984, loss: 2.5777
2023-04-11 00:36:50 - training - INFO - Epoch [1/5][261/631] lr: 4.2e-05, eta: 0:22:26.297482, loss: 1.7726
2023-04-11 00:36:54 - training - INFO - Epoch [1/5][271/631] lr: 4.2e-05, eta: 0:22:11.464932, loss: 1.5231
2023-04-11 00:36:57 - training - INFO - Epoch [1/5][281/631] lr: 4.1e-05, eta: 0:21:57.499080, loss: 2.2245
2023-04-11 00:37:01 - training - INFO - Epoch [1/5][291/631] lr: 4.1e-05, eta: 0:21:44.242688, loss: 2.2918
2023-04-11 00:37:05 - training - INFO - Epoch [1/5][301/631] lr: 4.1e-05, eta: 0:21:31.611948, loss: 2.7518
2023-04-11 00:37:09 - training - INFO - Epoch [1/5][311/631] lr: 4.1e-05, eta: 0:21:19.552572, loss: 2.5860
2023-04-11 00:37:12 - training - INFO - Epoch [1/5][321/631] lr: 4.1e-05, eta: 0:21:08.002450, loss: 2.6810
2023-04-11 00:37:16 - training - INFO - Epoch [1/5][331/631] lr: 4.1e-05, eta: 0:20:56.908744, loss: 2.5033
2023-04-11 00:37:20 - training - INFO - Epoch [1/5][341/631] lr: 4.0e-05, eta: 0:20:46.236180, loss: 1.6446
2023-04-11 00:37:23 - training - INFO - Epoch [1/5][351/631] lr: 4.0e-05, eta: 0:20:35.983572, loss: 2.2730
2023-04-11 00:37:27 - training - INFO - Epoch [1/5][361/631] lr: 4.0e-05, eta: 0:20:26.088226, loss: 1.4239
2023-04-11 00:37:31 - training - INFO - Epoch [1/5][371/631] lr: 4.0e-05, eta: 0:20:16.532832, loss: 2.6600
2023-04-11 00:37:34 - training - INFO - Epoch [1/5][381/631] lr: 4.0e-05, eta: 0:20:07.300280, loss: 2.5191
2023-04-11 00:37:38 - training - INFO - Epoch [1/5][391/631] lr: 4.0e-05, eta: 0:19:58.340492, loss: 1.8823
2023-04-11 00:37:42 - training - INFO - Epoch [1/5][401/631] lr: 4.0e-05, eta: 0:19:49.664658, loss: 1.9751
2023-04-11 00:37:46 - training - INFO - Epoch [1/5][411/631] lr: 3.9e-05, eta: 0:19:41.215168, loss: 2.2836
2023-04-11 00:37:49 - training - INFO - Epoch [1/5][421/631] lr: 3.9e-05, eta: 0:19:32.987158, loss: 2.0097
2023-04-11 00:37:53 - training - INFO - Epoch [1/5][431/631] lr: 3.9e-05, eta: 0:19:24.973080, loss: 1.8842
2023-04-11 00:37:57 - training - INFO - Epoch [1/5][441/631] lr: 3.9e-05, eta: 0:19:17.146468, loss: 1.5134
2023-04-11 00:38:00 - training - INFO - Epoch [1/5][451/631] lr: 3.9e-05, eta: 0:19:09.519072, loss: 2.8063
2023-04-11 00:38:04 - training - INFO - Epoch [1/5][461/631] lr: 3.9e-05, eta: 0:19:02.088972, loss: 1.9023
2023-04-11 00:38:08 - training - INFO - Epoch [1/5][471/631] lr: 3.9e-05, eta: 0:18:54.822040, loss: 1.8946
2023-04-11 00:38:12 - training - INFO - Epoch [1/5][481/631] lr: 3.8e-05, eta: 0:18:47.695324, loss: 2.5050
2023-04-11 00:38:15 - training - INFO - Epoch [1/5][491/631] lr: 3.8e-05, eta: 0:18:40.688856, loss: 2.0063
2023-04-11 00:38:19 - training - INFO - Epoch [1/5][501/631] lr: 3.8e-05, eta: 0:18:33.828066, loss: 1.8701
2023-04-11 00:38:23 - training - INFO - Epoch [1/5][511/631] lr: 3.8e-05, eta: 0:18:27.095680, loss: 1.9001
2023-04-11 00:38:26 - training - INFO - Epoch [1/5][521/631] lr: 3.8e-05, eta: 0:18:20.458860, loss: 1.7651
2023-04-11 00:38:30 - training - INFO - Epoch [1/5][531/631] lr: 3.8e-05, eta: 0:18:13.919360, loss: 2.0811
2023-04-11 00:38:34 - training - INFO - Epoch [1/5][541/631] lr: 3.8e-05, eta: 0:18:07.505034, loss: 1.9771
2023-04-11 00:38:37 - training - INFO - Epoch [1/5][551/631] lr: 3.7e-05, eta: 0:18:01.199028, loss: 1.7766
2023-04-11 00:38:41 - training - INFO - Epoch [1/5][561/631] lr: 3.7e-05, eta: 0:17:55.002886, loss: 1.7254
2023-04-11 00:38:45 - training - INFO - Epoch [1/5][571/631] lr: 3.7e-05, eta: 0:17:48.866432, loss: 1.4500
2023-04-11 00:38:49 - training - INFO - Epoch [1/5][581/631] lr: 3.7e-05, eta: 0:17:42.807174, loss: 1.3609
2023-04-11 00:38:52 - training - INFO - Epoch [1/5][591/631] lr: 3.7e-05, eta: 0:17:36.837212, loss: 2.1310
2023-04-11 00:38:56 - training - INFO - Epoch [1/5][601/631] lr: 3.7e-05, eta: 0:17:30.958230, loss: 2.4055
2023-04-11 00:39:00 - training - INFO - Epoch [1/5][611/631] lr: 3.7e-05, eta: 0:17:25.136256, loss: 1.0975
2023-04-11 00:39:03 - training - INFO - Epoch [1/5][621/631] lr: 3.6e-05, eta: 0:17:19.393586, loss: 1.7450
2023-04-11 00:39:07 - training - INFO - Epoch [1/5][631/631] lr: 3.6e-05, eta: 0:17:13.252404, loss: 2.5549
2023-04-11 00:39:20 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 2.3178, Validation Metrics: {'exact_match': 31.78114086146682, 'f1': 40.233469155635476}
2023-04-11 00:39:20 - training - INFO - Epoch [2/5][1/631] lr: 3.6e-05, eta: 9 days, 21:41:41.539964, loss: 2.4069
2023-04-11 00:39:24 - training - INFO - Epoch [2/5][11/631] lr: 3.6e-05, eta: 21:50:01.839240, loss: 1.9753
2023-04-11 00:39:27 - training - INFO - Epoch [2/5][21/631] lr: 3.6e-05, eta: 11:33:14.933770, loss: 1.4541
2023-04-11 00:39:31 - training - INFO - Epoch [2/5][31/631] lr: 3.6e-05, eta: 7:54:20.730276, loss: 1.7922
2023-04-11 00:39:35 - training - INFO - Epoch [2/5][41/631] lr: 3.6e-05, eta: 6:02:11.762106, loss: 1.6792
2023-04-11 00:39:39 - training - INFO - Epoch [2/5][51/631] lr: 3.6e-05, eta: 4:54:00.122016, loss: 1.5282
2023-04-11 00:39:42 - training - INFO - Epoch [2/5][61/631] lr: 3.5e-05, eta: 4:08:08.888014, loss: 1.6973
2023-04-11 00:39:46 - training - INFO - Epoch [2/5][71/631] lr: 3.5e-05, eta: 3:35:11.718036, loss: 1.9940
2023-04-11 00:39:50 - training - INFO - Epoch [2/5][81/631] lr: 3.5e-05, eta: 3:10:21.640662, loss: 1.3570
2023-04-11 00:39:53 - training - INFO - Epoch [2/5][91/631] lr: 3.5e-05, eta: 2:50:58.425200, loss: 1.7140
2023-04-11 00:39:57 - training - INFO - Epoch [2/5][101/631] lr: 3.5e-05, eta: 2:35:24.869820, loss: 1.5681
2023-04-11 00:40:01 - training - INFO - Epoch [2/5][111/631] lr: 3.5e-05, eta: 2:22:39.012660, loss: 2.1200
2023-04-11 00:40:04 - training - INFO - Epoch [2/5][121/631] lr: 3.5e-05, eta: 2:11:58.867428, loss: 1.4995
2023-04-11 00:40:08 - training - INFO - Epoch [2/5][131/631] lr: 3.4e-05, eta: 2:02:55.823280, loss: 2.2778
2023-04-11 00:40:12 - training - INFO - Epoch [2/5][141/631] lr: 3.4e-05, eta: 1:55:09.326754, loss: 1.6239
2023-04-11 00:40:16 - training - INFO - Epoch [2/5][151/631] lr: 3.4e-05, eta: 1:48:24.179692, loss: 1.2769
2023-04-11 00:40:19 - training - INFO - Epoch [2/5][161/631] lr: 3.4e-05, eta: 1:42:28.936482, loss: 1.4253
2023-04-11 00:40:23 - training - INFO - Epoch [2/5][171/631] lr: 3.4e-05, eta: 1:37:14.856904, loss: 1.6623
2023-04-11 00:40:27 - training - INFO - Epoch [2/5][181/631] lr: 3.4e-05, eta: 1:32:35.042406, loss: 2.1533
2023-04-11 00:40:30 - training - INFO - Epoch [2/5][191/631] lr: 3.4e-05, eta: 1:28:24.080964, loss: 1.9462
2023-04-11 00:40:34 - training - INFO - Epoch [2/5][201/631] lr: 3.3e-05, eta: 1:24:37.660140, loss: 1.2562
2023-04-11 00:40:38 - training - INFO - Epoch [2/5][211/631] lr: 3.3e-05, eta: 1:21:12.372992, loss: 1.7979
2023-04-11 00:40:42 - training - INFO - Epoch [2/5][221/631] lr: 3.3e-05, eta: 1:18:05.398488, loss: 1.7007
2023-04-11 00:40:45 - training - INFO - Epoch [2/5][231/631] lr: 3.3e-05, eta: 1:15:14.264184, loss: 1.3013
2023-04-11 00:40:49 - training - INFO - Epoch [2/5][241/631] lr: 3.3e-05, eta: 1:12:36.957434, loss: 1.5869
2023-04-11 00:40:53 - training - INFO - Epoch [2/5][251/631] lr: 3.3e-05, eta: 1:10:11.949984, loss: 2.1864
2023-04-11 00:40:56 - training - INFO - Epoch [2/5][261/631] lr: 3.3e-05, eta: 1:07:57.776230, loss: 2.7087
2023-04-11 00:41:00 - training - INFO - Epoch [2/5][271/631] lr: 3.2e-05, eta: 1:05:53.231464, loss: 2.1165
2023-04-11 00:41:04 - training - INFO - Epoch [2/5][281/631] lr: 3.2e-05, eta: 1:03:57.272832, loss: 1.1828
2023-04-11 00:41:08 - training - INFO - Epoch [2/5][291/631] lr: 3.2e-05, eta: 1:02:09.039696, loss: 1.3314
2023-04-11 00:41:11 - training - INFO - Epoch [2/5][301/631] lr: 3.2e-05, eta: 1:00:27.756502, loss: 1.8481
2023-04-11 00:41:15 - training - INFO - Epoch [2/5][311/631] lr: 3.2e-05, eta: 0:58:52.708728, loss: 1.5554
2023-04-11 00:41:19 - training - INFO - Epoch [2/5][321/631] lr: 3.2e-05, eta: 0:57:23.392186, loss: 1.6501
2023-04-11 00:41:22 - training - INFO - Epoch [2/5][331/631] lr: 3.2e-05, eta: 0:55:59.241192, loss: 1.5617
2023-04-11 00:41:26 - training - INFO - Epoch [2/5][341/631] lr: 3.1e-05, eta: 0:54:39.843630, loss: 1.5745
2023-04-11 00:41:30 - training - INFO - Epoch [2/5][351/631] lr: 3.1e-05, eta: 0:53:24.761700, loss: 1.8257
2023-04-11 00:41:34 - training - INFO - Epoch [2/5][361/631] lr: 3.1e-05, eta: 0:52:13.619082, loss: 1.5543
2023-04-11 00:41:37 - training - INFO - Epoch [2/5][371/631] lr: 3.1e-05, eta: 0:51:06.105504, loss: 1.8939
2023-04-11 00:41:41 - training - INFO - Epoch [2/5][381/631] lr: 3.1e-05, eta: 0:50:01.936806, loss: 1.6114
2023-04-11 00:41:45 - training - INFO - Epoch [2/5][391/631] lr: 3.1e-05, eta: 0:49:00.857304, loss: 1.2529
2023-04-11 00:41:48 - training - INFO - Epoch [2/5][401/631] lr: 3.1e-05, eta: 0:48:02.620062, loss: 1.5655
2023-04-11 00:41:52 - training - INFO - Epoch [2/5][411/631] lr: 3.0e-05, eta: 0:47:07.055392, loss: 1.3904
2023-04-11 00:41:56 - training - INFO - Epoch [2/5][421/631] lr: 3.0e-05, eta: 0:46:13.949208, loss: 2.0864
2023-04-11 00:42:00 - training - INFO - Epoch [2/5][431/631] lr: 3.0e-05, eta: 0:45:23.125596, loss: 1.4758
2023-04-11 00:42:03 - training - INFO - Epoch [2/5][441/631] lr: 3.0e-05, eta: 0:44:34.481446, loss: 1.7455
2023-04-11 00:42:07 - training - INFO - Epoch [2/5][451/631] lr: 3.0e-05, eta: 0:43:47.809392, loss: 1.4088
2023-04-11 00:42:11 - training - INFO - Epoch [2/5][461/631] lr: 3.0e-05, eta: 0:43:03.012588, loss: 1.5059
2023-04-11 00:42:14 - training - INFO - Epoch [2/5][471/631] lr: 3.0e-05, eta: 0:42:19.917512, loss: 1.9309
2023-04-11 00:42:18 - training - INFO - Epoch [2/5][481/631] lr: 2.9e-05, eta: 0:41:38.483988, loss: 1.7896
2023-04-11 00:42:22 - training - INFO - Epoch [2/5][491/631] lr: 2.9e-05, eta: 0:40:58.637568, loss: 1.4786
2023-04-11 00:42:26 - training - INFO - Epoch [2/5][501/631] lr: 2.9e-05, eta: 0:40:20.158714, loss: 1.6727
2023-04-11 00:42:29 - training - INFO - Epoch [2/5][511/631] lr: 2.9e-05, eta: 0:39:43.050420, loss: 1.4305
2023-04-11 00:42:33 - training - INFO - Epoch [2/5][521/631] lr: 2.9e-05, eta: 0:39:07.239054, loss: 1.8585
2023-04-11 00:42:37 - training - INFO - Epoch [2/5][531/631] lr: 2.9e-05, eta: 0:38:32.654528, loss: 1.9217
2023-04-11 00:42:40 - training - INFO - Epoch [2/5][541/631] lr: 2.9e-05, eta: 0:37:59.214564, loss: 1.7906
2023-04-11 00:42:44 - training - INFO - Epoch [2/5][551/631] lr: 2.8e-05, eta: 0:37:26.850984, loss: 1.3567
2023-04-11 00:42:48 - training - INFO - Epoch [2/5][561/631] lr: 2.8e-05, eta: 0:36:55.491302, loss: 1.4346
2023-04-11 00:42:52 - training - INFO - Epoch [2/5][571/631] lr: 2.8e-05, eta: 0:36:25.100168, loss: 1.4637
2023-04-11 00:42:55 - training - INFO - Epoch [2/5][581/631] lr: 2.8e-05, eta: 0:35:55.614318, loss: 1.2382
2023-04-11 00:42:59 - training - INFO - Epoch [2/5][591/631] lr: 2.8e-05, eta: 0:35:27.009788, loss: 1.6962
2023-04-11 00:43:03 - training - INFO - Epoch [2/5][601/631] lr: 2.8e-05, eta: 0:34:59.244976, loss: 1.3033
2023-04-11 00:43:06 - training - INFO - Epoch [2/5][611/631] lr: 2.8e-05, eta: 0:34:32.250816, loss: 1.2786
2023-04-11 00:43:10 - training - INFO - Epoch [2/5][621/631] lr: 2.7e-05, eta: 0:34:06.017484, loss: 1.3399
2023-04-11 00:43:14 - training - INFO - Epoch [2/5][631/631] lr: 2.7e-05, eta: 0:33:40.022824, loss: 1.1191
2023-04-11 00:43:26 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.6383, Validation Metrics: {'exact_match': 37.951105937136205, 'f1': 44.53148173232728}
2023-04-11 00:43:27 - training - INFO - Epoch [3/5][1/631] lr: 2.7e-05, eta: 18 days, 21:49:23.761752, loss: 1.3871
2023-04-11 00:43:30 - training - INFO - Epoch [3/5][11/631] lr: 2.7e-05, eta: 1 day, 17:25:11.839920, loss: 2.1041
2023-04-11 00:43:34 - training - INFO - Epoch [3/5][21/631] lr: 2.7e-05, eta: 21:46:50.915558, loss: 1.0982
2023-04-11 00:43:38 - training - INFO - Epoch [3/5][31/631] lr: 2.7e-05, eta: 14:48:41.419184, loss: 1.5971
2023-04-11 00:43:42 - training - INFO - Epoch [3/5][41/631] lr: 2.7e-05, eta: 11:14:29.600052, loss: 1.2068
2023-04-11 00:43:45 - training - INFO - Epoch [3/5][51/631] lr: 2.7e-05, eta: 9:04:15.787200, loss: 1.0426
2023-04-11 00:43:49 - training - INFO - Epoch [3/5][61/631] lr: 2.6e-05, eta: 7:36:42.332776, loss: 1.1890
2023-04-11 00:43:53 - training - INFO - Epoch [3/5][71/631] lr: 2.6e-05, eta: 6:33:47.766852, loss: 1.2178
2023-04-11 00:43:56 - training - INFO - Epoch [3/5][81/631] lr: 2.6e-05, eta: 5:46:24.260792, loss: 1.6335
2023-04-11 00:44:00 - training - INFO - Epoch [3/5][91/631] lr: 2.6e-05, eta: 5:09:24.785192, loss: 0.9139
2023-04-11 00:44:04 - training - INFO - Epoch [3/5][101/631] lr: 2.6e-05, eta: 4:39:44.072418, loss: 1.6315
2023-04-11 00:44:07 - training - INFO - Epoch [3/5][111/631] lr: 2.6e-05, eta: 4:15:23.693860, loss: 1.2360
2023-04-11 00:44:11 - training - INFO - Epoch [3/5][121/631] lr: 2.5e-05, eta: 3:55:03.964658, loss: 1.8134
2023-04-11 00:44:15 - training - INFO - Epoch [3/5][131/631] lr: 2.5e-05, eta: 3:37:49.933632, loss: 1.1049
2023-04-11 00:44:19 - training - INFO - Epoch [3/5][141/631] lr: 2.5e-05, eta: 3:23:02.042466, loss: 2.1673
2023-04-11 00:44:22 - training - INFO - Epoch [3/5][151/631] lr: 2.5e-05, eta: 3:10:11.273772, loss: 1.6086
2023-04-11 00:44:26 - training - INFO - Epoch [3/5][161/631] lr: 2.5e-05, eta: 2:58:55.735500, loss: 1.6428
2023-04-11 00:44:30 - training - INFO - Epoch [3/5][171/631] lr: 2.5e-05, eta: 2:48:58.859144, loss: 0.5759
2023-04-11 00:44:33 - training - INFO - Epoch [3/5][181/631] lr: 2.5e-05, eta: 2:40:07.468338, loss: 1.3535
2023-04-11 00:44:37 - training - INFO - Epoch [3/5][191/631] lr: 2.4e-05, eta: 2:32:11.399316, loss: 1.8793
2023-04-11 00:44:41 - training - INFO - Epoch [3/5][201/631] lr: 2.4e-05, eta: 2:25:02.286082, loss: 1.7827
2023-04-11 00:44:45 - training - INFO - Epoch [3/5][211/631] lr: 2.4e-05, eta: 2:18:33.499776, loss: 1.4403
2023-04-11 00:44:48 - training - INFO - Epoch [3/5][221/631] lr: 2.4e-05, eta: 2:12:39.607524, loss: 1.2949
2023-04-11 00:44:52 - training - INFO - Epoch [3/5][231/631] lr: 2.4e-05, eta: 2:07:15.990912, loss: 1.5710
2023-04-11 00:44:56 - training - INFO - Epoch [3/5][241/631] lr: 2.4e-05, eta: 2:02:18.964366, loss: 1.2643
2023-04-11 00:44:59 - training - INFO - Epoch [3/5][251/631] lr: 2.4e-05, eta: 1:57:45.278088, loss: 1.5505
2023-04-11 00:45:03 - training - INFO - Epoch [3/5][261/631] lr: 2.3e-05, eta: 1:53:32.253162, loss: 1.1438
2023-04-11 00:45:07 - training - INFO - Epoch [3/5][271/631] lr: 2.3e-05, eta: 1:49:37.654160, loss: 1.1324
2023-04-11 00:45:10 - training - INFO - Epoch [3/5][281/631] lr: 2.3e-05, eta: 1:45:59.498106, loss: 0.8842
2023-04-11 00:45:14 - training - INFO - Epoch [3/5][291/631] lr: 2.3e-05, eta: 1:42:36.067760, loss: 1.8780
2023-04-11 00:45:18 - training - INFO - Epoch [3/5][301/631] lr: 2.3e-05, eta: 1:39:25.901710, loss: 0.9541
2023-04-11 00:45:22 - training - INFO - Epoch [3/5][311/631] lr: 2.3e-05, eta: 1:36:27.807336, loss: 1.3408
2023-04-11 00:45:25 - training - INFO - Epoch [3/5][321/631] lr: 2.3e-05, eta: 1:33:40.541836, loss: 1.4501
2023-04-11 00:45:29 - training - INFO - Epoch [3/5][331/631] lr: 2.2e-05, eta: 1:31:03.138136, loss: 1.4784
2023-04-11 00:45:33 - training - INFO - Epoch [3/5][341/631] lr: 2.2e-05, eta: 1:28:34.762404, loss: 1.5895
2023-04-11 00:45:36 - training - INFO - Epoch [3/5][351/631] lr: 2.2e-05, eta: 1:26:14.613760, loss: 1.3383
2023-04-11 00:45:40 - training - INFO - Epoch [3/5][361/631] lr: 2.2e-05, eta: 1:24:02.016078, loss: 1.3590
2023-04-11 00:45:44 - training - INFO - Epoch [3/5][371/631] lr: 2.2e-05, eta: 1:21:56.349120, loss: 1.3355
2023-04-11 00:45:48 - training - INFO - Epoch [3/5][381/631] lr: 2.2e-05, eta: 1:19:57.089296, loss: 1.0961
2023-04-11 00:45:51 - training - INFO - Epoch [3/5][391/631] lr: 2.2e-05, eta: 1:18:03.736200, loss: 1.2565
2023-04-11 00:45:55 - training - INFO - Epoch [3/5][401/631] lr: 2.1e-05, eta: 1:16:15.897684, loss: 1.4954
2023-04-11 00:45:59 - training - INFO - Epoch [3/5][411/631] lr: 2.1e-05, eta: 1:14:33.126112, loss: 1.0563
2023-04-11 00:46:02 - training - INFO - Epoch [3/5][421/631] lr: 2.1e-05, eta: 1:12:55.053426, loss: 1.9403
2023-04-11 00:46:06 - training - INFO - Epoch [3/5][431/631] lr: 2.1e-05, eta: 1:11:21.338040, loss: 1.1694
2023-04-11 00:46:10 - training - INFO - Epoch [3/5][441/631] lr: 2.1e-05, eta: 1:09:51.743146, loss: 1.2644
2023-04-11 00:46:13 - training - INFO - Epoch [3/5][451/631] lr: 2.1e-05, eta: 1:08:25.921248, loss: 1.0198
2023-04-11 00:46:17 - training - INFO - Epoch [3/5][461/631] lr: 2.1e-05, eta: 1:07:03.669498, loss: 1.7222
2023-04-11 00:46:21 - training - INFO - Epoch [3/5][471/631] lr: 2.0e-05, eta: 1:05:44.779476, loss: 1.1511
2023-04-11 00:46:25 - training - INFO - Epoch [3/5][481/631] lr: 2.0e-05, eta: 1:04:29.005252, loss: 1.4300
2023-04-11 00:46:28 - training - INFO - Epoch [3/5][491/631] lr: 2.0e-05, eta: 1:03:16.133400, loss: 1.7516
2023-04-11 00:46:32 - training - INFO - Epoch [3/5][501/631] lr: 2.0e-05, eta: 1:02:06.056760, loss: 1.4078
2023-04-11 00:46:36 - training - INFO - Epoch [3/5][511/631] lr: 2.0e-05, eta: 1:00:58.584764, loss: 1.3331
2023-04-11 00:46:39 - training - INFO - Epoch [3/5][521/631] lr: 2.0e-05, eta: 0:59:53.566200, loss: 2.0952
2023-04-11 00:46:43 - training - INFO - Epoch [3/5][531/631] lr: 2.0e-05, eta: 0:58:50.825536, loss: 1.4554
2023-04-11 00:46:47 - training - INFO - Epoch [3/5][541/631] lr: 1.9e-05, eta: 0:57:50.270594, loss: 1.4275
2023-04-11 00:46:51 - training - INFO - Epoch [3/5][551/631] lr: 1.9e-05, eta: 0:56:51.794652, loss: 1.3223
2023-04-11 00:46:54 - training - INFO - Epoch [3/5][561/631] lr: 1.9e-05, eta: 0:55:55.261180, loss: 1.4932
2023-04-11 00:46:58 - training - INFO - Epoch [3/5][571/631] lr: 1.9e-05, eta: 0:55:00.566456, loss: 1.8094
2023-04-11 00:47:02 - training - INFO - Epoch [3/5][581/631] lr: 1.9e-05, eta: 0:54:07.646688, loss: 1.4534
2023-04-11 00:47:05 - training - INFO - Epoch [3/5][591/631] lr: 1.9e-05, eta: 0:53:16.395216, loss: 0.6909
2023-04-11 00:47:09 - training - INFO - Epoch [3/5][601/631] lr: 1.9e-05, eta: 0:52:26.727212, loss: 1.6382
2023-04-11 00:47:13 - training - INFO - Epoch [3/5][611/631] lr: 1.8e-05, eta: 0:51:38.548752, loss: 1.4440
2023-04-11 00:47:17 - training - INFO - Epoch [3/5][621/631] lr: 1.8e-05, eta: 0:50:51.789958, loss: 1.2867
2023-04-11 00:47:20 - training - INFO - Epoch [3/5][631/631] lr: 1.8e-05, eta: 0:50:05.942656, loss: 1.3444
2023-04-11 00:47:33 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 1.3722, Validation Metrics: {'exact_match': 37.13620488940629, 'f1': 43.55954006457626}
2023-04-11 00:47:33 - training - INFO - Epoch [4/5][1/631] lr: 1.8e-05, eta: 27 days, 21:46:17.350824, loss: 1.3769
2023-04-11 00:47:37 - training - INFO - Epoch [4/5][11/631] lr: 1.8e-05, eta: 2 days, 12:59:20.765256, loss: 1.1816
2023-04-11 00:47:41 - training - INFO - Epoch [4/5][21/631] lr: 1.8e-05, eta: 1 day, 7:59:55.353636, loss: 1.0792
2023-04-11 00:47:44 - training - INFO - Epoch [4/5][31/631] lr: 1.8e-05, eta: 21:42:40.033908, loss: 1.1085
2023-04-11 00:47:48 - training - INFO - Epoch [4/5][41/631] lr: 1.8e-05, eta: 16:26:28.791366, loss: 1.0556
2023-04-11 00:47:52 - training - INFO - Epoch [4/5][51/631] lr: 1.7e-05, eta: 13:14:15.873408, loss: 1.0305
2023-04-11 00:47:55 - training - INFO - Epoch [4/5][61/631] lr: 1.7e-05, eta: 11:05:03.225180, loss: 1.1879
2023-04-11 00:47:59 - training - INFO - Epoch [4/5][71/631] lr: 1.7e-05, eta: 9:32:13.234464, loss: 1.2476
2023-04-11 00:48:03 - training - INFO - Epoch [4/5][81/631] lr: 1.7e-05, eta: 8:22:17.606664, loss: 1.3643
2023-04-11 00:48:07 - training - INFO - Epoch [4/5][91/631] lr: 1.7e-05, eta: 7:27:43.506632, loss: 1.6058
2023-04-11 00:48:10 - training - INFO - Epoch [4/5][101/631] lr: 1.7e-05, eta: 6:43:57.157854, loss: 1.2775
2023-04-11 00:48:14 - training - INFO - Epoch [4/5][111/631] lr: 1.7e-05, eta: 6:08:03.388988, loss: 1.3886
2023-04-11 00:48:18 - training - INFO - Epoch [4/5][121/631] lr: 1.6e-05, eta: 5:38:04.865866, loss: 1.0043
2023-04-11 00:48:21 - training - INFO - Epoch [4/5][131/631] lr: 1.6e-05, eta: 5:12:40.224672, loss: 1.4696
2023-04-11 00:48:25 - training - INFO - Epoch [4/5][141/631] lr: 1.6e-05, eta: 4:50:51.331260, loss: 1.1786
2023-04-11 00:48:29 - training - INFO - Epoch [4/5][151/631] lr: 1.6e-05, eta: 4:31:55.414920, loss: 1.1559
2023-04-11 00:48:33 - training - INFO - Epoch [4/5][161/631] lr: 1.6e-05, eta: 4:15:20.160276, loss: 0.7454
2023-04-11 00:48:36 - training - INFO - Epoch [4/5][171/631] lr: 1.6e-05, eta: 4:00:40.862104, loss: 0.9195
2023-04-11 00:48:40 - training - INFO - Epoch [4/5][181/631] lr: 1.6e-05, eta: 3:47:38.410244, loss: 0.8373
2023-04-11 00:48:44 - training - INFO - Epoch [4/5][191/631] lr: 1.5e-05, eta: 3:35:57.372012, loss: 1.0174
2023-04-11 00:48:47 - training - INFO - Epoch [4/5][201/631] lr: 1.5e-05, eta: 3:25:25.715654, loss: 1.0162
2023-04-11 00:48:51 - training - INFO - Epoch [4/5][211/631] lr: 1.5e-05, eta: 3:15:53.657984, loss: 1.1184
2023-04-11 00:48:55 - training - INFO - Epoch [4/5][221/631] lr: 1.5e-05, eta: 3:07:12.968634, loss: 1.2249
2023-04-11 00:48:58 - training - INFO - Epoch [4/5][231/631] lr: 1.5e-05, eta: 2:59:17.027576, loss: 1.6658
2023-04-11 00:49:02 - training - INFO - Epoch [4/5][241/631] lr: 1.5e-05, eta: 2:52:00.280680, loss: 1.3023
2023-04-11 00:49:06 - training - INFO - Epoch [4/5][251/631] lr: 1.5e-05, eta: 2:45:18.045720, loss: 1.0810
2023-04-11 00:49:10 - training - INFO - Epoch [4/5][261/631] lr: 1.4e-05, eta: 2:39:06.359662, loss: 1.2548
2023-04-11 00:49:13 - training - INFO - Epoch [4/5][271/631] lr: 1.4e-05, eta: 2:33:21.846136, loss: 1.7715
2023-04-11 00:49:17 - training - INFO - Epoch [4/5][281/631] lr: 1.4e-05, eta: 2:28:01.599798, loss: 1.3060
2023-04-11 00:49:21 - training - INFO - Epoch [4/5][291/631] lr: 1.4e-05, eta: 2:23:03.113008, loss: 1.4100
2023-04-11 00:49:24 - training - INFO - Epoch [4/5][301/631] lr: 1.4e-05, eta: 2:18:24.212450, loss: 1.2827
2023-04-11 00:49:28 - training - INFO - Epoch [4/5][311/631] lr: 1.4e-05, eta: 2:14:02.974200, loss: 1.2403
2023-04-11 00:49:32 - training - INFO - Epoch [4/5][321/631] lr: 1.4e-05, eta: 2:09:57.762336, loss: 1.0253
2023-04-11 00:49:36 - training - INFO - Epoch [4/5][331/631] lr: 1.3e-05, eta: 2:06:07.207344, loss: 1.1618
2023-04-11 00:49:39 - training - INFO - Epoch [4/5][341/631] lr: 1.3e-05, eta: 2:02:29.942880, loss: 1.0955
2023-04-11 00:49:43 - training - INFO - Epoch [4/5][351/631] lr: 1.3e-05, eta: 1:59:04.830340, loss: 0.5860
2023-04-11 00:49:47 - training - INFO - Epoch [4/5][361/631] lr: 1.3e-05, eta: 1:55:50.893642, loss: 0.9709
2023-04-11 00:49:50 - training - INFO - Epoch [4/5][371/631] lr: 1.3e-05, eta: 1:52:47.196864, loss: 1.1741
2023-04-11 00:49:54 - training - INFO - Epoch [4/5][381/631] lr: 1.3e-05, eta: 1:49:52.943608, loss: 1.1081
2023-04-11 00:49:58 - training - INFO - Epoch [4/5][391/631] lr: 1.3e-05, eta: 1:47:07.383488, loss: 1.1804
2023-04-11 00:50:02 - training - INFO - Epoch [4/5][401/631] lr: 1.2e-05, eta: 1:44:29.962950, loss: 1.4487
2023-04-11 00:50:05 - training - INFO - Epoch [4/5][411/631] lr: 1.2e-05, eta: 1:41:59.992592, loss: 1.4957
2023-04-11 00:50:09 - training - INFO - Epoch [4/5][421/631] lr: 1.2e-05, eta: 1:39:36.966908, loss: 1.1781
2023-04-11 00:50:13 - training - INFO - Epoch [4/5][431/631] lr: 1.2e-05, eta: 1:37:20.367684, loss: 1.1819
2023-04-11 00:50:16 - training - INFO - Epoch [4/5][441/631] lr: 1.2e-05, eta: 1:35:09.813618, loss: 0.8901
2023-04-11 00:50:20 - training - INFO - Epoch [4/5][451/631] lr: 1.2e-05, eta: 1:33:04.941648, loss: 1.0376
2023-04-11 00:50:24 - training - INFO - Epoch [4/5][461/631] lr: 1.2e-05, eta: 1:31:05.315106, loss: 1.0797
2023-04-11 00:50:28 - training - INFO - Epoch [4/5][471/631] lr: 1.1e-05, eta: 1:29:10.596944, loss: 1.1388
2023-04-11 00:50:31 - training - INFO - Epoch [4/5][481/631] lr: 1.1e-05, eta: 1:27:20.491830, loss: 1.1640
2023-04-11 00:50:35 - training - INFO - Epoch [4/5][491/631] lr: 1.1e-05, eta: 1:25:34.686840, loss: 0.9901
2023-04-11 00:50:39 - training - INFO - Epoch [4/5][501/631] lr: 1.1e-05, eta: 1:23:52.971288, loss: 1.0149
2023-04-11 00:50:42 - training - INFO - Epoch [4/5][511/631] lr: 1.1e-05, eta: 1:22:15.100032, loss: 0.9631
2023-04-11 00:50:46 - training - INFO - Epoch [4/5][521/631] lr: 1.1e-05, eta: 1:20:40.833684, loss: 1.1942
2023-04-11 00:50:50 - training - INFO - Epoch [4/5][531/631] lr: 1.1e-05, eta: 1:19:10.004160, loss: 1.2717
2023-04-11 00:50:54 - training - INFO - Epoch [4/5][541/631] lr: 1.0e-05, eta: 1:17:42.361768, loss: 1.2261
2023-04-11 00:50:57 - training - INFO - Epoch [4/5][551/631] lr: 1.0e-05, eta: 1:16:17.795544, loss: 1.1192
2023-04-11 00:51:01 - training - INFO - Epoch [4/5][561/631] lr: 1.0e-05, eta: 1:14:56.104974, loss: 0.9452
2023-04-11 00:51:05 - training - INFO - Epoch [4/5][571/631] lr: 9.9e-06, eta: 1:13:37.143864, loss: 1.2475
2023-04-11 00:51:08 - training - INFO - Epoch [4/5][581/631] lr: 9.8e-06, eta: 1:12:20.773008, loss: 1.4512
2023-04-11 00:51:12 - training - INFO - Epoch [4/5][591/631] lr: 9.7e-06, eta: 1:11:06.852396, loss: 1.0307
2023-04-11 00:51:16 - training - INFO - Epoch [4/5][601/631] lr: 9.5e-06, eta: 1:09:55.274466, loss: 1.1952
2023-04-11 00:51:20 - training - INFO - Epoch [4/5][611/631] lr: 9.4e-06, eta: 1:08:45.925344, loss: 1.5136
2023-04-11 00:51:23 - training - INFO - Epoch [4/5][621/631] lr: 9.2e-06, eta: 1:07:38.672324, loss: 0.5598
2023-04-11 00:51:27 - training - INFO - Epoch [4/5][631/631] lr: 9.1e-06, eta: 1:06:32.980620, loss: 1.1143
2023-04-11 00:51:40 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 1.1927, Validation Metrics: {'exact_match': 35.85564610011642, 'f1': 42.8233865088957}
2023-04-11 00:51:40 - training - INFO - Epoch [5/5][1/631] lr: 9.1e-06, eta: 36 days, 21:59:32.414232, loss: 1.0439
2023-04-11 00:51:44 - training - INFO - Epoch [5/5][11/631] lr: 8.9e-06, eta: 3 days, 8:34:59.480088, loss: 0.7659
2023-04-11 00:51:47 - training - INFO - Epoch [5/5][21/631] lr: 8.8e-06, eta: 1 day, 18:13:48.052180, loss: 0.8812
2023-04-11 00:51:51 - training - INFO - Epoch [5/5][31/631] lr: 8.6e-06, eta: 1 day, 4:37:12.419072, loss: 1.0927
2023-04-11 00:51:55 - training - INFO - Epoch [5/5][41/631] lr: 8.5e-06, eta: 21:38:55.139874, loss: 1.2689
2023-04-11 00:51:59 - training - INFO - Epoch [5/5][51/631] lr: 8.3e-06, eta: 17:24:38.255648, loss: 0.9338
2023-04-11 00:52:02 - training - INFO - Epoch [5/5][61/631] lr: 8.2e-06, eta: 14:33:42.672302, loss: 0.8220
2023-04-11 00:52:06 - training - INFO - Epoch [5/5][71/631] lr: 8.1e-06, eta: 12:30:54.769716, loss: 0.7074
2023-04-11 00:52:10 - training - INFO - Epoch [5/5][81/631] lr: 7.9e-06, eta: 10:58:25.317338, loss: 0.9035
2023-04-11 00:52:13 - training - INFO - Epoch [5/5][91/631] lr: 7.8e-06, eta: 9:46:14.670976, loss: 1.1468
2023-04-11 00:52:17 - training - INFO - Epoch [5/5][101/631] lr: 7.6e-06, eta: 8:48:20.886480, loss: 1.1906
2023-04-11 00:52:21 - training - INFO - Epoch [5/5][111/631] lr: 7.5e-06, eta: 8:00:52.267864, loss: 1.2962
2023-04-11 00:52:24 - training - INFO - Epoch [5/5][121/631] lr: 7.3e-06, eta: 7:21:13.916398, loss: 0.9508
2023-04-11 00:52:28 - training - INFO - Epoch [5/5][131/631] lr: 7.2e-06, eta: 6:47:38.202720, loss: 0.8561
2023-04-11 00:52:32 - training - INFO - Epoch [5/5][141/631] lr: 7.1e-06, eta: 6:18:47.814472, loss: 1.1155
2023-04-11 00:52:36 - training - INFO - Epoch [5/5][151/631] lr: 6.9e-06, eta: 5:53:46.161864, loss: 0.9939
2023-04-11 00:52:39 - training - INFO - Epoch [5/5][161/631] lr: 6.8e-06, eta: 5:31:50.552094, loss: 0.9862
2023-04-11 00:52:43 - training - INFO - Epoch [5/5][171/631] lr: 6.6e-06, eta: 5:12:28.424256, loss: 0.9791
2023-04-11 00:52:47 - training - INFO - Epoch [5/5][181/631] lr: 6.5e-06, eta: 4:55:14.300886, loss: 1.0282
2023-04-11 00:52:50 - training - INFO - Epoch [5/5][191/631] lr: 6.3e-06, eta: 4:39:48.007080, loss: 0.8952
2023-04-11 00:52:54 - training - INFO - Epoch [5/5][201/631] lr: 6.2e-06, eta: 4:25:53.517146, loss: 0.7892
2023-04-11 00:52:58 - training - INFO - Epoch [5/5][211/631] lr: 6.0e-06, eta: 4:13:17.705216, loss: 1.2450
2023-04-11 00:53:02 - training - INFO - Epoch [5/5][221/631] lr: 5.9e-06, eta: 4:01:50.003112, loss: 0.9886
2023-04-11 00:53:05 - training - INFO - Epoch [5/5][231/631] lr: 5.8e-06, eta: 3:51:21.558420, loss: 0.7798
2023-04-11 00:53:09 - training - INFO - Epoch [5/5][241/631] lr: 5.6e-06, eta: 3:41:44.980148, loss: 1.0939
2023-04-11 00:53:13 - training - INFO - Epoch [5/5][251/631] lr: 5.5e-06, eta: 3:32:54.051312, loss: 1.2751
2023-04-11 00:53:16 - training - INFO - Epoch [5/5][261/631] lr: 5.3e-06, eta: 3:24:43.475922, loss: 1.4584
2023-04-11 00:53:20 - training - INFO - Epoch [5/5][271/631] lr: 5.2e-06, eta: 3:17:08.841360, loss: 1.1151
2023-04-11 00:53:24 - training - INFO - Epoch [5/5][281/631] lr: 5.0e-06, eta: 3:10:06.311082, loss: 0.7846
2023-04-11 00:53:28 - training - INFO - Epoch [5/5][291/631] lr: 4.9e-06, eta: 3:03:32.555424, loss: 1.1406
2023-04-11 00:53:31 - training - INFO - Epoch [5/5][301/631] lr: 4.7e-06, eta: 2:57:24.697938, loss: 1.0524
2023-04-11 00:53:35 - training - INFO - Epoch [5/5][311/631] lr: 4.6e-06, eta: 2:51:40.276908, loss: 0.9094
2023-04-11 00:53:39 - training - INFO - Epoch [5/5][321/631] lr: 4.5e-06, eta: 2:46:17.077162, loss: 0.7578
2023-04-11 00:53:42 - training - INFO - Epoch [5/5][331/631] lr: 4.3e-06, eta: 2:41:13.196872, loss: 0.5211
2023-04-11 00:53:46 - training - INFO - Epoch [5/5][341/631] lr: 4.2e-06, eta: 2:36:26.929944, loss: 1.2705
2023-04-11 00:53:50 - training - INFO - Epoch [5/5][351/631] lr: 4.0e-06, eta: 2:31:56.788204, loss: 1.2419
2023-04-11 00:53:54 - training - INFO - Epoch [5/5][361/631] lr: 3.9e-06, eta: 2:27:41.391726, loss: 1.1815
2023-04-11 00:53:57 - training - INFO - Epoch [5/5][371/631] lr: 3.7e-06, eta: 2:23:39.508992, loss: 1.2269
2023-04-11 00:54:01 - training - INFO - Epoch [5/5][381/631] lr: 3.6e-06, eta: 2:19:50.162728, loss: 0.7832
2023-04-11 00:54:05 - training - INFO - Epoch [5/5][391/631] lr: 3.5e-06, eta: 2:16:12.357496, loss: 0.9484
2023-04-11 00:54:08 - training - INFO - Epoch [5/5][401/631] lr: 3.3e-06, eta: 2:12:45.262008, loss: 1.3090
2023-04-11 00:54:12 - training - INFO - Epoch [5/5][411/631] lr: 3.2e-06, eta: 2:09:28.055456, loss: 1.3084
2023-04-11 00:54:16 - training - INFO - Epoch [5/5][421/631] lr: 3.0e-06, eta: 2:06:20.023202, loss: 0.9104
2023-04-11 00:54:20 - training - INFO - Epoch [5/5][431/631] lr: 2.9e-06, eta: 2:03:20.508720, loss: 1.2451
2023-04-11 00:54:23 - training - INFO - Epoch [5/5][441/631] lr: 2.7e-06, eta: 2:00:28.983260, loss: 0.9063
2023-04-11 00:54:27 - training - INFO - Epoch [5/5][451/631] lr: 2.6e-06, eta: 1:57:44.927376, loss: 1.0977
2023-04-11 00:54:31 - training - INFO - Epoch [5/5][461/631] lr: 2.4e-06, eta: 1:55:07.809324, loss: 1.0399
2023-04-11 00:54:34 - training - INFO - Epoch [5/5][471/631] lr: 2.3e-06, eta: 1:52:37.198140, loss: 1.5436
2023-04-11 00:54:38 - training - INFO - Epoch [5/5][481/631] lr: 2.2e-06, eta: 1:50:12.711084, loss: 1.0721
2023-04-11 00:54:42 - training - INFO - Epoch [5/5][491/631] lr: 2.0e-06, eta: 1:47:53.964888, loss: 1.3387
2023-04-11 00:54:46 - training - INFO - Epoch [5/5][501/631] lr: 1.9e-06, eta: 1:45:40.583818, loss: 1.1840
2023-04-11 00:54:49 - training - INFO - Epoch [5/5][511/631] lr: 1.7e-06, eta: 1:43:32.278944, loss: 1.1452
2023-04-11 00:54:53 - training - INFO - Epoch [5/5][521/631] lr: 1.6e-06, eta: 1:41:28.780740, loss: 1.1582
2023-04-11 00:54:57 - training - INFO - Epoch [5/5][531/631] lr: 1.4e-06, eta: 1:39:29.791552, loss: 1.1293
2023-04-11 00:55:00 - training - INFO - Epoch [5/5][541/631] lr: 1.3e-06, eta: 1:37:35.062004, loss: 1.0439
2023-04-11 00:55:04 - training - INFO - Epoch [5/5][551/631] lr: 1.2e-06, eta: 1:35:44.335464, loss: 1.2119
2023-04-11 00:55:08 - training - INFO - Epoch [5/5][561/631] lr: 1.0e-06, eta: 1:33:57.449410, loss: 1.4742
2023-04-11 00:55:12 - training - INFO - Epoch [5/5][571/631] lr: 8.6e-07, eta: 1:32:14.165720, loss: 0.6980
2023-04-11 00:55:15 - training - INFO - Epoch [5/5][581/631] lr: 7.2e-07, eta: 1:30:34.306020, loss: 1.2680
2023-04-11 00:55:19 - training - INFO - Epoch [5/5][591/631] lr: 5.8e-07, eta: 1:28:57.701868, loss: 1.6410
2023-04-11 00:55:23 - training - INFO - Epoch [5/5][601/631] lr: 4.3e-07, eta: 1:27:24.204820, loss: 0.8278
2023-04-11 00:55:26 - training - INFO - Epoch [5/5][611/631] lr: 2.9e-07, eta: 1:25:53.645376, loss: 1.1431
2023-04-11 00:55:30 - training - INFO - Epoch [5/5][621/631] lr: 1.4e-07, eta: 1:24:25.868906, loss: 0.9264
2023-04-11 00:55:34 - training - INFO - Epoch [5/5][631/631] lr: 0.0e+00, eta: 1:23:00.293700, loss: 1.3564
2023-04-11 00:55:46 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 1.0718, Validation Metrics: {'exact_match': 35.0407450523865, 'f1': 43.10653040632821}
2023-04-11 00:55:59 - training - INFO - Final Test - Train Loss: 1.0718, Test Metrics: {'exact_match': 35.77235772357724, 'f1': 42.67655592568484}
