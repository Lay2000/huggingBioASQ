{'model': {'model_checkpoint': 'roberta-base'}, 'data': {'task_type': 'list', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 4.54e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/roberta_list_base'}}
Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-44a167365c0b341b/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e...
Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 3/3 [00:00<00:00, 15270.52it/s]
Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 207.11it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00,  9.73 examples/s]                                                            Generating val split: 0 examples [00:00, ? examples/s]                                                      Generating test split: 0 examples [00:00, ? examples/s]                                                       Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-44a167365c0b341b/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e. Subsequent calls will reuse this data.
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 274.44it/s]
Map:   0%|          | 0/6878 [00:00<?, ? examples/s]Map:  15%|█▍        | 1000/6878 [00:00<00:03, 1911.67 examples/s]Map:  29%|██▉       | 2000/6878 [00:01<00:02, 2013.92 examples/s]Map:  44%|████▎     | 3000/6878 [00:01<00:01, 2024.67 examples/s]Map:  58%|█████▊    | 4000/6878 [00:01<00:01, 2029.16 examples/s]Map:  73%|███████▎  | 5000/6878 [00:02<00:00, 2026.87 examples/s]Map:  87%|████████▋ | 6000/6878 [00:02<00:00, 2026.80 examples/s]Map: 100%|██████████| 6878/6878 [00:03<00:00, 1842.83 examples/s]                                                                 Map:   0%|          | 0/859 [00:00<?, ? examples/s]Map: 100%|██████████| 859/859 [00:00<00:00, 1562.84 examples/s]                                                               Map:   0%|          | 0/861 [00:00<?, ? examples/s]Map: 100%|██████████| 861/861 [00:00<00:00, 1549.76 examples/s]                                                               Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-09 19:19:13 - training - INFO - Epoch [1/5][1/631] lr: 4.5e-05, eta: 0:27:00.481044, loss: 5.9572
2023-04-09 19:19:16 - training - INFO - Epoch [1/5][11/631] lr: 4.5e-05, eta: 0:20:09.955824, loss: 4.0624
2023-04-09 19:19:20 - training - INFO - Epoch [1/5][21/631] lr: 4.5e-05, eta: 0:19:48.218492, loss: 3.4454
2023-04-09 19:19:24 - training - INFO - Epoch [1/5][31/631] lr: 4.5e-05, eta: 0:19:38.138500, loss: 3.3032
2023-04-09 19:19:28 - training - INFO - Epoch [1/5][41/631] lr: 4.5e-05, eta: 0:19:31.583334, loss: 3.1337
2023-04-09 19:19:31 - training - INFO - Epoch [1/5][51/631] lr: 4.5e-05, eta: 0:19:26.508032, loss: 3.3591
2023-04-09 19:19:35 - training - INFO - Epoch [1/5][61/631] lr: 4.5e-05, eta: 0:19:21.843410, loss: 2.3642
2023-04-09 19:19:39 - training - INFO - Epoch [1/5][71/631] lr: 4.4e-05, eta: 0:19:17.388192, loss: 2.4838
2023-04-09 19:19:43 - training - INFO - Epoch [1/5][81/631] lr: 4.4e-05, eta: 0:19:13.361726, loss: 2.0306
2023-04-09 19:19:46 - training - INFO - Epoch [1/5][91/631] lr: 4.4e-05, eta: 0:19:09.419768, loss: 1.8591
2023-04-09 19:19:50 - training - INFO - Epoch [1/5][101/631] lr: 4.4e-05, eta: 0:19:05.408808, loss: 2.8926
2023-04-09 19:19:54 - training - INFO - Epoch [1/5][111/631] lr: 4.4e-05, eta: 0:19:01.566968, loss: 2.5030
2023-04-09 19:19:57 - training - INFO - Epoch [1/5][121/631] lr: 4.4e-05, eta: 0:18:57.783374, loss: 3.0109
2023-04-09 19:20:01 - training - INFO - Epoch [1/5][131/631] lr: 4.4e-05, eta: 0:18:54.081648, loss: 2.3506
2023-04-09 19:20:05 - training - INFO - Epoch [1/5][141/631] lr: 4.3e-05, eta: 0:18:50.442896, loss: 2.9456
2023-04-09 19:20:09 - training - INFO - Epoch [1/5][151/631] lr: 4.3e-05, eta: 0:18:46.803404, loss: 2.0802
2023-04-09 19:20:13 - training - INFO - Epoch [1/5][161/631] lr: 4.3e-05, eta: 0:18:43.010478, loss: 1.8059
2023-04-09 19:20:16 - training - INFO - Epoch [1/5][171/631] lr: 4.3e-05, eta: 0:18:39.298400, loss: 2.3048
2023-04-09 19:20:20 - training - INFO - Epoch [1/5][181/631] lr: 4.3e-05, eta: 0:18:35.562270, loss: 2.5311
2023-04-09 19:20:24 - training - INFO - Epoch [1/5][191/631] lr: 4.3e-05, eta: 0:18:31.837896, loss: 1.4731
2023-04-09 19:20:28 - training - INFO - Epoch [1/5][201/631] lr: 4.3e-05, eta: 0:18:28.110388, loss: 2.9950
2023-04-09 19:20:31 - training - INFO - Epoch [1/5][211/631] lr: 4.2e-05, eta: 0:18:24.456320, loss: 1.8698
2023-04-09 19:20:35 - training - INFO - Epoch [1/5][221/631] lr: 4.2e-05, eta: 0:18:20.728242, loss: 2.2704
2023-04-09 19:20:39 - training - INFO - Epoch [1/5][231/631] lr: 4.2e-05, eta: 0:18:17.052636, loss: 2.5087
2023-04-09 19:20:43 - training - INFO - Epoch [1/5][241/631] lr: 4.2e-05, eta: 0:18:13.341542, loss: 1.9474
2023-04-09 19:20:46 - training - INFO - Epoch [1/5][251/631] lr: 4.2e-05, eta: 0:18:09.731808, loss: 1.6761
2023-04-09 19:20:50 - training - INFO - Epoch [1/5][261/631] lr: 4.2e-05, eta: 0:18:06.045850, loss: 2.3810
2023-04-09 19:20:54 - training - INFO - Epoch [1/5][271/631] lr: 4.2e-05, eta: 0:18:02.359432, loss: 1.8668
2023-04-09 19:20:58 - training - INFO - Epoch [1/5][281/631] lr: 4.1e-05, eta: 0:17:58.686924, loss: 2.7397
2023-04-09 19:21:01 - training - INFO - Epoch [1/5][291/631] lr: 4.1e-05, eta: 0:17:55.019584, loss: 1.7367
2023-04-09 19:21:05 - training - INFO - Epoch [1/5][301/631] lr: 4.1e-05, eta: 0:17:51.377330, loss: 1.9172
2023-04-09 19:21:09 - training - INFO - Epoch [1/5][311/631] lr: 4.1e-05, eta: 0:17:47.751360, loss: 1.5338
2023-04-09 19:21:13 - training - INFO - Epoch [1/5][321/631] lr: 4.1e-05, eta: 0:17:44.147162, loss: 2.0304
2023-04-09 19:21:16 - training - INFO - Epoch [1/5][331/631] lr: 4.1e-05, eta: 0:17:40.541904, loss: 2.1191
2023-04-09 19:21:20 - training - INFO - Epoch [1/5][341/631] lr: 4.0e-05, eta: 0:17:36.873678, loss: 2.2637
2023-04-09 19:21:24 - training - INFO - Epoch [1/5][351/631] lr: 4.0e-05, eta: 0:17:33.207636, loss: 2.0314
2023-04-09 19:21:28 - training - INFO - Epoch [1/5][361/631] lr: 4.0e-05, eta: 0:17:29.577276, loss: 2.2198
2023-04-09 19:21:32 - training - INFO - Epoch [1/5][371/631] lr: 4.0e-05, eta: 0:17:25.912608, loss: 2.0359
2023-04-09 19:21:35 - training - INFO - Epoch [1/5][381/631] lr: 4.0e-05, eta: 0:17:22.258376, loss: 2.4573
2023-04-09 19:21:39 - training - INFO - Epoch [1/5][391/631] lr: 4.0e-05, eta: 0:17:18.619988, loss: 1.6332
2023-04-09 19:21:43 - training - INFO - Epoch [1/5][401/631] lr: 4.0e-05, eta: 0:17:14.986248, loss: 1.7377
2023-04-09 19:21:47 - training - INFO - Epoch [1/5][411/631] lr: 3.9e-05, eta: 0:17:11.335144, loss: 2.0455
2023-04-09 19:21:50 - training - INFO - Epoch [1/5][421/631] lr: 3.9e-05, eta: 0:17:07.694196, loss: 1.5954
2023-04-09 19:21:54 - training - INFO - Epoch [1/5][431/631] lr: 3.9e-05, eta: 0:17:04.060560, loss: 2.3666
2023-04-09 19:21:58 - training - INFO - Epoch [1/5][441/631] lr: 3.9e-05, eta: 0:17:00.412434, loss: 1.7246
2023-04-09 19:22:02 - training - INFO - Epoch [1/5][451/631] lr: 3.9e-05, eta: 0:16:56.752672, loss: 1.8698
2023-04-09 19:22:05 - training - INFO - Epoch [1/5][461/631] lr: 3.9e-05, eta: 0:16:53.092170, loss: 2.0570
2023-04-09 19:22:09 - training - INFO - Epoch [1/5][471/631] lr: 3.9e-05, eta: 0:16:49.449716, loss: 1.7359
2023-04-09 19:22:13 - training - INFO - Epoch [1/5][481/631] lr: 3.8e-05, eta: 0:16:45.835796, loss: 1.6139
2023-04-09 19:22:17 - training - INFO - Epoch [1/5][491/631] lr: 3.8e-05, eta: 0:16:42.223440, loss: 1.6969
2023-04-09 19:22:21 - training - INFO - Epoch [1/5][501/631] lr: 3.8e-05, eta: 0:16:38.596694, loss: 2.3471
2023-04-09 19:22:24 - training - INFO - Epoch [1/5][511/631] lr: 3.8e-05, eta: 0:16:34.929268, loss: 2.1461
2023-04-09 19:22:28 - training - INFO - Epoch [1/5][521/631] lr: 3.8e-05, eta: 0:16:31.269024, loss: 2.4527
2023-04-09 19:22:32 - training - INFO - Epoch [1/5][531/631] lr: 3.8e-05, eta: 0:16:27.600128, loss: 1.5590
2023-04-09 19:22:36 - training - INFO - Epoch [1/5][541/631] lr: 3.8e-05, eta: 0:16:23.922670, loss: 1.7500
2023-04-09 19:22:40 - training - INFO - Epoch [1/5][551/631] lr: 3.7e-05, eta: 0:16:20.260176, loss: 1.4282
2023-04-09 19:22:43 - training - INFO - Epoch [1/5][561/631] lr: 3.7e-05, eta: 0:16:16.615060, loss: 1.4486
2023-04-09 19:22:47 - training - INFO - Epoch [1/5][571/631] lr: 3.7e-05, eta: 0:16:12.976776, loss: 2.3966
2023-04-09 19:22:51 - training - INFO - Epoch [1/5][581/631] lr: 3.7e-05, eta: 0:16:09.324642, loss: 2.1389
2023-04-09 19:22:55 - training - INFO - Epoch [1/5][591/631] lr: 3.7e-05, eta: 0:16:05.687012, loss: 2.0513
2023-04-09 19:22:59 - training - INFO - Epoch [1/5][601/631] lr: 3.7e-05, eta: 0:16:02.045828, loss: 1.9214
2023-04-09 19:23:02 - training - INFO - Epoch [1/5][611/631] lr: 3.7e-05, eta: 0:15:58.406208, loss: 1.8387
2023-04-09 19:23:06 - training - INFO - Epoch [1/5][621/631] lr: 3.6e-05, eta: 0:15:54.757986, loss: 3.1809
2023-04-09 19:23:10 - training - INFO - Epoch [1/5][631/631] lr: 3.6e-05, eta: 0:15:50.601500, loss: 2.1011
2023-04-09 19:23:23 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 2.2531, Validation Metrics: {'exact_match': 34.69150174621653, 'f1': 42.45822192750904}
2023-04-09 19:23:23 - training - INFO - Epoch [2/5][1/631] lr: 3.6e-05, eta: 9 days, 3:57:47.948436, loss: 1.2857
2023-04-09 19:23:27 - training - INFO - Epoch [2/5][11/631] lr: 3.6e-05, eta: 20:14:02.996448, loss: 1.7597
2023-04-09 19:23:31 - training - INFO - Epoch [2/5][21/631] lr: 3.6e-05, eta: 10:43:20.055164, loss: 1.1005
2023-04-09 19:23:35 - training - INFO - Epoch [2/5][31/631] lr: 3.6e-05, eta: 7:20:46.971760, loss: 1.2932
2023-04-09 19:23:38 - training - INFO - Epoch [2/5][41/631] lr: 3.6e-05, eta: 5:36:59.871510, loss: 1.4082
2023-04-09 19:23:42 - training - INFO - Epoch [2/5][51/631] lr: 3.6e-05, eta: 4:33:53.637568, loss: 1.4243
2023-04-09 19:23:46 - training - INFO - Epoch [2/5][61/631] lr: 3.5e-05, eta: 3:51:27.663426, loss: 1.3885
2023-04-09 19:23:50 - training - INFO - Epoch [2/5][71/631] lr: 3.5e-05, eta: 3:20:57.517884, loss: 1.8201
2023-04-09 19:23:53 - training - INFO - Epoch [2/5][81/631] lr: 3.5e-05, eta: 2:57:58.184540, loss: 1.1273
2023-04-09 19:23:57 - training - INFO - Epoch [2/5][91/631] lr: 3.5e-05, eta: 2:40:01.414744, loss: 2.0654
2023-04-09 19:24:01 - training - INFO - Epoch [2/5][101/631] lr: 3.5e-05, eta: 2:25:36.880146, loss: 1.4937
2023-04-09 19:24:05 - training - INFO - Epoch [2/5][111/631] lr: 3.5e-05, eta: 2:13:47.511996, loss: 1.8031
2023-04-09 19:24:09 - training - INFO - Epoch [2/5][121/631] lr: 3.5e-05, eta: 2:03:54.707776, loss: 1.8702
2023-04-09 19:24:12 - training - INFO - Epoch [2/5][131/631] lr: 3.4e-05, eta: 1:55:31.815408, loss: 1.4557
2023-04-09 19:24:16 - training - INFO - Epoch [2/5][141/631] lr: 3.4e-05, eta: 1:48:19.697028, loss: 2.1947
2023-04-09 19:24:20 - training - INFO - Epoch [2/5][151/631] lr: 3.4e-05, eta: 1:42:04.275828, loss: 1.1629
2023-04-09 19:24:24 - training - INFO - Epoch [2/5][161/631] lr: 3.4e-05, eta: 1:36:35.375022, loss: 1.8135
2023-04-09 19:24:28 - training - INFO - Epoch [2/5][171/631] lr: 3.4e-05, eta: 1:31:44.325192, loss: 1.9018
2023-04-09 19:24:31 - training - INFO - Epoch [2/5][181/631] lr: 3.4e-05, eta: 1:27:25.068334, loss: 1.3500
2023-04-09 19:24:35 - training - INFO - Epoch [2/5][191/631] lr: 3.4e-05, eta: 1:23:32.518212, loss: 2.0750
2023-04-09 19:24:39 - training - INFO - Epoch [2/5][201/631] lr: 3.3e-05, eta: 1:20:02.722498, loss: 1.3753
2023-04-09 19:24:43 - training - INFO - Epoch [2/5][211/631] lr: 3.3e-05, eta: 1:16:52.547328, loss: 1.4219
2023-04-09 19:24:46 - training - INFO - Epoch [2/5][221/631] lr: 3.3e-05, eta: 1:13:59.159604, loss: 1.9215
2023-04-09 19:24:50 - training - INFO - Epoch [2/5][231/631] lr: 3.3e-05, eta: 1:11:20.481612, loss: 1.8002
2023-04-09 19:24:54 - training - INFO - Epoch [2/5][241/631] lr: 3.3e-05, eta: 1:08:54.558040, loss: 1.4908
2023-04-09 19:24:58 - training - INFO - Epoch [2/5][251/631] lr: 3.3e-05, eta: 1:06:39.984120, loss: 1.5075
2023-04-09 19:25:02 - training - INFO - Epoch [2/5][261/631] lr: 3.3e-05, eta: 1:04:35.462478, loss: 1.6446
2023-04-09 19:25:05 - training - INFO - Epoch [2/5][271/631] lr: 3.2e-05, eta: 1:02:39.890988, loss: 1.2090
2023-04-09 19:25:09 - training - INFO - Epoch [2/5][281/631] lr: 3.2e-05, eta: 1:00:52.247586, loss: 1.2412
2023-04-09 19:25:13 - training - INFO - Epoch [2/5][291/631] lr: 3.2e-05, eta: 0:59:11.792464, loss: 1.3031
2023-04-09 19:25:17 - training - INFO - Epoch [2/5][301/631] lr: 3.2e-05, eta: 0:57:37.752284, loss: 2.6020
2023-04-09 19:25:21 - training - INFO - Epoch [2/5][311/631] lr: 3.2e-05, eta: 0:56:09.517164, loss: 1.0871
2023-04-09 19:25:24 - training - INFO - Epoch [2/5][321/631] lr: 3.2e-05, eta: 0:54:46.535954, loss: 1.4851
2023-04-09 19:25:28 - training - INFO - Epoch [2/5][331/631] lr: 3.2e-05, eta: 0:53:28.340752, loss: 1.0643
2023-04-09 19:25:32 - training - INFO - Epoch [2/5][341/631] lr: 3.1e-05, eta: 0:52:14.663742, loss: 1.5256
2023-04-09 19:25:36 - training - INFO - Epoch [2/5][351/631] lr: 3.1e-05, eta: 0:51:04.917808, loss: 1.7680
2023-04-09 19:25:40 - training - INFO - Epoch [2/5][361/631] lr: 3.1e-05, eta: 0:49:58.819758, loss: 1.5858
2023-04-09 19:25:43 - training - INFO - Epoch [2/5][371/631] lr: 3.1e-05, eta: 0:48:56.078784, loss: 1.6363
2023-04-09 19:25:47 - training - INFO - Epoch [2/5][381/631] lr: 3.1e-05, eta: 0:47:56.385566, loss: 1.4905
2023-04-09 19:25:51 - training - INFO - Epoch [2/5][391/631] lr: 3.1e-05, eta: 0:46:59.550872, loss: 1.5903
2023-04-09 19:25:55 - training - INFO - Epoch [2/5][401/631] lr: 3.1e-05, eta: 0:46:05.357496, loss: 1.4519
2023-04-09 19:25:59 - training - INFO - Epoch [2/5][411/631] lr: 3.0e-05, eta: 0:45:13.626664, loss: 1.2342
2023-04-09 19:26:02 - training - INFO - Epoch [2/5][421/631] lr: 3.0e-05, eta: 0:44:24.181842, loss: 1.3092
2023-04-09 19:26:06 - training - INFO - Epoch [2/5][431/631] lr: 3.0e-05, eta: 0:43:36.832392, loss: 1.9786
2023-04-09 19:26:10 - training - INFO - Epoch [2/5][441/631] lr: 3.0e-05, eta: 0:42:51.517714, loss: 1.6865
2023-04-09 19:26:14 - training - INFO - Epoch [2/5][451/631] lr: 3.0e-05, eta: 0:42:08.037200, loss: 1.3747
2023-04-09 19:26:18 - training - INFO - Epoch [2/5][461/631] lr: 3.0e-05, eta: 0:41:26.244108, loss: 1.3585
2023-04-09 19:26:21 - training - INFO - Epoch [2/5][471/631] lr: 3.0e-05, eta: 0:40:46.060716, loss: 1.3523
2023-04-09 19:26:25 - training - INFO - Epoch [2/5][481/631] lr: 2.9e-05, eta: 0:40:07.391504, loss: 1.5912
2023-04-09 19:26:29 - training - INFO - Epoch [2/5][491/631] lr: 2.9e-05, eta: 0:39:30.152808, loss: 1.3968
2023-04-09 19:26:33 - training - INFO - Epoch [2/5][501/631] lr: 2.9e-05, eta: 0:38:54.219540, loss: 1.8451
2023-04-09 19:26:37 - training - INFO - Epoch [2/5][511/631] lr: 2.9e-05, eta: 0:38:19.571408, loss: 1.5522
2023-04-09 19:26:40 - training - INFO - Epoch [2/5][521/631] lr: 2.9e-05, eta: 0:37:46.138194, loss: 1.1261
2023-04-09 19:26:44 - training - INFO - Epoch [2/5][531/631] lr: 2.9e-05, eta: 0:37:13.853184, loss: 1.3008
2023-04-09 19:26:48 - training - INFO - Epoch [2/5][541/631] lr: 2.9e-05, eta: 0:36:42.590382, loss: 1.2812
2023-04-09 19:26:52 - training - INFO - Epoch [2/5][551/631] lr: 2.8e-05, eta: 0:36:12.324504, loss: 1.7297
2023-04-09 19:26:56 - training - INFO - Epoch [2/5][561/631] lr: 2.8e-05, eta: 0:35:42.991596, loss: 2.0865
2023-04-09 19:26:59 - training - INFO - Epoch [2/5][571/631] lr: 2.8e-05, eta: 0:35:14.549216, loss: 1.3608
2023-04-09 19:27:03 - training - INFO - Epoch [2/5][581/631] lr: 2.8e-05, eta: 0:34:46.968312, loss: 1.1752
2023-04-09 19:27:07 - training - INFO - Epoch [2/5][591/631] lr: 2.8e-05, eta: 0:34:20.209896, loss: 1.6751
2023-04-09 19:27:11 - training - INFO - Epoch [2/5][601/631] lr: 2.8e-05, eta: 0:33:54.202258, loss: 1.7632
2023-04-09 19:27:15 - training - INFO - Epoch [2/5][611/631] lr: 2.8e-05, eta: 0:33:28.897584, loss: 2.1790
2023-04-09 19:27:18 - training - INFO - Epoch [2/5][621/631] lr: 2.7e-05, eta: 0:33:04.299380, loss: 1.6837
2023-04-09 19:27:22 - training - INFO - Epoch [2/5][631/631] lr: 2.7e-05, eta: 0:32:39.888524, loss: 1.9350
2023-04-09 19:27:35 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.5827, Validation Metrics: {'exact_match': 38.41676367869616, 'f1': 44.820995199342136}
2023-04-09 19:27:36 - training - INFO - Epoch [3/5][1/631] lr: 2.7e-05, eta: 18 days, 9:05:05.898046, loss: 1.3589
2023-04-09 19:27:39 - training - INFO - Epoch [3/5][11/631] lr: 2.7e-05, eta: 1 day, 16:16:21.318744, loss: 0.9407
2023-04-09 19:27:43 - training - INFO - Epoch [3/5][21/631] lr: 2.7e-05, eta: 21:11:08.234232, loss: 0.8410
2023-04-09 19:27:47 - training - INFO - Epoch [3/5][31/631] lr: 2.7e-05, eta: 14:24:43.985560, loss: 1.3032
2023-04-09 19:27:51 - training - INFO - Epoch [3/5][41/631] lr: 2.7e-05, eta: 10:56:32.402058, loss: 0.6618
2023-04-09 19:27:55 - training - INFO - Epoch [3/5][51/631] lr: 2.7e-05, eta: 8:49:57.934720, loss: 1.5461
2023-04-09 19:27:58 - training - INFO - Epoch [3/5][61/631] lr: 2.6e-05, eta: 7:24:51.873026, loss: 1.1614
2023-04-09 19:28:02 - training - INFO - Epoch [3/5][71/631] lr: 2.6e-05, eta: 6:23:42.963612, loss: 1.6416
2023-04-09 19:28:06 - training - INFO - Epoch [3/5][81/631] lr: 2.6e-05, eta: 5:37:38.766640, loss: 1.1761
2023-04-09 19:28:10 - training - INFO - Epoch [3/5][91/631] lr: 2.6e-05, eta: 5:01:41.477752, loss: 1.1243
2023-04-09 19:28:14 - training - INFO - Epoch [3/5][101/631] lr: 2.6e-05, eta: 4:32:50.542494, loss: 1.3773
2023-04-09 19:28:17 - training - INFO - Epoch [3/5][111/631] lr: 2.6e-05, eta: 4:09:10.895180, loss: 0.8553
2023-04-09 19:28:21 - training - INFO - Epoch [3/5][121/631] lr: 2.5e-05, eta: 3:49:25.151810, loss: 1.3438
2023-04-09 19:28:25 - training - INFO - Epoch [3/5][131/631] lr: 2.5e-05, eta: 3:32:39.861744, loss: 1.3340
2023-04-09 19:28:29 - training - INFO - Epoch [3/5][141/631] lr: 2.5e-05, eta: 3:18:16.664890, loss: 1.3360
2023-04-09 19:28:32 - training - INFO - Epoch [3/5][151/631] lr: 2.5e-05, eta: 3:05:47.297272, loss: 1.3371
2023-04-09 19:28:36 - training - INFO - Epoch [3/5][161/631] lr: 2.5e-05, eta: 2:54:50.589774, loss: 1.1348
2023-04-09 19:28:40 - training - INFO - Epoch [3/5][171/631] lr: 2.5e-05, eta: 2:45:10.269824, loss: 1.3417
2023-04-09 19:28:44 - training - INFO - Epoch [3/5][181/631] lr: 2.5e-05, eta: 2:36:33.587180, loss: 0.8668
2023-04-09 19:28:48 - training - INFO - Epoch [3/5][191/631] lr: 2.4e-05, eta: 2:28:50.573496, loss: 0.8749
2023-04-09 19:28:51 - training - INFO - Epoch [3/5][201/631] lr: 2.4e-05, eta: 2:21:53.339380, loss: 1.5969
2023-04-09 19:28:55 - training - INFO - Epoch [3/5][211/631] lr: 2.4e-05, eta: 2:15:35.381888, loss: 1.2837
2023-04-09 19:28:59 - training - INFO - Epoch [3/5][221/631] lr: 2.4e-05, eta: 2:09:51.289812, loss: 0.9226
2023-04-09 19:29:03 - training - INFO - Epoch [3/5][231/631] lr: 2.4e-05, eta: 2:04:36.714784, loss: 1.6604
2023-04-09 19:29:07 - training - INFO - Epoch [3/5][241/631] lr: 2.4e-05, eta: 1:59:47.774390, loss: 1.5636
2023-04-09 19:29:10 - training - INFO - Epoch [3/5][251/631] lr: 2.4e-05, eta: 1:55:21.553320, loss: 1.5188
2023-04-09 19:29:14 - training - INFO - Epoch [3/5][261/631] lr: 2.3e-05, eta: 1:51:15.442206, loss: 1.2295
2023-04-09 19:29:18 - training - INFO - Epoch [3/5][271/631] lr: 2.3e-05, eta: 1:47:27.216608, loss: 1.2169
2023-04-09 19:29:22 - training - INFO - Epoch [3/5][281/631] lr: 2.3e-05, eta: 1:43:55.007922, loss: 1.1536
2023-04-09 19:29:26 - training - INFO - Epoch [3/5][291/631] lr: 2.3e-05, eta: 1:40:37.180256, loss: 1.5898
2023-04-09 19:29:29 - training - INFO - Epoch [3/5][301/631] lr: 2.3e-05, eta: 1:37:32.195496, loss: 1.7873
2023-04-09 19:29:33 - training - INFO - Epoch [3/5][311/631] lr: 2.3e-05, eta: 1:34:38.873604, loss: 0.8123
2023-04-09 19:29:37 - training - INFO - Epoch [3/5][321/631] lr: 2.3e-05, eta: 1:31:56.128774, loss: 0.9858
2023-04-09 19:29:41 - training - INFO - Epoch [3/5][331/631] lr: 2.2e-05, eta: 1:29:23.021688, loss: 1.6369
2023-04-09 19:29:45 - training - INFO - Epoch [3/5][341/631] lr: 2.2e-05, eta: 1:26:58.686816, loss: 1.5559
2023-04-09 19:29:48 - training - INFO - Epoch [3/5][351/631] lr: 2.2e-05, eta: 1:24:42.342532, loss: 1.2074
2023-04-09 19:29:52 - training - INFO - Epoch [3/5][361/631] lr: 2.2e-05, eta: 1:22:33.331724, loss: 1.3015
2023-04-09 19:29:56 - training - INFO - Epoch [3/5][371/631] lr: 2.2e-05, eta: 1:20:31.066848, loss: 1.1740
2023-04-09 19:30:00 - training - INFO - Epoch [3/5][381/631] lr: 2.2e-05, eta: 1:18:35.064890, loss: 1.1314
2023-04-09 19:30:04 - training - INFO - Epoch [3/5][391/631] lr: 2.2e-05, eta: 1:16:44.796360, loss: 1.2805
2023-04-09 19:30:07 - training - INFO - Epoch [3/5][401/631] lr: 2.1e-05, eta: 1:14:59.840466, loss: 0.9371
2023-04-09 19:30:11 - training - INFO - Epoch [3/5][411/631] lr: 2.1e-05, eta: 1:13:19.869544, loss: 1.8215
2023-04-09 19:30:15 - training - INFO - Epoch [3/5][421/631] lr: 2.1e-05, eta: 1:11:44.384994, loss: 1.7518
2023-04-09 19:30:19 - training - INFO - Epoch [3/5][431/631] lr: 2.1e-05, eta: 1:10:13.148148, loss: 1.0997
2023-04-09 19:30:23 - training - INFO - Epoch [3/5][441/631] lr: 2.1e-05, eta: 1:08:45.860796, loss: 1.4389
2023-04-09 19:30:26 - training - INFO - Epoch [3/5][451/631] lr: 2.1e-05, eta: 1:07:22.271792, loss: 1.4515
2023-04-09 19:30:30 - training - INFO - Epoch [3/5][461/631] lr: 2.1e-05, eta: 1:06:02.176254, loss: 1.4470
2023-04-09 19:30:34 - training - INFO - Epoch [3/5][471/631] lr: 2.0e-05, eta: 1:04:45.280564, loss: 2.2127
2023-04-09 19:30:38 - training - INFO - Epoch [3/5][481/631] lr: 2.0e-05, eta: 1:03:31.471468, loss: 1.0977
2023-04-09 19:30:42 - training - INFO - Epoch [3/5][491/631] lr: 2.0e-05, eta: 1:02:20.503752, loss: 1.3522
2023-04-09 19:30:45 - training - INFO - Epoch [3/5][501/631] lr: 2.0e-05, eta: 1:01:12.233640, loss: 1.2787
2023-04-09 19:30:49 - training - INFO - Epoch [3/5][511/631] lr: 2.0e-05, eta: 1:00:06.479456, loss: 1.2274
2023-04-09 19:30:53 - training - INFO - Epoch [3/5][521/631] lr: 2.0e-05, eta: 0:59:03.096126, loss: 1.2779
2023-04-09 19:30:57 - training - INFO - Epoch [3/5][531/631] lr: 2.0e-05, eta: 0:58:01.971904, loss: 1.0635
2023-04-09 19:31:01 - training - INFO - Epoch [3/5][541/631] lr: 1.9e-05, eta: 0:57:03.027772, loss: 1.3599
2023-04-09 19:31:04 - training - INFO - Epoch [3/5][551/631] lr: 1.9e-05, eta: 0:56:06.008520, loss: 1.7098
2023-04-09 19:31:08 - training - INFO - Epoch [3/5][561/631] lr: 1.9e-05, eta: 0:55:10.934908, loss: 1.0290
2023-04-09 19:31:12 - training - INFO - Epoch [3/5][571/631] lr: 1.9e-05, eta: 0:54:17.656552, loss: 1.1603
2023-04-09 19:31:16 - training - INFO - Epoch [3/5][581/631] lr: 1.9e-05, eta: 0:53:26.086884, loss: 1.1960
2023-04-09 19:31:20 - training - INFO - Epoch [3/5][591/631] lr: 1.9e-05, eta: 0:52:36.119904, loss: 1.7612
2023-04-09 19:31:23 - training - INFO - Epoch [3/5][601/631] lr: 1.9e-05, eta: 0:51:47.676552, loss: 1.5121
2023-04-09 19:31:27 - training - INFO - Epoch [3/5][611/631] lr: 1.8e-05, eta: 0:51:00.686400, loss: 1.1420
2023-04-09 19:31:31 - training - INFO - Epoch [3/5][621/631] lr: 1.8e-05, eta: 0:50:15.074832, loss: 1.4892
2023-04-09 19:31:35 - training - INFO - Epoch [3/5][631/631] lr: 1.8e-05, eta: 0:49:30.321444, loss: 0.9638
2023-04-09 19:31:48 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 1.3272, Validation Metrics: {'exact_match': 35.0407450523865, 'f1': 42.1772978670534}
2023-04-09 19:31:48 - training - INFO - Epoch [4/5][1/631] lr: 1.8e-05, eta: 27 days, 14:20:14.421302, loss: 1.1256
2023-04-09 19:31:52 - training - INFO - Epoch [4/5][11/631] lr: 1.8e-05, eta: 2 days, 12:19:22.301976, loss: 0.8371
2023-04-09 19:31:56 - training - INFO - Epoch [4/5][21/631] lr: 1.8e-05, eta: 1 day, 7:39:16.793702, loss: 1.0558
2023-04-09 19:32:00 - training - INFO - Epoch [4/5][31/631] lr: 1.8e-05, eta: 21:28:53.029884, loss: 1.3376
2023-04-09 19:32:03 - training - INFO - Epoch [4/5][41/631] lr: 1.8e-05, eta: 16:16:12.835938, loss: 1.0334
2023-04-09 19:32:07 - training - INFO - Epoch [4/5][51/631] lr: 1.7e-05, eta: 13:06:08.287776, loss: 0.9364
2023-04-09 19:32:11 - training - INFO - Epoch [4/5][61/631] lr: 1.7e-05, eta: 10:58:21.308392, loss: 1.4908
2023-04-09 19:32:15 - training - INFO - Epoch [4/5][71/631] lr: 1.7e-05, eta: 9:26:33.272808, loss: 0.9727
2023-04-09 19:32:19 - training - INFO - Epoch [4/5][81/631] lr: 1.7e-05, eta: 8:17:24.448506, loss: 1.0940
2023-04-09 19:32:22 - training - INFO - Epoch [4/5][91/631] lr: 1.7e-05, eta: 7:23:26.568784, loss: 1.2897
2023-04-09 19:32:26 - training - INFO - Epoch [4/5][101/631] lr: 1.7e-05, eta: 6:40:09.039324, loss: 0.9467
2023-04-09 19:32:30 - training - INFO - Epoch [4/5][111/631] lr: 1.7e-05, eta: 6:04:38.664768, loss: 1.0005
2023-04-09 19:32:34 - training - INFO - Epoch [4/5][121/631] lr: 1.6e-05, eta: 5:34:59.943566, loss: 0.8236
2023-04-09 19:32:38 - training - INFO - Epoch [4/5][131/631] lr: 1.6e-05, eta: 5:09:52.289856, loss: 0.9810
2023-04-09 19:32:41 - training - INFO - Epoch [4/5][141/631] lr: 1.6e-05, eta: 4:48:17.903590, loss: 1.2180
2023-04-09 19:32:45 - training - INFO - Epoch [4/5][151/631] lr: 1.6e-05, eta: 4:29:34.428188, loss: 1.0078
2023-04-09 19:32:49 - training - INFO - Epoch [4/5][161/631] lr: 1.6e-05, eta: 4:13:09.969180, loss: 1.5075
2023-04-09 19:32:53 - training - INFO - Epoch [4/5][171/631] lr: 1.6e-05, eta: 3:58:40.326408, loss: 1.1501
2023-04-09 19:32:57 - training - INFO - Epoch [4/5][181/631] lr: 1.6e-05, eta: 3:45:46.370742, loss: 1.1047
2023-04-09 19:33:00 - training - INFO - Epoch [4/5][191/631] lr: 1.5e-05, eta: 3:34:12.971040, loss: 1.1645
2023-04-09 19:33:04 - training - INFO - Epoch [4/5][201/631] lr: 1.5e-05, eta: 3:23:48.207068, loss: 0.8392
2023-04-09 19:33:08 - training - INFO - Epoch [4/5][211/631] lr: 1.5e-05, eta: 3:14:22.417536, loss: 1.3421
2023-04-09 19:33:12 - training - INFO - Epoch [4/5][221/631] lr: 1.5e-05, eta: 3:05:47.416128, loss: 0.9613
2023-04-09 19:33:16 - training - INFO - Epoch [4/5][231/631] lr: 1.5e-05, eta: 2:57:56.670208, loss: 1.4952
2023-04-09 19:33:19 - training - INFO - Epoch [4/5][241/631] lr: 1.5e-05, eta: 2:50:44.714832, loss: 1.2320
2023-04-09 19:33:23 - training - INFO - Epoch [4/5][251/631] lr: 1.5e-05, eta: 2:44:06.886104, loss: 1.2436
2023-04-09 19:33:27 - training - INFO - Epoch [4/5][261/631] lr: 1.4e-05, eta: 2:37:59.247802, loss: 1.6389
2023-04-09 19:33:31 - training - INFO - Epoch [4/5][271/631] lr: 1.4e-05, eta: 2:32:18.487540, loss: 1.7662
2023-04-09 19:33:35 - training - INFO - Epoch [4/5][281/631] lr: 1.4e-05, eta: 2:27:01.679772, loss: 1.0304
2023-04-09 19:33:38 - training - INFO - Epoch [4/5][291/631] lr: 1.4e-05, eta: 2:22:06.288384, loss: 1.0407
2023-04-09 19:33:42 - training - INFO - Epoch [4/5][301/631] lr: 1.4e-05, eta: 2:17:30.280412, loss: 1.3960
2023-04-09 19:33:46 - training - INFO - Epoch [4/5][311/631] lr: 1.4e-05, eta: 2:13:11.802108, loss: 1.4022
2023-04-09 19:33:50 - training - INFO - Epoch [4/5][321/631] lr: 1.4e-05, eta: 2:09:09.207414, loss: 1.6051
2023-04-09 19:33:54 - training - INFO - Epoch [4/5][331/631] lr: 1.3e-05, eta: 2:05:21.091424, loss: 0.8172
2023-04-09 19:33:57 - training - INFO - Epoch [4/5][341/631] lr: 1.3e-05, eta: 2:01:46.058550, loss: 1.3349
2023-04-09 19:34:01 - training - INFO - Epoch [4/5][351/631] lr: 1.3e-05, eta: 1:58:23.075976, loss: 0.8351
2023-04-09 19:34:05 - training - INFO - Epoch [4/5][361/631] lr: 1.3e-05, eta: 1:55:11.112670, loss: 1.3353
2023-04-09 19:34:09 - training - INFO - Epoch [4/5][371/631] lr: 1.3e-05, eta: 1:52:09.342816, loss: 1.3407
2023-04-09 19:34:13 - training - INFO - Epoch [4/5][381/631] lr: 1.3e-05, eta: 1:49:16.953732, loss: 1.2169
2023-04-09 19:34:17 - training - INFO - Epoch [4/5][391/631] lr: 1.3e-05, eta: 1:46:33.137528, loss: 1.2060
2023-04-09 19:34:20 - training - INFO - Epoch [4/5][401/631] lr: 1.2e-05, eta: 1:43:57.303264, loss: 1.2835
2023-04-09 19:34:24 - training - INFO - Epoch [4/5][411/631] lr: 1.2e-05, eta: 1:41:28.856424, loss: 0.7859
2023-04-09 19:34:28 - training - INFO - Epoch [4/5][421/631] lr: 1.2e-05, eta: 1:39:07.292072, loss: 1.2809
2023-04-09 19:34:32 - training - INFO - Epoch [4/5][431/631] lr: 1.2e-05, eta: 1:36:52.127976, loss: 1.0260
2023-04-09 19:34:36 - training - INFO - Epoch [4/5][441/631] lr: 1.2e-05, eta: 1:34:42.993870, loss: 1.1609
2023-04-09 19:34:39 - training - INFO - Epoch [4/5][451/631] lr: 1.2e-05, eta: 1:32:39.340176, loss: 0.8935
2023-04-09 19:34:43 - training - INFO - Epoch [4/5][461/631] lr: 1.2e-05, eta: 1:30:40.899384, loss: 1.1538
2023-04-09 19:34:47 - training - INFO - Epoch [4/5][471/631] lr: 1.1e-05, eta: 1:28:47.310560, loss: 0.8997
2023-04-09 19:34:51 - training - INFO - Epoch [4/5][481/631] lr: 1.1e-05, eta: 1:26:58.278912, loss: 1.3861
2023-04-09 19:34:55 - training - INFO - Epoch [4/5][491/631] lr: 1.1e-05, eta: 1:25:13.537344, loss: 1.1681
2023-04-09 19:34:58 - training - INFO - Epoch [4/5][501/631] lr: 1.1e-05, eta: 1:23:32.806196, loss: 1.0111
2023-04-09 19:35:02 - training - INFO - Epoch [4/5][511/631] lr: 1.1e-05, eta: 1:21:55.899304, loss: 1.2472
2023-04-09 19:35:06 - training - INFO - Epoch [4/5][521/631] lr: 1.1e-05, eta: 1:20:22.532652, loss: 1.2931
2023-04-09 19:35:10 - training - INFO - Epoch [4/5][531/631] lr: 1.1e-05, eta: 1:18:52.538816, loss: 1.6082
2023-04-09 19:35:14 - training - INFO - Epoch [4/5][541/631] lr: 1.0e-05, eta: 1:17:25.755026, loss: 0.9376
2023-04-09 19:35:17 - training - INFO - Epoch [4/5][551/631] lr: 1.0e-05, eta: 1:16:01.991868, loss: 0.8859
2023-04-09 19:35:21 - training - INFO - Epoch [4/5][561/631] lr: 1.0e-05, eta: 1:14:41.057180, loss: 0.7381
2023-04-09 19:35:25 - training - INFO - Epoch [4/5][571/631] lr: 9.9e-06, eta: 1:13:22.869848, loss: 0.9918
2023-04-09 19:35:29 - training - INFO - Epoch [4/5][581/631] lr: 9.8e-06, eta: 1:12:07.187436, loss: 0.8280
2023-04-09 19:35:33 - training - INFO - Epoch [4/5][591/631] lr: 9.7e-06, eta: 1:10:53.986244, loss: 0.8215
2023-04-09 19:35:36 - training - INFO - Epoch [4/5][601/631] lr: 9.5e-06, eta: 1:09:43.109764, loss: 1.2028
2023-04-09 19:35:40 - training - INFO - Epoch [4/5][611/631] lr: 9.4e-06, eta: 1:08:34.421376, loss: 1.0019
2023-04-09 19:35:44 - training - INFO - Epoch [4/5][621/631] lr: 9.2e-06, eta: 1:07:27.803998, loss: 0.9742
2023-04-09 19:35:48 - training - INFO - Epoch [4/5][631/631] lr: 9.1e-06, eta: 1:06:22.680176, loss: 0.7777
2023-04-09 19:36:01 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 1.1650, Validation Metrics: {'exact_match': 37.60186263096624, 'f1': 43.76175499574801}
2023-04-09 19:36:01 - training - INFO - Epoch [5/5][1/631] lr: 9.1e-06, eta: 36 days, 20:06:36.505716, loss: 1.5092
2023-04-09 19:36:05 - training - INFO - Epoch [5/5][11/631] lr: 8.9e-06, eta: 3 days, 8:25:12.294072, loss: 1.0088
2023-04-09 19:36:09 - training - INFO - Epoch [5/5][21/631] lr: 8.8e-06, eta: 1 day, 18:08:53.932548, loss: 1.1168
2023-04-09 19:36:13 - training - INFO - Epoch [5/5][31/631] lr: 8.6e-06, eta: 1 day, 4:34:02.842256, loss: 1.3124
2023-04-09 19:36:16 - training - INFO - Epoch [5/5][41/631] lr: 8.5e-06, eta: 21:36:38.980224, loss: 1.0748
2023-04-09 19:36:20 - training - INFO - Epoch [5/5][51/631] lr: 8.3e-06, eta: 17:22:54.780704, loss: 1.1673
2023-04-09 19:36:24 - training - INFO - Epoch [5/5][61/631] lr: 8.2e-06, eta: 14:32:20.628704, loss: 1.0689
2023-04-09 19:36:28 - training - INFO - Epoch [5/5][71/631] lr: 8.1e-06, eta: 12:29:48.075132, loss: 0.9398
2023-04-09 19:36:32 - training - INFO - Epoch [5/5][81/631] lr: 7.9e-06, eta: 10:57:30.172852, loss: 0.8651
2023-04-09 19:36:35 - training - INFO - Epoch [5/5][91/631] lr: 7.8e-06, eta: 9:45:28.465856, loss: 0.4060
2023-04-09 19:36:39 - training - INFO - Epoch [5/5][101/631] lr: 7.6e-06, eta: 8:47:41.734200, loss: 0.8607
2023-04-09 19:36:43 - training - INFO - Epoch [5/5][111/631] lr: 7.5e-06, eta: 8:00:19.167408, loss: 0.8340
2023-04-09 19:36:47 - training - INFO - Epoch [5/5][121/631] lr: 7.3e-06, eta: 7:20:45.782116, loss: 1.1382
2023-04-09 19:36:51 - training - INFO - Epoch [5/5][131/631] lr: 7.2e-06, eta: 6:47:14.149824, loss: 0.9638
2023-04-09 19:36:54 - training - INFO - Epoch [5/5][141/631] lr: 7.1e-06, eta: 6:18:27.376538, loss: 0.9242
2023-04-09 19:36:58 - training - INFO - Epoch [5/5][151/631] lr: 6.9e-06, eta: 5:53:28.621508, loss: 0.7572
2023-04-09 19:37:02 - training - INFO - Epoch [5/5][161/631] lr: 6.8e-06, eta: 5:31:35.567124, loss: 1.3998
2023-04-09 19:37:06 - training - INFO - Epoch [5/5][171/631] lr: 6.6e-06, eta: 5:12:15.625880, loss: 1.4902
2023-04-09 19:37:10 - training - INFO - Epoch [5/5][181/631] lr: 6.5e-06, eta: 4:55:03.451734, loss: 1.0705
2023-04-09 19:37:13 - training - INFO - Epoch [5/5][191/631] lr: 6.3e-06, eta: 4:39:38.975772, loss: 0.8143
2023-04-09 19:37:17 - training - INFO - Epoch [5/5][201/631] lr: 6.2e-06, eta: 4:25:46.200088, loss: 1.0211
2023-04-09 19:37:21 - training - INFO - Epoch [5/5][211/631] lr: 6.0e-06, eta: 4:13:11.976192, loss: 0.7371
2023-04-09 19:37:25 - training - INFO - Epoch [5/5][221/631] lr: 5.9e-06, eta: 4:01:45.657858, loss: 1.0788
2023-04-09 19:37:29 - training - INFO - Epoch [5/5][231/631] lr: 5.8e-06, eta: 3:51:18.441436, loss: 0.8299
2023-04-09 19:37:32 - training - INFO - Epoch [5/5][241/631] lr: 5.6e-06, eta: 3:41:42.949090, loss: 0.9628
2023-04-09 19:37:36 - training - INFO - Epoch [5/5][251/631] lr: 5.5e-06, eta: 3:32:53.002968, loss: 0.9041
2023-04-09 19:37:40 - training - INFO - Epoch [5/5][261/631] lr: 5.3e-06, eta: 3:24:43.423830, loss: 1.2669
2023-04-09 19:37:44 - training - INFO - Epoch [5/5][271/631] lr: 5.2e-06, eta: 3:17:09.697908, loss: 1.5650
2023-04-09 19:37:48 - training - INFO - Epoch [5/5][281/631] lr: 5.0e-06, eta: 3:10:08.029734, loss: 0.7936
2023-04-09 19:37:51 - training - INFO - Epoch [5/5][291/631] lr: 4.9e-06, eta: 3:03:35.124432, loss: 0.9950
2023-04-09 19:37:55 - training - INFO - Epoch [5/5][301/631] lr: 4.7e-06, eta: 2:57:28.048534, loss: 0.7871
2023-04-09 19:37:59 - training - INFO - Epoch [5/5][311/631] lr: 4.6e-06, eta: 2:51:44.326764, loss: 0.7758
2023-04-09 19:38:03 - training - INFO - Epoch [5/5][321/631] lr: 4.5e-06, eta: 2:46:21.741926, loss: 0.7302
2023-04-09 19:38:07 - training - INFO - Epoch [5/5][331/631] lr: 4.3e-06, eta: 2:41:18.407152, loss: 0.9440
2023-04-09 19:38:10 - training - INFO - Epoch [5/5][341/631] lr: 4.2e-06, eta: 2:36:32.659248, loss: 1.2376
2023-04-09 19:38:14 - training - INFO - Epoch [5/5][351/631] lr: 4.0e-06, eta: 2:32:02.982240, loss: 0.9898
2023-04-09 19:38:18 - training - INFO - Epoch [5/5][361/631] lr: 3.9e-06, eta: 2:27:48.007918, loss: 0.6375
2023-04-09 19:38:22 - training - INFO - Epoch [5/5][371/631] lr: 3.7e-06, eta: 2:23:46.563648, loss: 0.7274
2023-04-09 19:38:26 - training - INFO - Epoch [5/5][381/631] lr: 3.6e-06, eta: 2:19:57.619240, loss: 1.0322
2023-04-09 19:38:29 - training - INFO - Epoch [5/5][391/631] lr: 3.5e-06, eta: 2:16:20.185144, loss: 1.1057
2023-04-09 19:38:33 - training - INFO - Epoch [5/5][401/631] lr: 3.3e-06, eta: 2:12:53.389062, loss: 0.9391
2023-04-09 19:38:37 - training - INFO - Epoch [5/5][411/631] lr: 3.2e-06, eta: 2:09:36.526184, loss: 1.0324
2023-04-09 19:38:41 - training - INFO - Epoch [5/5][421/631] lr: 3.0e-06, eta: 2:06:28.845820, loss: 1.0441
2023-04-09 19:38:45 - training - INFO - Epoch [5/5][431/631] lr: 2.9e-06, eta: 2:03:29.680428, loss: 0.9345
2023-04-09 19:38:48 - training - INFO - Epoch [5/5][441/631] lr: 2.7e-06, eta: 2:00:38.468690, loss: 1.1878
2023-04-09 19:38:52 - training - INFO - Epoch [5/5][451/631] lr: 2.6e-06, eta: 1:57:54.678000, loss: 1.0229
2023-04-09 19:38:56 - training - INFO - Epoch [5/5][461/631] lr: 2.4e-06, eta: 1:55:17.836392, loss: 1.1225
2023-04-09 19:39:00 - training - INFO - Epoch [5/5][471/631] lr: 2.3e-06, eta: 1:52:47.526172, loss: 1.0971
2023-04-09 19:39:04 - training - INFO - Epoch [5/5][481/631] lr: 2.2e-06, eta: 1:50:23.305472, loss: 1.1810
2023-04-09 19:39:07 - training - INFO - Epoch [5/5][491/631] lr: 2.0e-06, eta: 1:48:04.815360, loss: 1.0397
2023-04-09 19:39:11 - training - INFO - Epoch [5/5][501/631] lr: 1.9e-06, eta: 1:45:51.682846, loss: 1.2333
2023-04-09 19:39:15 - training - INFO - Epoch [5/5][511/631] lr: 1.7e-06, eta: 1:43:43.587332, loss: 0.9892
2023-04-09 19:39:19 - training - INFO - Epoch [5/5][521/631] lr: 1.6e-06, eta: 1:41:40.264980, loss: 1.0680
2023-04-09 19:39:23 - training - INFO - Epoch [5/5][531/631] lr: 1.4e-06, eta: 1:39:41.423744, loss: 1.1622
2023-04-09 19:39:26 - training - INFO - Epoch [5/5][541/631] lr: 1.3e-06, eta: 1:37:46.853758, loss: 0.6797
2023-04-09 19:39:30 - training - INFO - Epoch [5/5][551/631] lr: 1.2e-06, eta: 1:35:56.306052, loss: 0.8623
2023-04-09 19:39:34 - training - INFO - Epoch [5/5][561/631] lr: 1.0e-06, eta: 1:34:09.563390, loss: 0.8150
2023-04-09 19:39:38 - training - INFO - Epoch [5/5][571/631] lr: 8.6e-07, eta: 1:32:26.429384, loss: 1.1095
2023-04-09 19:39:42 - training - INFO - Epoch [5/5][581/631] lr: 7.2e-07, eta: 1:30:46.728144, loss: 1.1588
2023-04-09 19:39:45 - training - INFO - Epoch [5/5][591/631] lr: 5.8e-07, eta: 1:29:10.250084, loss: 0.9581
2023-04-09 19:39:49 - training - INFO - Epoch [5/5][601/631] lr: 4.3e-07, eta: 1:27:36.867552, loss: 1.0713
2023-04-09 19:39:53 - training - INFO - Epoch [5/5][611/631] lr: 2.9e-07, eta: 1:26:06.423888, loss: 1.3993
2023-04-09 19:39:57 - training - INFO - Epoch [5/5][621/631] lr: 1.4e-07, eta: 1:24:38.764432, loss: 0.8497
2023-04-09 19:40:00 - training - INFO - Epoch [5/5][631/631] lr: 0.0e+00, eta: 1:23:13.269584, loss: 0.9731
2023-04-09 19:40:14 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 1.0458, Validation Metrics: {'exact_match': 38.0675203725262, 'f1': 44.33461046930881}
2023-04-09 19:40:27 - training - INFO - Final Test - Train Loss: 1.0458, Test Metrics: {'exact_match': 36.70150987224158, 'f1': 42.92115828151037}
