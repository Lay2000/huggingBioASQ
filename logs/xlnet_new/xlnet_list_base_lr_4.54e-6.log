2023-04-14 03:09:58 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-44a167365c0b341b/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'xlnet-base-cased'}, 'data': {'task_type': 'list', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 4.54e-06, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/xlnet_list_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 544.38it/s]
Map:   0%|          | 0/6878 [00:00<?, ? examples/s]Map:  15%|█▍        | 1000/6878 [00:00<00:03, 1476.69 examples/s]Map:  29%|██▉       | 2000/6878 [00:01<00:03, 1528.10 examples/s]Map:  44%|████▎     | 3000/6878 [00:01<00:02, 1526.22 examples/s]Map:  58%|█████▊    | 4000/6878 [00:02<00:01, 1520.17 examples/s]Map:  73%|███████▎  | 5000/6878 [00:03<00:01, 1520.47 examples/s]Map:  87%|████████▋ | 6000/6878 [00:03<00:00, 1511.37 examples/s]Map: 100%|██████████| 6878/6878 [00:04<00:00, 1400.98 examples/s]                                                                 Map:   0%|          | 0/859 [00:00<?, ? examples/s]Map: 100%|██████████| 859/859 [00:00<00:00, 1176.81 examples/s]                                                               Map:   0%|          | 0/861 [00:00<?, ? examples/s]Map: 100%|██████████| 861/861 [00:00<00:00, 1214.62 examples/s]                                                               Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForQuestionAnsweringSimple: ['lm_loss.weight', 'lm_loss.bias']
- This IS expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForQuestionAnsweringSimple were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-14 03:11:34 - training - INFO - First Test - Val Metrics:{'exact_match': 0.11641443538998836, 'f1': 3.5108483369545644} Test Metrics: {'exact_match': 0.8130081300813008, 'f1': 4.255093738648391}
2023-04-14 03:11:35 - training - INFO - Epoch [1/5][1/690] lr: 4.5e-06, eta: 3 days, 6:45:23.982708, loss: 7.2861
2023-04-14 03:11:43 - training - INFO - Epoch [1/5][11/690] lr: 4.5e-06, eta: 7:51:46.175148, loss: 6.4534
2023-04-14 03:11:51 - training - INFO - Epoch [1/5][21/690] lr: 4.5e-06, eta: 4:28:08.027895, loss: 5.6729
2023-04-14 03:11:59 - training - INFO - Epoch [1/5][31/690] lr: 4.5e-06, eta: 3:15:24.106576, loss: 5.6087
2023-04-14 03:12:07 - training - INFO - Epoch [1/5][41/690] lr: 4.5e-06, eta: 2:38:48.567489, loss: 5.4931
2023-04-14 03:12:16 - training - INFO - Epoch [1/5][51/690] lr: 4.5e-06, eta: 2:16:17.287008, loss: 5.2176
2023-04-14 03:12:24 - training - INFO - Epoch [1/5][61/690] lr: 4.5e-06, eta: 2:01:06.897140, loss: 4.9007
2023-04-14 03:12:32 - training - INFO - Epoch [1/5][71/690] lr: 4.4e-06, eta: 1:50:05.171209, loss: 4.9942
2023-04-14 03:12:40 - training - INFO - Epoch [1/5][81/690] lr: 4.4e-06, eta: 1:41:59.876094, loss: 4.6080
2023-04-14 03:12:48 - training - INFO - Epoch [1/5][91/690] lr: 4.4e-06, eta: 1:35:25.522988, loss: 5.2414
2023-04-14 03:12:56 - training - INFO - Epoch [1/5][101/690] lr: 4.4e-06, eta: 1:30:26.689459, loss: 4.7926
2023-04-14 03:13:05 - training - INFO - Epoch [1/5][111/690] lr: 4.4e-06, eta: 1:26:10.728654, loss: 4.4456
2023-04-14 03:13:12 - training - INFO - Epoch [1/5][121/690] lr: 4.4e-06, eta: 1:22:19.357144, loss: 4.5424
2023-04-14 03:13:20 - training - INFO - Epoch [1/5][131/690] lr: 4.4e-06, eta: 1:19:07.919113, loss: 3.8462
2023-04-14 03:13:28 - training - INFO - Epoch [1/5][141/690] lr: 4.4e-06, eta: 1:16:26.588355, loss: 4.4530
2023-04-14 03:13:37 - training - INFO - Epoch [1/5][151/690] lr: 4.3e-06, eta: 1:14:14.606710, loss: 3.8263
2023-04-14 03:13:45 - training - INFO - Epoch [1/5][161/690] lr: 4.3e-06, eta: 1:12:16.885267, loss: 4.5894
2023-04-14 03:13:53 - training - INFO - Epoch [1/5][171/690] lr: 4.3e-06, eta: 1:10:30.228063, loss: 4.3513
2023-04-14 03:14:02 - training - INFO - Epoch [1/5][181/690] lr: 4.3e-06, eta: 1:08:56.040139, loss: 4.5888
2023-04-14 03:14:10 - training - INFO - Epoch [1/5][191/690] lr: 4.3e-06, eta: 1:07:20.469092, loss: 4.4910
2023-04-14 03:14:18 - training - INFO - Epoch [1/5][201/690] lr: 4.3e-06, eta: 1:05:58.542612, loss: 3.8401
2023-04-14 03:14:26 - training - INFO - Epoch [1/5][211/690] lr: 4.3e-06, eta: 1:04:44.260624, loss: 3.9711
2023-04-14 03:14:34 - training - INFO - Epoch [1/5][221/690] lr: 4.2e-06, eta: 1:03:29.693673, loss: 3.5771
2023-04-14 03:14:41 - training - INFO - Epoch [1/5][231/690] lr: 4.2e-06, eta: 1:02:20.033778, loss: 4.0787
2023-04-14 03:14:49 - training - INFO - Epoch [1/5][241/690] lr: 4.2e-06, eta: 1:01:16.929962, loss: 3.9989
2023-04-14 03:14:57 - training - INFO - Epoch [1/5][251/690] lr: 4.2e-06, eta: 1:00:25.960933, loss: 3.4874
2023-04-14 03:15:05 - training - INFO - Epoch [1/5][261/690] lr: 4.2e-06, eta: 0:59:33.861276, loss: 3.0675
2023-04-14 03:15:13 - training - INFO - Epoch [1/5][271/690] lr: 4.2e-06, eta: 0:58:43.215762, loss: 3.5836
2023-04-14 03:15:21 - training - INFO - Epoch [1/5][281/690] lr: 4.2e-06, eta: 0:57:56.941237, loss: 3.6033
2023-04-14 03:15:29 - training - INFO - Epoch [1/5][291/690] lr: 4.2e-06, eta: 0:57:14.802813, loss: 3.5681
2023-04-14 03:15:38 - training - INFO - Epoch [1/5][301/690] lr: 4.1e-06, eta: 0:56:39.062090, loss: 3.9536
2023-04-14 03:15:46 - training - INFO - Epoch [1/5][311/690] lr: 4.1e-06, eta: 0:56:01.919224, loss: 3.5474
2023-04-14 03:15:54 - training - INFO - Epoch [1/5][321/690] lr: 4.1e-06, eta: 0:55:24.859755, loss: 4.0059
2023-04-14 03:16:02 - training - INFO - Epoch [1/5][331/690] lr: 4.1e-06, eta: 0:54:53.311553, loss: 3.4792
2023-04-14 03:16:11 - training - INFO - Epoch [1/5][341/690] lr: 4.1e-06, eta: 0:54:21.325455, loss: 2.7961
2023-04-14 03:16:19 - training - INFO - Epoch [1/5][351/690] lr: 4.1e-06, eta: 0:53:50.555649, loss: 2.6722
2023-04-14 03:16:27 - training - INFO - Epoch [1/5][361/690] lr: 4.1e-06, eta: 0:53:21.918395, loss: 3.4421
2023-04-14 03:16:35 - training - INFO - Epoch [1/5][371/690] lr: 4.1e-06, eta: 0:52:52.770945, loss: 3.8570
2023-04-14 03:16:43 - training - INFO - Epoch [1/5][381/690] lr: 4.0e-06, eta: 0:52:21.873405, loss: 3.9457
2023-04-14 03:16:51 - training - INFO - Epoch [1/5][391/690] lr: 4.0e-06, eta: 0:51:58.470019, loss: 3.0661
2023-04-14 03:17:00 - training - INFO - Epoch [1/5][401/690] lr: 4.0e-06, eta: 0:51:33.481861, loss: 3.1917
2023-04-14 03:17:09 - training - INFO - Epoch [1/5][411/690] lr: 4.0e-06, eta: 0:51:15.252231, loss: 3.1766
2023-04-14 03:17:16 - training - INFO - Epoch [1/5][421/690] lr: 4.0e-06, eta: 0:50:48.261411, loss: 3.0767
2023-04-14 03:17:24 - training - INFO - Epoch [1/5][431/690] lr: 4.0e-06, eta: 0:50:23.495291, loss: 3.3811
2023-04-14 03:17:33 - training - INFO - Epoch [1/5][441/690] lr: 4.0e-06, eta: 0:50:00.878709, loss: 4.1717
2023-04-14 03:17:40 - training - INFO - Epoch [1/5][451/690] lr: 3.9e-06, eta: 0:49:36.756417, loss: 3.7078
2023-04-14 03:17:48 - training - INFO - Epoch [1/5][461/690] lr: 3.9e-06, eta: 0:49:14.025711, loss: 3.6001
2023-04-14 03:17:57 - training - INFO - Epoch [1/5][471/690] lr: 3.9e-06, eta: 0:48:53.460027, loss: 3.2117
2023-04-14 03:18:04 - training - INFO - Epoch [1/5][481/690] lr: 3.9e-06, eta: 0:48:30.923391, loss: 3.3676
2023-04-14 03:18:12 - training - INFO - Epoch [1/5][491/690] lr: 3.9e-06, eta: 0:48:10.306815, loss: 3.4379
2023-04-14 03:18:20 - training - INFO - Epoch [1/5][501/690] lr: 3.9e-06, eta: 0:47:50.123097, loss: 3.2209
2023-04-14 03:18:29 - training - INFO - Epoch [1/5][511/690] lr: 3.9e-06, eta: 0:47:31.558872, loss: 2.8341
2023-04-14 03:18:37 - training - INFO - Epoch [1/5][521/690] lr: 3.9e-06, eta: 0:47:13.391582, loss: 2.7125
2023-04-14 03:18:45 - training - INFO - Epoch [1/5][531/690] lr: 3.8e-06, eta: 0:46:56.259957, loss: 2.8633
2023-04-14 03:18:53 - training - INFO - Epoch [1/5][541/690] lr: 3.8e-06, eta: 0:46:36.087165, loss: 3.5678
2023-04-14 03:19:01 - training - INFO - Epoch [1/5][551/690] lr: 3.8e-06, eta: 0:46:18.546550, loss: 3.2401
2023-04-14 03:19:09 - training - INFO - Epoch [1/5][561/690] lr: 3.8e-06, eta: 0:45:59.991705, loss: 3.3092
2023-04-14 03:19:17 - training - INFO - Epoch [1/5][571/690] lr: 3.8e-06, eta: 0:45:42.377055, loss: 2.3254
2023-04-14 03:19:25 - training - INFO - Epoch [1/5][581/690] lr: 3.8e-06, eta: 0:45:27.767737, loss: 3.0917
2023-04-14 03:19:33 - training - INFO - Epoch [1/5][591/690] lr: 3.8e-06, eta: 0:45:10.532130, loss: 2.8201
2023-04-14 03:19:41 - training - INFO - Epoch [1/5][601/690] lr: 3.7e-06, eta: 0:44:54.339186, loss: 2.4737
2023-04-14 03:19:49 - training - INFO - Epoch [1/5][611/690] lr: 3.7e-06, eta: 0:44:38.698704, loss: 2.5572
2023-04-14 03:19:58 - training - INFO - Epoch [1/5][621/690] lr: 3.7e-06, eta: 0:44:24.114564, loss: 2.1991
2023-04-14 03:20:06 - training - INFO - Epoch [1/5][631/690] lr: 3.7e-06, eta: 0:44:09.245458, loss: 2.8423
2023-04-14 03:20:15 - training - INFO - Epoch [1/5][641/690] lr: 3.7e-06, eta: 0:43:57.648191, loss: 3.5523
2023-04-14 03:20:23 - training - INFO - Epoch [1/5][651/690] lr: 3.7e-06, eta: 0:43:41.812104, loss: 3.0037
2023-04-14 03:20:31 - training - INFO - Epoch [1/5][661/690] lr: 3.7e-06, eta: 0:43:27.352430, loss: 2.9029
2023-04-14 03:20:39 - training - INFO - Epoch [1/5][671/690] lr: 3.7e-06, eta: 0:43:12.807000, loss: 1.9499
2023-04-14 03:20:47 - training - INFO - Epoch [1/5][681/690] lr: 3.6e-06, eta: 0:42:58.296201, loss: 2.8565
2023-04-14 03:22:15 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 3.6471, Validation Metrics: {'exact_match': 22.58440046565774, 'f1': 31.342717540245456}, Test Metrics: {'exact_match': 24.738675958188153, 'f1': 32.82469043485645}
2023-04-14 03:22:16 - training - INFO - Epoch [2/5][1/690] lr: 3.6e-06, eta: 28 days, 20:46:18.225466, loss: 1.5303
2023-04-14 03:22:25 - training - INFO - Epoch [2/5][11/690] lr: 3.6e-06, eta: 2 days, 15:35:33.906734, loss: 2.3524
2023-04-14 03:22:33 - training - INFO - Epoch [2/5][21/690] lr: 3.6e-06, eta: 1 day, 9:34:56.938800, loss: 2.4938
2023-04-14 03:22:42 - training - INFO - Epoch [2/5][31/690] lr: 3.6e-06, eta: 22:56:15.462346, loss: 1.2778
2023-04-14 03:22:49 - training - INFO - Epoch [2/5][41/690] lr: 3.6e-06, eta: 17:28:24.125921, loss: 3.1050
2023-04-14 03:22:58 - training - INFO - Epoch [2/5][51/690] lr: 3.6e-06, eta: 14:09:28.055985, loss: 3.1668
2023-04-14 03:23:06 - training - INFO - Epoch [2/5][61/690] lr: 3.6e-06, eta: 11:55:45.960407, loss: 1.8438
2023-04-14 03:23:14 - training - INFO - Epoch [2/5][71/690] lr: 3.5e-06, eta: 10:19:14.379067, loss: 2.6663
2023-04-14 03:23:21 - training - INFO - Epoch [2/5][81/690] lr: 3.5e-06, eta: 9:06:37.595697, loss: 3.0737
2023-04-14 03:23:29 - training - INFO - Epoch [2/5][91/690] lr: 3.5e-06, eta: 8:10:00.883612, loss: 2.4102
2023-04-14 03:23:37 - training - INFO - Epoch [2/5][101/690] lr: 3.5e-06, eta: 7:24:34.225717, loss: 2.3906
2023-04-14 03:23:46 - training - INFO - Epoch [2/5][111/690] lr: 3.5e-06, eta: 6:47:26.955960, loss: 2.2747
2023-04-14 03:23:54 - training - INFO - Epoch [2/5][121/690] lr: 3.5e-06, eta: 6:16:30.677225, loss: 2.3128
2023-04-14 03:24:02 - training - INFO - Epoch [2/5][131/690] lr: 3.5e-06, eta: 5:50:14.832644, loss: 3.2894
2023-04-14 03:24:10 - training - INFO - Epoch [2/5][141/690] lr: 3.4e-06, eta: 5:27:34.417665, loss: 2.6340
2023-04-14 03:24:19 - training - INFO - Epoch [2/5][151/690] lr: 3.4e-06, eta: 5:07:56.732393, loss: 1.9306
2023-04-14 03:24:27 - training - INFO - Epoch [2/5][161/690] lr: 3.4e-06, eta: 4:50:52.062199, loss: 3.0482
2023-04-14 03:24:35 - training - INFO - Epoch [2/5][171/690] lr: 3.4e-06, eta: 4:35:30.485001, loss: 2.4801
2023-04-14 03:24:43 - training - INFO - Epoch [2/5][181/690] lr: 3.4e-06, eta: 4:21:55.593278, loss: 1.6507
2023-04-14 03:24:51 - training - INFO - Epoch [2/5][191/690] lr: 3.4e-06, eta: 4:09:37.686128, loss: 2.4036
2023-04-14 03:24:59 - training - INFO - Epoch [2/5][201/690] lr: 3.4e-06, eta: 3:58:39.866781, loss: 1.8708
2023-04-14 03:25:07 - training - INFO - Epoch [2/5][211/690] lr: 3.4e-06, eta: 3:48:47.730618, loss: 2.6916
2023-04-14 03:25:15 - training - INFO - Epoch [2/5][221/690] lr: 3.3e-06, eta: 3:39:43.393490, loss: 2.6430
2023-04-14 03:25:23 - training - INFO - Epoch [2/5][231/690] lr: 3.3e-06, eta: 3:31:25.187337, loss: 2.6693
2023-04-14 03:25:31 - training - INFO - Epoch [2/5][241/690] lr: 3.3e-06, eta: 3:23:48.841155, loss: 2.4564
2023-04-14 03:25:39 - training - INFO - Epoch [2/5][251/690] lr: 3.3e-06, eta: 3:16:48.801396, loss: 2.4585
2023-04-14 03:25:48 - training - INFO - Epoch [2/5][261/690] lr: 3.3e-06, eta: 3:10:21.633108, loss: 2.1341
2023-04-14 03:25:55 - training - INFO - Epoch [2/5][271/690] lr: 3.3e-06, eta: 3:04:17.181905, loss: 2.2916
2023-04-14 03:26:03 - training - INFO - Epoch [2/5][281/690] lr: 3.3e-06, eta: 2:58:38.207645, loss: 2.4010
2023-04-14 03:26:11 - training - INFO - Epoch [2/5][291/690] lr: 3.2e-06, eta: 2:53:24.036981, loss: 2.1608
2023-04-14 03:26:19 - training - INFO - Epoch [2/5][301/690] lr: 3.2e-06, eta: 2:48:32.380551, loss: 1.9790
2023-04-14 03:26:28 - training - INFO - Epoch [2/5][311/690] lr: 3.2e-06, eta: 2:43:59.999084, loss: 1.8489
2023-04-14 03:26:36 - training - INFO - Epoch [2/5][321/690] lr: 3.2e-06, eta: 2:39:41.833443, loss: 2.4105
2023-04-14 03:26:44 - training - INFO - Epoch [2/5][331/690] lr: 3.2e-06, eta: 2:35:36.296078, loss: 2.0045
2023-04-14 03:26:51 - training - INFO - Epoch [2/5][341/690] lr: 3.2e-06, eta: 2:31:43.587260, loss: 2.5020
2023-04-14 03:27:00 - training - INFO - Epoch [2/5][351/690] lr: 3.2e-06, eta: 2:28:08.220207, loss: 1.8818
2023-04-14 03:27:08 - training - INFO - Epoch [2/5][361/690] lr: 3.2e-06, eta: 2:24:44.281773, loss: 2.6541
2023-04-14 03:27:16 - training - INFO - Epoch [2/5][371/690] lr: 3.1e-06, eta: 2:21:32.599407, loss: 2.0644
2023-04-14 03:27:24 - training - INFO - Epoch [2/5][381/690] lr: 3.1e-06, eta: 2:18:28.439766, loss: 2.1433
2023-04-14 03:27:32 - training - INFO - Epoch [2/5][391/690] lr: 3.1e-06, eta: 2:15:32.886825, loss: 2.0726
2023-04-14 03:27:40 - training - INFO - Epoch [2/5][401/690] lr: 3.1e-06, eta: 2:12:44.613045, loss: 2.0247
2023-04-14 03:27:48 - training - INFO - Epoch [2/5][411/690] lr: 3.1e-06, eta: 2:10:03.814671, loss: 2.4254
2023-04-14 03:27:57 - training - INFO - Epoch [2/5][421/690] lr: 3.1e-06, eta: 2:07:34.513204, loss: 1.7947
2023-04-14 03:28:05 - training - INFO - Epoch [2/5][431/690] lr: 3.1e-06, eta: 2:05:09.991944, loss: 1.7595
2023-04-14 03:28:13 - training - INFO - Epoch [2/5][441/690] lr: 3.1e-06, eta: 2:02:50.623734, loss: 2.0392
2023-04-14 03:28:22 - training - INFO - Epoch [2/5][451/690] lr: 3.0e-06, eta: 2:00:39.502028, loss: 1.6063
2023-04-14 03:28:30 - training - INFO - Epoch [2/5][461/690] lr: 3.0e-06, eta: 1:58:32.702114, loss: 2.6402
2023-04-14 03:28:38 - training - INFO - Epoch [2/5][471/690] lr: 3.0e-06, eta: 1:56:30.283080, loss: 2.7011
2023-04-14 03:28:46 - training - INFO - Epoch [2/5][481/690] lr: 3.0e-06, eta: 1:54:31.901919, loss: 2.9811
2023-04-14 03:28:54 - training - INFO - Epoch [2/5][491/690] lr: 3.0e-06, eta: 1:52:38.104485, loss: 1.9078
2023-04-14 03:29:02 - training - INFO - Epoch [2/5][501/690] lr: 3.0e-06, eta: 1:50:47.104980, loss: 2.0250
2023-04-14 03:29:11 - training - INFO - Epoch [2/5][511/690] lr: 3.0e-06, eta: 1:49:03.742280, loss: 2.8028
2023-04-14 03:29:19 - training - INFO - Epoch [2/5][521/690] lr: 2.9e-06, eta: 1:47:23.266922, loss: 2.4759
2023-04-14 03:29:27 - training - INFO - Epoch [2/5][531/690] lr: 2.9e-06, eta: 1:45:46.770024, loss: 2.9710
2023-04-14 03:29:35 - training - INFO - Epoch [2/5][541/690] lr: 2.9e-06, eta: 1:44:11.461363, loss: 2.4573
2023-04-14 03:29:44 - training - INFO - Epoch [2/5][551/690] lr: 2.9e-06, eta: 1:42:42.131794, loss: 1.7770
2023-04-14 03:29:52 - training - INFO - Epoch [2/5][561/690] lr: 2.9e-06, eta: 1:41:12.547995, loss: 2.1838
2023-04-14 03:30:00 - training - INFO - Epoch [2/5][571/690] lr: 2.9e-06, eta: 1:39:47.908303, loss: 1.9923
2023-04-14 03:30:08 - training - INFO - Epoch [2/5][581/690] lr: 2.9e-06, eta: 1:38:23.842545, loss: 2.1482
2023-04-14 03:30:16 - training - INFO - Epoch [2/5][591/690] lr: 2.9e-06, eta: 1:37:02.018997, loss: 2.5774
2023-04-14 03:30:24 - training - INFO - Epoch [2/5][601/690] lr: 2.8e-06, eta: 1:35:43.512775, loss: 2.1401
2023-04-14 03:30:33 - training - INFO - Epoch [2/5][611/690] lr: 2.8e-06, eta: 1:34:30.070673, loss: 2.4016
2023-04-14 03:30:42 - training - INFO - Epoch [2/5][621/690] lr: 2.8e-06, eta: 1:33:17.428281, loss: 2.1500
2023-04-14 03:30:50 - training - INFO - Epoch [2/5][631/690] lr: 2.8e-06, eta: 1:32:07.204843, loss: 2.5003
2023-04-14 03:30:58 - training - INFO - Epoch [2/5][641/690] lr: 2.8e-06, eta: 1:30:58.499362, loss: 1.9893
2023-04-14 03:31:07 - training - INFO - Epoch [2/5][651/690] lr: 2.8e-06, eta: 1:29:51.397413, loss: 2.2510
2023-04-14 03:31:15 - training - INFO - Epoch [2/5][661/690] lr: 2.8e-06, eta: 1:28:43.576264, loss: 2.2226
2023-04-14 03:31:23 - training - INFO - Epoch [2/5][671/690] lr: 2.7e-06, eta: 1:27:39.768836, loss: 2.7088
2023-04-14 03:31:31 - training - INFO - Epoch [2/5][681/690] lr: 2.7e-06, eta: 1:26:36.031269, loss: 2.2860
2023-04-14 03:32:59 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 2.3197, Validation Metrics: {'exact_match': 24.447031431897557, 'f1': 32.94635513922646}, Test Metrics: {'exact_match': 27.40998838559814, 'f1': 36.07765462961202}
2023-04-14 03:33:00 - training - INFO - Epoch [3/5][1/690] lr: 2.7e-06, eta: 54 days, 14:08:32.873542, loss: 1.7403
2023-04-14 03:33:08 - training - INFO - Epoch [3/5][11/690] lr: 2.7e-06, eta: 4 days, 23:25:36.571856, loss: 2.1866
2023-04-14 03:33:16 - training - INFO - Epoch [3/5][21/690] lr: 2.7e-06, eta: 2 days, 14:43:45.257772, loss: 2.4354
2023-04-14 03:33:24 - training - INFO - Epoch [3/5][31/690] lr: 2.7e-06, eta: 1 day, 18:37:26.350863, loss: 2.3328
2023-04-14 03:33:33 - training - INFO - Epoch [3/5][41/690] lr: 2.7e-06, eta: 1 day, 8:20:12.795576, loss: 2.1922
2023-04-14 03:33:42 - training - INFO - Epoch [3/5][51/690] lr: 2.7e-06, eta: 1 day, 2:04:52.334283, loss: 1.8816
2023-04-14 03:33:50 - training - INFO - Epoch [3/5][61/690] lr: 2.6e-06, eta: 21:51:55.669257, loss: 2.2579
2023-04-14 03:33:58 - training - INFO - Epoch [3/5][71/690] lr: 2.6e-06, eta: 18:50:27.454307, loss: 1.8292
2023-04-14 03:34:06 - training - INFO - Epoch [3/5][81/690] lr: 2.6e-06, eta: 16:33:30.452628, loss: 2.0461
2023-04-14 03:34:14 - training - INFO - Epoch [3/5][91/690] lr: 2.6e-06, eta: 14:46:45.189528, loss: 1.7120
2023-04-14 03:34:23 - training - INFO - Epoch [3/5][101/690] lr: 2.6e-06, eta: 13:21:16.385305, loss: 2.3170
2023-04-14 03:34:31 - training - INFO - Epoch [3/5][111/690] lr: 2.6e-06, eta: 12:10:58.179036, loss: 1.4432
2023-04-14 03:34:39 - training - INFO - Epoch [3/5][121/690] lr: 2.6e-06, eta: 11:12:13.095391, loss: 1.5332
2023-04-14 03:34:47 - training - INFO - Epoch [3/5][131/690] lr: 2.6e-06, eta: 10:22:26.300725, loss: 2.2629
2023-04-14 03:34:55 - training - INFO - Epoch [3/5][141/690] lr: 2.5e-06, eta: 9:39:41.855301, loss: 2.2349
2023-04-14 03:35:03 - training - INFO - Epoch [3/5][151/690] lr: 2.5e-06, eta: 9:02:44.280545, loss: 2.4182
2023-04-14 03:35:11 - training - INFO - Epoch [3/5][161/690] lr: 2.5e-06, eta: 8:30:15.110526, loss: 2.2572
2023-04-14 03:35:19 - training - INFO - Epoch [3/5][171/690] lr: 2.5e-06, eta: 8:01:23.907645, loss: 3.0848
2023-04-14 03:35:28 - training - INFO - Epoch [3/5][181/690] lr: 2.5e-06, eta: 7:36:01.222714, loss: 1.6252
2023-04-14 03:35:36 - training - INFO - Epoch [3/5][191/690] lr: 2.5e-06, eta: 7:13:11.730830, loss: 1.9175
2023-04-14 03:35:44 - training - INFO - Epoch [3/5][201/690] lr: 2.5e-06, eta: 6:52:28.698672, loss: 1.7588
2023-04-14 03:35:52 - training - INFO - Epoch [3/5][211/690] lr: 2.4e-06, eta: 6:33:48.009433, loss: 1.7343
2023-04-14 03:36:00 - training - INFO - Epoch [3/5][221/690] lr: 2.4e-06, eta: 6:16:42.383261, loss: 3.4184
2023-04-14 03:36:08 - training - INFO - Epoch [3/5][231/690] lr: 2.4e-06, eta: 6:01:14.621460, loss: 1.6452
2023-04-14 03:36:16 - training - INFO - Epoch [3/5][241/690] lr: 2.4e-06, eta: 5:46:55.881271, loss: 1.8979
2023-04-14 03:36:25 - training - INFO - Epoch [3/5][251/690] lr: 2.4e-06, eta: 5:33:58.942273, loss: 1.9769
2023-04-14 03:36:34 - training - INFO - Epoch [3/5][261/690] lr: 2.4e-06, eta: 5:21:53.614047, loss: 2.0557
2023-04-14 03:36:41 - training - INFO - Epoch [3/5][271/690] lr: 2.4e-06, eta: 5:10:34.156739, loss: 2.2627
2023-04-14 03:36:49 - training - INFO - Epoch [3/5][281/690] lr: 2.4e-06, eta: 5:00:04.651317, loss: 2.6396
2023-04-14 03:36:58 - training - INFO - Epoch [3/5][291/690] lr: 2.3e-06, eta: 4:50:20.068575, loss: 2.4225
2023-04-14 03:37:06 - training - INFO - Epoch [3/5][301/690] lr: 2.3e-06, eta: 4:41:14.836008, loss: 2.3362
2023-04-14 03:37:14 - training - INFO - Epoch [3/5][311/690] lr: 2.3e-06, eta: 4:32:41.130329, loss: 2.1941
2023-04-14 03:37:22 - training - INFO - Epoch [3/5][321/690] lr: 2.3e-06, eta: 4:24:36.583548, loss: 1.7411
2023-04-14 03:37:30 - training - INFO - Epoch [3/5][331/690] lr: 2.3e-06, eta: 4:17:06.296409, loss: 2.1434
2023-04-14 03:37:38 - training - INFO - Epoch [3/5][341/690] lr: 2.3e-06, eta: 4:10:00.788204, loss: 2.3805
2023-04-14 03:37:46 - training - INFO - Epoch [3/5][351/690] lr: 2.3e-06, eta: 4:03:19.825959, loss: 1.6334
2023-04-14 03:37:55 - training - INFO - Epoch [3/5][361/690] lr: 2.2e-06, eta: 3:57:00.508044, loss: 1.8661
2023-04-14 03:38:03 - training - INFO - Epoch [3/5][371/690] lr: 2.2e-06, eta: 3:51:00.463348, loss: 2.4340
2023-04-14 03:38:11 - training - INFO - Epoch [3/5][381/690] lr: 2.2e-06, eta: 3:45:16.492869, loss: 2.3045
2023-04-14 03:38:19 - training - INFO - Epoch [3/5][391/690] lr: 2.2e-06, eta: 3:39:52.491179, loss: 1.4964
2023-04-14 03:38:27 - training - INFO - Epoch [3/5][401/690] lr: 2.2e-06, eta: 3:34:44.162349, loss: 1.6314
2023-04-14 03:38:35 - training - INFO - Epoch [3/5][411/690] lr: 2.2e-06, eta: 3:29:48.577338, loss: 2.3013
2023-04-14 03:38:43 - training - INFO - Epoch [3/5][421/690] lr: 2.2e-06, eta: 3:25:07.399481, loss: 1.9156
2023-04-14 03:38:52 - training - INFO - Epoch [3/5][431/690] lr: 2.2e-06, eta: 3:20:38.974984, loss: 2.0095
2023-04-14 03:39:00 - training - INFO - Epoch [3/5][441/690] lr: 2.1e-06, eta: 3:16:22.852830, loss: 2.2586
2023-04-14 03:39:08 - training - INFO - Epoch [3/5][451/690] lr: 2.1e-06, eta: 3:12:15.791454, loss: 1.9953
2023-04-14 03:39:16 - training - INFO - Epoch [3/5][461/690] lr: 2.1e-06, eta: 3:08:21.848383, loss: 2.3136
2023-04-14 03:39:25 - training - INFO - Epoch [3/5][471/690] lr: 2.1e-06, eta: 3:04:39.803637, loss: 1.8336
2023-04-14 03:39:33 - training - INFO - Epoch [3/5][481/690] lr: 2.1e-06, eta: 3:01:04.013381, loss: 2.1022
2023-04-14 03:39:41 - training - INFO - Epoch [3/5][491/690] lr: 2.1e-06, eta: 2:57:36.634329, loss: 1.7412
2023-04-14 03:39:49 - training - INFO - Epoch [3/5][501/690] lr: 2.1e-06, eta: 2:54:16.269300, loss: 2.2024
2023-04-14 03:39:57 - training - INFO - Epoch [3/5][511/690] lr: 2.1e-06, eta: 2:51:02.899830, loss: 2.0232
2023-04-14 03:40:05 - training - INFO - Epoch [3/5][521/690] lr: 2.0e-06, eta: 2:47:57.180565, loss: 2.3464
2023-04-14 03:40:13 - training - INFO - Epoch [3/5][531/690] lr: 2.0e-06, eta: 2:44:57.660549, loss: 2.4510
2023-04-14 03:40:21 - training - INFO - Epoch [3/5][541/690] lr: 2.0e-06, eta: 2:42:04.458283, loss: 1.8746
2023-04-14 03:40:30 - training - INFO - Epoch [3/5][551/690] lr: 2.0e-06, eta: 2:39:18.316092, loss: 2.1401
2023-04-14 03:40:38 - training - INFO - Epoch [3/5][561/690] lr: 2.0e-06, eta: 2:36:36.764289, loss: 1.5627
2023-04-14 03:40:45 - training - INFO - Epoch [3/5][571/690] lr: 2.0e-06, eta: 2:34:00.075646, loss: 2.5143
2023-04-14 03:40:53 - training - INFO - Epoch [3/5][581/690] lr: 2.0e-06, eta: 2:31:27.485775, loss: 1.2638
2023-04-14 03:41:01 - training - INFO - Epoch [3/5][591/690] lr: 1.9e-06, eta: 2:29:01.313793, loss: 1.5611
2023-04-14 03:41:09 - training - INFO - Epoch [3/5][601/690] lr: 1.9e-06, eta: 2:26:39.575246, loss: 2.1016
2023-04-14 03:41:17 - training - INFO - Epoch [3/5][611/690] lr: 1.9e-06, eta: 2:24:23.086423, loss: 1.7061
2023-04-14 03:41:25 - training - INFO - Epoch [3/5][621/690] lr: 1.9e-06, eta: 2:22:10.241265, loss: 1.5542
2023-04-14 03:41:34 - training - INFO - Epoch [3/5][631/690] lr: 1.9e-06, eta: 2:20:02.240925, loss: 1.7311
2023-04-14 03:41:42 - training - INFO - Epoch [3/5][641/690] lr: 1.9e-06, eta: 2:17:57.128614, loss: 2.1211
2023-04-14 03:41:49 - training - INFO - Epoch [3/5][651/690] lr: 1.9e-06, eta: 2:15:54.057996, loss: 1.9305
2023-04-14 03:41:57 - training - INFO - Epoch [3/5][661/690] lr: 1.9e-06, eta: 2:13:55.357221, loss: 1.9438
2023-04-14 03:42:05 - training - INFO - Epoch [3/5][671/690] lr: 1.8e-06, eta: 2:11:59.538620, loss: 1.6098
2023-04-14 03:42:13 - training - INFO - Epoch [3/5][681/690] lr: 1.8e-06, eta: 2:10:07.273032, loss: 1.7200
2023-04-14 03:43:42 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 2.0449, Validation Metrics: {'exact_match': 26.19324796274738, 'f1': 34.43152915124247}, Test Metrics: {'exact_match': 29.849012775842045, 'f1': 38.58481130053175}
2023-04-14 03:43:43 - training - INFO - Epoch [4/5][1/690] lr: 1.8e-06, eta: 80 days, 6:04:34.125088, loss: 2.0297
2023-04-14 03:43:52 - training - INFO - Epoch [4/5][11/690] lr: 1.8e-06, eta: 7 days, 7:19:11.712154, loss: 1.7106
2023-04-14 03:44:00 - training - INFO - Epoch [4/5][21/690] lr: 1.8e-06, eta: 3 days, 19:56:30.746958, loss: 1.7686
2023-04-14 03:44:08 - training - INFO - Epoch [4/5][31/690] lr: 1.8e-06, eta: 2 days, 14:21:35.386375, loss: 1.7980
2023-04-14 03:44:16 - training - INFO - Epoch [4/5][41/690] lr: 1.8e-06, eta: 1 day, 23:11:32.918265, loss: 2.7539
2023-04-14 03:44:25 - training - INFO - Epoch [4/5][51/690] lr: 1.7e-06, eta: 1 day, 13:59:06.606777, loss: 1.2572
2023-04-14 03:44:33 - training - INFO - Epoch [4/5][61/690] lr: 1.7e-06, eta: 1 day, 7:47:22.737709, loss: 1.6839
2023-04-14 03:44:41 - training - INFO - Epoch [4/5][71/690] lr: 1.7e-06, eta: 1 day, 3:20:07.593531, loss: 1.3681
2023-04-14 03:44:49 - training - INFO - Epoch [4/5][81/690] lr: 1.7e-06, eta: 23:58:57.982896, loss: 2.1291
2023-04-14 03:44:57 - training - INFO - Epoch [4/5][91/690] lr: 1.7e-06, eta: 21:21:53.616148, loss: 2.2810
2023-04-14 03:45:05 - training - INFO - Epoch [4/5][101/690] lr: 1.7e-06, eta: 19:16:00.462502, loss: 2.5508
2023-04-14 03:45:13 - training - INFO - Epoch [4/5][111/690] lr: 1.7e-06, eta: 17:32:41.238546, loss: 1.8634
2023-04-14 03:45:20 - training - INFO - Epoch [4/5][121/690] lr: 1.7e-06, eta: 16:06:16.429201, loss: 1.8825
2023-04-14 03:45:28 - training - INFO - Epoch [4/5][131/690] lr: 1.6e-06, eta: 14:53:07.143511, loss: 1.8862
2023-04-14 03:45:36 - training - INFO - Epoch [4/5][141/690] lr: 1.6e-06, eta: 13:50:22.689789, loss: 2.0035
2023-04-14 03:45:44 - training - INFO - Epoch [4/5][151/690] lr: 1.6e-06, eta: 12:56:02.811780, loss: 2.3222
2023-04-14 03:45:52 - training - INFO - Epoch [4/5][161/690] lr: 1.6e-06, eta: 12:08:25.001626, loss: 1.9625
2023-04-14 03:46:00 - training - INFO - Epoch [4/5][171/690] lr: 1.6e-06, eta: 11:26:22.036512, loss: 1.9404
2023-04-14 03:46:09 - training - INFO - Epoch [4/5][181/690] lr: 1.6e-06, eta: 10:49:02.671873, loss: 1.2584
2023-04-14 03:46:17 - training - INFO - Epoch [4/5][191/690] lr: 1.6e-06, eta: 10:15:23.912711, loss: 1.4107
2023-04-14 03:46:25 - training - INFO - Epoch [4/5][201/690] lr: 1.6e-06, eta: 9:45:06.078555, loss: 1.3796
2023-04-14 03:46:32 - training - INFO - Epoch [4/5][211/690] lr: 1.5e-06, eta: 9:17:39.229529, loss: 1.9725
2023-04-14 03:46:41 - training - INFO - Epoch [4/5][221/690] lr: 1.5e-06, eta: 8:52:44.807410, loss: 1.9231
2023-04-14 03:46:48 - training - INFO - Epoch [4/5][231/690] lr: 1.5e-06, eta: 8:29:56.398641, loss: 1.5985
2023-04-14 03:46:56 - training - INFO - Epoch [4/5][241/690] lr: 1.5e-06, eta: 8:09:01.513963, loss: 2.0170
2023-04-14 03:47:05 - training - INFO - Epoch [4/5][251/690] lr: 1.5e-06, eta: 7:49:54.357709, loss: 1.8672
2023-04-14 03:47:13 - training - INFO - Epoch [4/5][261/690] lr: 1.5e-06, eta: 7:32:12.155505, loss: 1.8168
2023-04-14 03:47:22 - training - INFO - Epoch [4/5][271/690] lr: 1.5e-06, eta: 7:15:44.054673, loss: 1.6398
2023-04-14 03:47:30 - training - INFO - Epoch [4/5][281/690] lr: 1.4e-06, eta: 7:00:25.769223, loss: 1.4322
2023-04-14 03:47:38 - training - INFO - Epoch [4/5][291/690] lr: 1.4e-06, eta: 6:46:10.377174, loss: 1.9063
2023-04-14 03:47:46 - training - INFO - Epoch [4/5][301/690] lr: 1.4e-06, eta: 6:32:50.923141, loss: 1.1848
2023-04-14 03:47:54 - training - INFO - Epoch [4/5][311/690] lr: 1.4e-06, eta: 6:20:23.957788, loss: 1.8339
2023-04-14 03:48:03 - training - INFO - Epoch [4/5][321/690] lr: 1.4e-06, eta: 6:08:44.398653, loss: 2.0148
2023-04-14 03:48:11 - training - INFO - Epoch [4/5][331/690] lr: 1.4e-06, eta: 5:57:45.372827, loss: 1.7531
2023-04-14 03:48:19 - training - INFO - Epoch [4/5][341/690] lr: 1.4e-06, eta: 5:47:26.762155, loss: 2.2828
2023-04-14 03:48:27 - training - INFO - Epoch [4/5][351/690] lr: 1.4e-06, eta: 5:37:38.680533, loss: 1.6461
2023-04-14 03:48:35 - training - INFO - Epoch [4/5][361/690] lr: 1.3e-06, eta: 5:28:22.821998, loss: 1.3593
2023-04-14 03:48:43 - training - INFO - Epoch [4/5][371/690] lr: 1.3e-06, eta: 5:19:35.119090, loss: 2.6996
2023-04-14 03:48:52 - training - INFO - Epoch [4/5][381/690] lr: 1.3e-06, eta: 5:11:18.250107, loss: 1.5744
2023-04-14 03:49:00 - training - INFO - Epoch [4/5][391/690] lr: 1.3e-06, eta: 5:03:27.684971, loss: 1.3816
2023-04-14 03:49:08 - training - INFO - Epoch [4/5][401/690] lr: 1.3e-06, eta: 4:55:53.290340, loss: 1.8784
2023-04-14 03:49:16 - training - INFO - Epoch [4/5][411/690] lr: 1.3e-06, eta: 4:48:45.266064, loss: 1.9263
2023-04-14 03:49:24 - training - INFO - Epoch [4/5][421/690] lr: 1.3e-06, eta: 4:41:57.743453, loss: 2.1330
2023-04-14 03:49:33 - training - INFO - Epoch [4/5][431/690] lr: 1.2e-06, eta: 4:35:31.648511, loss: 1.5196
2023-04-14 03:49:41 - training - INFO - Epoch [4/5][441/690] lr: 1.2e-06, eta: 4:29:19.891671, loss: 1.3930
2023-04-14 03:49:50 - training - INFO - Epoch [4/5][451/690] lr: 1.2e-06, eta: 4:23:24.894945, loss: 2.3005
2023-04-14 03:49:57 - training - INFO - Epoch [4/5][461/690] lr: 1.2e-06, eta: 4:17:41.143509, loss: 2.2791
2023-04-14 03:50:05 - training - INFO - Epoch [4/5][471/690] lr: 1.2e-06, eta: 4:12:12.140316, loss: 2.3272
2023-04-14 03:50:13 - training - INFO - Epoch [4/5][481/690] lr: 1.2e-06, eta: 4:06:57.171563, loss: 1.7421
2023-04-14 03:50:22 - training - INFO - Epoch [4/5][491/690] lr: 1.2e-06, eta: 4:01:56.587690, loss: 2.2508
2023-04-14 03:50:30 - training - INFO - Epoch [4/5][501/690] lr: 1.2e-06, eta: 3:57:05.530701, loss: 2.6214
2023-04-14 03:50:38 - training - INFO - Epoch [4/5][511/690] lr: 1.1e-06, eta: 3:52:26.201580, loss: 2.3271
2023-04-14 03:50:46 - training - INFO - Epoch [4/5][521/690] lr: 1.1e-06, eta: 3:47:59.194469, loss: 1.4767
2023-04-14 03:50:54 - training - INFO - Epoch [4/5][531/690] lr: 1.1e-06, eta: 3:43:40.499484, loss: 1.4200
2023-04-14 03:51:02 - training - INFO - Epoch [4/5][541/690] lr: 1.1e-06, eta: 3:39:30.637132, loss: 2.0445
2023-04-14 03:51:10 - training - INFO - Epoch [4/5][551/690] lr: 1.1e-06, eta: 3:35:28.160076, loss: 1.9257
2023-04-14 03:51:18 - training - INFO - Epoch [4/5][561/690] lr: 1.1e-06, eta: 3:31:36.652314, loss: 2.3235
2023-04-14 03:51:27 - training - INFO - Epoch [4/5][571/690] lr: 1.1e-06, eta: 3:27:54.200296, loss: 1.1343
2023-04-14 03:51:35 - training - INFO - Epoch [4/5][581/690] lr: 1.1e-06, eta: 3:24:18.858292, loss: 2.2257
2023-04-14 03:51:43 - training - INFO - Epoch [4/5][591/690] lr: 1.0e-06, eta: 3:20:48.400659, loss: 1.9572
2023-04-14 03:51:51 - training - INFO - Epoch [4/5][601/690] lr: 1.0e-06, eta: 3:17:23.019496, loss: 0.8104
2023-04-14 03:51:59 - training - INFO - Epoch [4/5][611/690] lr: 1.0e-06, eta: 3:14:05.498508, loss: 1.7757
2023-04-14 03:52:07 - training - INFO - Epoch [4/5][621/690] lr: 1.0e-06, eta: 3:10:54.493695, loss: 1.8986
2023-04-14 03:52:15 - training - INFO - Epoch [4/5][631/690] lr: 9.9e-07, eta: 3:07:48.394338, loss: 2.0610
2023-04-14 03:52:23 - training - INFO - Epoch [4/5][641/690] lr: 9.7e-07, eta: 3:04:46.993786, loss: 1.5499
2023-04-14 03:52:31 - training - INFO - Epoch [4/5][651/690] lr: 9.6e-07, eta: 3:01:51.397680, loss: 1.8243
2023-04-14 03:52:38 - training - INFO - Epoch [4/5][661/690] lr: 9.5e-07, eta: 2:59:00.664909, loss: 1.3719
2023-04-14 03:52:47 - training - INFO - Epoch [4/5][671/690] lr: 9.3e-07, eta: 2:56:17.993937, loss: 1.7003
2023-04-14 03:52:55 - training - INFO - Epoch [4/5][681/690] lr: 9.2e-07, eta: 2:53:39.320574, loss: 1.8671
2023-04-14 03:54:24 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 1.9019, Validation Metrics: {'exact_match': 27.82305005820722, 'f1': 36.768556048804875}, Test Metrics: {'exact_match': 31.358885017421603, 'f1': 39.72589695381253}
2023-04-14 03:54:24 - training - INFO - Epoch [5/5][1/690] lr: 9.1e-07, eta: 105 days, 20:22:43.723156, loss: 1.5501
2023-04-14 03:54:32 - training - INFO - Epoch [5/5][11/690] lr: 8.9e-07, eta: 9 days, 14:56:01.852848, loss: 1.0708
2023-04-14 03:54:40 - training - INFO - Epoch [5/5][21/690] lr: 8.8e-07, eta: 5 days, 0:58:51.986694, loss: 1.7130
2023-04-14 03:54:48 - training - INFO - Epoch [5/5][31/690] lr: 8.7e-07, eta: 3 days, 9:58:12.881395, loss: 1.3380
2023-04-14 03:54:57 - training - INFO - Epoch [5/5][41/690] lr: 8.5e-07, eta: 2 days, 13:59:58.155845, loss: 2.1453
2023-04-14 03:55:05 - training - INFO - Epoch [5/5][51/690] lr: 8.4e-07, eta: 2 days, 1:50:36.768753, loss: 1.4841
2023-04-14 03:55:13 - training - INFO - Epoch [5/5][61/690] lr: 8.3e-07, eta: 1 day, 17:40:32.836337, loss: 1.2008
2023-04-14 03:55:21 - training - INFO - Epoch [5/5][71/690] lr: 8.1e-07, eta: 1 day, 11:48:13.769523, loss: 1.6333
2023-04-14 03:55:29 - training - INFO - Epoch [5/5][81/690] lr: 8.0e-07, eta: 1 day, 7:22:48.927303, loss: 2.1174
2023-04-14 03:55:37 - training - INFO - Epoch [5/5][91/690] lr: 7.9e-07, eta: 1 day, 3:55:48.339590, loss: 2.9956
2023-04-14 03:55:45 - training - INFO - Epoch [5/5][101/690] lr: 7.8e-07, eta: 1 day, 1:09:52.134547, loss: 1.5311
2023-04-14 03:55:53 - training - INFO - Epoch [5/5][111/690] lr: 7.6e-07, eta: 22:53:54.133530, loss: 1.7864
2023-04-14 03:56:01 - training - INFO - Epoch [5/5][121/690] lr: 7.5e-07, eta: 21:00:17.562542, loss: 1.6514
2023-04-14 03:56:09 - training - INFO - Epoch [5/5][131/690] lr: 7.4e-07, eta: 19:23:48.115738, loss: 2.0580
2023-04-14 03:56:17 - training - INFO - Epoch [5/5][141/690] lr: 7.2e-07, eta: 18:01:19.877355, loss: 1.8650
2023-04-14 03:56:25 - training - INFO - Epoch [5/5][151/690] lr: 7.1e-07, eta: 16:49:35.020669, loss: 2.5669
2023-04-14 03:56:33 - training - INFO - Epoch [5/5][161/690] lr: 7.0e-07, eta: 15:46:40.434691, loss: 1.6822
2023-04-14 03:56:41 - training - INFO - Epoch [5/5][171/690] lr: 6.8e-07, eta: 14:51:04.973772, loss: 1.9336
2023-04-14 03:56:49 - training - INFO - Epoch [5/5][181/690] lr: 6.7e-07, eta: 14:01:44.189939, loss: 1.3595
2023-04-14 03:56:57 - training - INFO - Epoch [5/5][191/690] lr: 6.6e-07, eta: 13:17:32.656347, loss: 2.5208
2023-04-14 03:57:06 - training - INFO - Epoch [5/5][201/690] lr: 6.4e-07, eta: 12:37:53.198940, loss: 1.4690
2023-04-14 03:57:14 - training - INFO - Epoch [5/5][211/690] lr: 6.3e-07, eta: 12:01:50.502274, loss: 1.8085
2023-04-14 03:57:22 - training - INFO - Epoch [5/5][221/690] lr: 6.2e-07, eta: 11:28:58.568578, loss: 2.1064
2023-04-14 03:57:30 - training - INFO - Epoch [5/5][231/690] lr: 6.0e-07, eta: 10:58:57.985548, loss: 2.3927
2023-04-14 03:57:38 - training - INFO - Epoch [5/5][241/690] lr: 5.9e-07, eta: 10:31:30.261082, loss: 2.0930
2023-04-14 03:57:46 - training - INFO - Epoch [5/5][251/690] lr: 5.8e-07, eta: 10:06:08.656842, loss: 2.4517
2023-04-14 03:57:54 - training - INFO - Epoch [5/5][261/690] lr: 5.6e-07, eta: 9:42:43.503987, loss: 2.1983
2023-04-14 03:58:03 - training - INFO - Epoch [5/5][271/690] lr: 5.5e-07, eta: 9:21:03.276614, loss: 1.6476
2023-04-14 03:58:11 - training - INFO - Epoch [5/5][281/690] lr: 5.4e-07, eta: 9:00:54.654348, loss: 1.2905
2023-04-14 03:58:19 - training - INFO - Epoch [5/5][291/690] lr: 5.3e-07, eta: 8:42:08.475867, loss: 2.5350
2023-04-14 03:58:27 - training - INFO - Epoch [5/5][301/690] lr: 5.1e-07, eta: 8:24:35.279548, loss: 1.7503
2023-04-14 03:58:35 - training - INFO - Epoch [5/5][311/690] lr: 5.0e-07, eta: 8:08:10.599132, loss: 2.0857
2023-04-14 03:58:43 - training - INFO - Epoch [5/5][321/690] lr: 4.9e-07, eta: 7:52:49.604172, loss: 2.5539
2023-04-14 03:58:51 - training - INFO - Epoch [5/5][331/690] lr: 4.7e-07, eta: 7:38:18.145746, loss: 1.5779
2023-04-14 03:58:59 - training - INFO - Epoch [5/5][341/690] lr: 4.6e-07, eta: 7:24:38.055408, loss: 1.7185
2023-04-14 03:59:07 - training - INFO - Epoch [5/5][351/690] lr: 4.5e-07, eta: 7:11:46.211361, loss: 0.9704
2023-04-14 03:59:16 - training - INFO - Epoch [5/5][361/690] lr: 4.3e-07, eta: 6:59:40.073081, loss: 1.7293
2023-04-14 03:59:24 - training - INFO - Epoch [5/5][371/690] lr: 4.2e-07, eta: 6:48:10.095048, loss: 1.8080
2023-04-14 03:59:32 - training - INFO - Epoch [5/5][381/690] lr: 4.1e-07, eta: 6:37:16.754205, loss: 1.6859
2023-04-14 03:59:40 - training - INFO - Epoch [5/5][391/690] lr: 3.9e-07, eta: 6:26:55.546340, loss: 1.9960
2023-04-14 03:59:48 - training - INFO - Epoch [5/5][401/690] lr: 3.8e-07, eta: 6:17:02.302469, loss: 1.6818
2023-04-14 03:59:56 - training - INFO - Epoch [5/5][411/690] lr: 3.7e-07, eta: 6:07:38.146923, loss: 1.8155
2023-04-14 04:00:04 - training - INFO - Epoch [5/5][421/690] lr: 3.5e-07, eta: 5:58:43.147126, loss: 1.7950
2023-04-14 04:00:12 - training - INFO - Epoch [5/5][431/690] lr: 3.4e-07, eta: 5:50:09.830838, loss: 2.7475
2023-04-14 04:00:21 - training - INFO - Epoch [5/5][441/690] lr: 3.3e-07, eta: 5:42:03.308769, loss: 1.4405
2023-04-14 04:00:29 - training - INFO - Epoch [5/5][451/690] lr: 3.1e-07, eta: 5:34:16.058418, loss: 1.4400
2023-04-14 04:00:37 - training - INFO - Epoch [5/5][461/690] lr: 3.0e-07, eta: 5:26:49.430148, loss: 1.9207
2023-04-14 04:00:45 - training - INFO - Epoch [5/5][471/690] lr: 2.9e-07, eta: 5:19:39.090963, loss: 1.4568
2023-04-14 04:00:54 - training - INFO - Epoch [5/5][481/690] lr: 2.8e-07, eta: 5:12:49.522177, loss: 1.7261
2023-04-14 04:01:02 - training - INFO - Epoch [5/5][491/690] lr: 2.6e-07, eta: 5:06:15.336738, loss: 1.9434
2023-04-14 04:01:10 - training - INFO - Epoch [5/5][501/690] lr: 2.5e-07, eta: 4:59:57.121812, loss: 1.9083
2023-04-14 04:01:18 - training - INFO - Epoch [5/5][511/690] lr: 2.4e-07, eta: 4:53:49.873644, loss: 1.7283
2023-04-14 04:01:27 - training - INFO - Epoch [5/5][521/690] lr: 2.2e-07, eta: 4:48:00.625502, loss: 1.7716
2023-04-14 04:01:34 - training - INFO - Epoch [5/5][531/690] lr: 2.1e-07, eta: 4:42:20.130438, loss: 1.2414
2023-04-14 04:01:43 - training - INFO - Epoch [5/5][541/690] lr: 2.0e-07, eta: 4:36:54.154246, loss: 0.8970
2023-04-14 04:01:50 - training - INFO - Epoch [5/5][551/690] lr: 1.8e-07, eta: 4:31:36.429903, loss: 1.8295
2023-04-14 04:01:58 - training - INFO - Epoch [5/5][561/690] lr: 1.7e-07, eta: 4:26:32.501517, loss: 1.7468
2023-04-14 04:02:07 - training - INFO - Epoch [5/5][571/690] lr: 1.6e-07, eta: 4:21:41.285791, loss: 1.5981
2023-04-14 04:02:15 - training - INFO - Epoch [5/5][581/690] lr: 1.4e-07, eta: 4:16:57.274405, loss: 1.9943
2023-04-14 04:02:23 - training - INFO - Epoch [5/5][591/690] lr: 1.3e-07, eta: 4:12:22.544832, loss: 2.4909
2023-04-14 04:02:31 - training - INFO - Epoch [5/5][601/690] lr: 1.2e-07, eta: 4:07:56.432417, loss: 2.0682
2023-04-14 04:02:39 - training - INFO - Epoch [5/5][611/690] lr: 1.0e-07, eta: 4:03:40.182835, loss: 2.2866
2023-04-14 04:02:47 - training - INFO - Epoch [5/5][621/690] lr: 9.1e-08, eta: 3:59:30.052608, loss: 1.6167
2023-04-14 04:02:55 - training - INFO - Epoch [5/5][631/690] lr: 7.8e-08, eta: 3:55:28.095060, loss: 2.2011
2023-04-14 04:03:03 - training - INFO - Epoch [5/5][641/690] lr: 6.4e-08, eta: 3:51:32.555570, loss: 2.1380
2023-04-14 04:03:11 - training - INFO - Epoch [5/5][651/690] lr: 5.1e-08, eta: 3:47:44.768382, loss: 1.4857
2023-04-14 04:03:19 - training - INFO - Epoch [5/5][661/690] lr: 3.8e-08, eta: 3:44:03.309102, loss: 1.7283
2023-04-14 04:03:27 - training - INFO - Epoch [5/5][671/690] lr: 2.5e-08, eta: 3:40:29.051556, loss: 1.6142
2023-04-14 04:03:35 - training - INFO - Epoch [5/5][681/690] lr: 1.2e-08, eta: 3:37:01.610160, loss: 1.3547
2023-04-14 04:05:03 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 1.8377, Validation Metrics: {'exact_match': 27.939464493597207, 'f1': 36.49658014350914}, Test Metrics: {'exact_match': 31.358885017421603, 'f1': 39.79456940924455}
2023-04-14 04:05:42 - training - INFO - Final Test - Train Loss: 1.8377, Test Metrics: {'exact_match': 31.358885017421603, 'f1': 39.79456940924455}
