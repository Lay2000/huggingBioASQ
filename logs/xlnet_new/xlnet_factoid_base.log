2023-04-14 01:22:47 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1380cc367820a3f3/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'xlnet-base-cased'}, 'data': {'task_type': 'factoid', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 4.54e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/xlnet_factoid_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 526.17it/s]
Map:   0%|          | 0/4429 [00:00<?, ? examples/s]Map:  23%|██▎       | 1000/4429 [00:00<00:02, 1234.81 examples/s]Map:  45%|████▌     | 2000/4429 [00:01<00:01, 1443.51 examples/s]Map:  68%|██████▊   | 3000/4429 [00:02<00:00, 1499.89 examples/s]Map:  90%|█████████ | 4000/4429 [00:02<00:00, 1523.31 examples/s]Map: 100%|██████████| 4429/4429 [00:02<00:00, 1510.59 examples/s]                                                                 Map:   0%|          | 0/553 [00:00<?, ? examples/s]Map: 100%|██████████| 553/553 [00:00<00:00, 1259.84 examples/s]                                                               Map:   0%|          | 0/555 [00:00<?, ? examples/s]Map: 100%|██████████| 555/555 [00:00<00:00, 1244.46 examples/s]                                                               Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForQuestionAnsweringSimple: ['lm_loss.bias', 'lm_loss.weight']
- This IS expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForQuestionAnsweringSimple were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-14 01:23:49 - training - INFO - First Test - Val Metrics:{'exact_match': 0.5424954792043399, 'f1': 5.423327858099389} Test Metrics: {'exact_match': 0.36036036036036034, 'f1': 6.186024174878664}
2023-04-14 01:23:49 - training - INFO - Epoch [1/5][1/438] lr: 4.5e-05, eta: 1 day, 6:35:58.528953, loss: 6.8915
2023-04-14 01:23:57 - training - INFO - Epoch [1/5][11/438] lr: 4.5e-05, eta: 3:11:41.400447, loss: 5.0580
2023-04-14 01:24:05 - training - INFO - Epoch [1/5][21/438] lr: 4.5e-05, eta: 1:53:33.644544, loss: 5.1275
2023-04-14 01:24:13 - training - INFO - Epoch [1/5][31/438] lr: 4.5e-05, eta: 1:25:51.155941, loss: 4.1416
2023-04-14 01:24:21 - training - INFO - Epoch [1/5][41/438] lr: 4.5e-05, eta: 1:11:20.247111, loss: 4.1845
2023-04-14 01:24:28 - training - INFO - Epoch [1/5][51/438] lr: 4.4e-05, eta: 1:02:28.195368, loss: 4.0622
2023-04-14 01:24:36 - training - INFO - Epoch [1/5][61/438] lr: 4.4e-05, eta: 0:56:27.894732, loss: 3.7059
2023-04-14 01:24:44 - training - INFO - Epoch [1/5][71/438] lr: 4.4e-05, eta: 0:52:17.200690, loss: 3.0601
2023-04-14 01:24:52 - training - INFO - Epoch [1/5][81/438] lr: 4.4e-05, eta: 0:48:54.045018, loss: 3.4054
2023-04-14 01:24:59 - training - INFO - Epoch [1/5][91/438] lr: 4.4e-05, eta: 0:46:16.110113, loss: 2.7364
2023-04-14 01:25:07 - training - INFO - Epoch [1/5][101/438] lr: 4.3e-05, eta: 0:44:11.170790, loss: 2.3940
2023-04-14 01:25:15 - training - INFO - Epoch [1/5][111/438] lr: 4.3e-05, eta: 0:42:24.045273, loss: 1.8347
2023-04-14 01:25:23 - training - INFO - Epoch [1/5][121/438] lr: 4.3e-05, eta: 0:40:54.076073, loss: 2.6069
2023-04-14 01:25:30 - training - INFO - Epoch [1/5][131/438] lr: 4.3e-05, eta: 0:39:39.180677, loss: 2.3847
2023-04-14 01:25:38 - training - INFO - Epoch [1/5][141/438] lr: 4.2e-05, eta: 0:38:33.782025, loss: 2.0108
2023-04-14 01:25:46 - training - INFO - Epoch [1/5][151/438] lr: 4.2e-05, eta: 0:37:37.313691, loss: 1.9283
2023-04-14 01:25:54 - training - INFO - Epoch [1/5][161/438] lr: 4.2e-05, eta: 0:36:44.386760, loss: 1.7263
2023-04-14 01:26:02 - training - INFO - Epoch [1/5][171/438] lr: 4.2e-05, eta: 0:35:54.997821, loss: 2.2258
2023-04-14 01:26:09 - training - INFO - Epoch [1/5][181/438] lr: 4.2e-05, eta: 0:35:11.884908, loss: 1.5275
2023-04-14 01:26:17 - training - INFO - Epoch [1/5][191/438] lr: 4.1e-05, eta: 0:34:32.951006, loss: 0.9844
2023-04-14 01:26:25 - training - INFO - Epoch [1/5][201/438] lr: 4.1e-05, eta: 0:33:56.704176, loss: 1.4768
2023-04-14 01:26:33 - training - INFO - Epoch [1/5][211/438] lr: 4.1e-05, eta: 0:33:27.744975, loss: 1.4932
2023-04-14 01:26:41 - training - INFO - Epoch [1/5][221/438] lr: 4.1e-05, eta: 0:32:59.402227, loss: 2.5753
2023-04-14 01:26:49 - training - INFO - Epoch [1/5][231/438] lr: 4.1e-05, eta: 0:32:29.385228, loss: 1.7572
2023-04-14 01:26:57 - training - INFO - Epoch [1/5][241/438] lr: 4.0e-05, eta: 0:32:00.406221, loss: 1.4819
2023-04-14 01:27:04 - training - INFO - Epoch [1/5][251/438] lr: 4.0e-05, eta: 0:31:34.370037, loss: 2.0758
2023-04-14 01:27:12 - training - INFO - Epoch [1/5][261/438] lr: 4.0e-05, eta: 0:31:12.260394, loss: 1.5776
2023-04-14 01:27:20 - training - INFO - Epoch [1/5][271/438] lr: 4.0e-05, eta: 0:30:50.111738, loss: 1.4662
2023-04-14 01:27:28 - training - INFO - Epoch [1/5][281/438] lr: 4.0e-05, eta: 0:30:26.901546, loss: 1.5522
2023-04-14 01:27:36 - training - INFO - Epoch [1/5][291/438] lr: 3.9e-05, eta: 0:30:05.479947, loss: 1.9449
2023-04-14 01:27:44 - training - INFO - Epoch [1/5][301/438] lr: 3.9e-05, eta: 0:29:45.216451, loss: 1.9731
2023-04-14 01:27:51 - training - INFO - Epoch [1/5][311/438] lr: 3.9e-05, eta: 0:29:26.149139, loss: 2.0401
2023-04-14 01:27:59 - training - INFO - Epoch [1/5][321/438] lr: 3.9e-05, eta: 0:29:08.582199, loss: 2.3963
2023-04-14 01:28:07 - training - INFO - Epoch [1/5][331/438] lr: 3.9e-05, eta: 0:28:49.461162, loss: 1.5138
2023-04-14 01:28:15 - training - INFO - Epoch [1/5][341/438] lr: 3.8e-05, eta: 0:28:31.656280, loss: 1.4501
2023-04-14 01:28:23 - training - INFO - Epoch [1/5][351/438] lr: 3.8e-05, eta: 0:28:15.212268, loss: 1.4036
2023-04-14 01:28:30 - training - INFO - Epoch [1/5][361/438] lr: 3.8e-05, eta: 0:27:57.948377, loss: 1.5163
2023-04-14 01:28:38 - training - INFO - Epoch [1/5][371/438] lr: 3.8e-05, eta: 0:27:41.791106, loss: 1.8280
2023-04-14 01:28:46 - training - INFO - Epoch [1/5][381/438] lr: 3.8e-05, eta: 0:27:25.790211, loss: 0.9945
2023-04-14 01:28:53 - training - INFO - Epoch [1/5][391/438] lr: 3.7e-05, eta: 0:27:10.241207, loss: 1.6669
2023-04-14 01:29:01 - training - INFO - Epoch [1/5][401/438] lr: 3.7e-05, eta: 0:26:55.327458, loss: 1.9692
2023-04-14 01:29:09 - training - INFO - Epoch [1/5][411/438] lr: 3.7e-05, eta: 0:26:41.631921, loss: 1.1125
2023-04-14 01:29:17 - training - INFO - Epoch [1/5][421/438] lr: 3.7e-05, eta: 0:26:28.816736, loss: 1.5752
2023-04-14 01:29:25 - training - INFO - Epoch [1/5][431/438] lr: 3.6e-05, eta: 0:26:15.624250, loss: 1.1026
2023-04-14 01:30:20 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 2.3170, Validation Metrics: {'exact_match': 73.05605786618445, 'f1': 78.52251655243974}, Test Metrics: {'exact_match': 75.49549549549549, 'f1': 80.7318706792391}
2023-04-14 01:30:21 - training - INFO - Epoch [2/5][1/438] lr: 3.6e-05, eta: 11 days, 4:26:12.679283, loss: 1.3374
2023-04-14 01:30:29 - training - INFO - Epoch [2/5][11/438] lr: 3.6e-05, eta: 1 day, 0:44:24.171446, loss: 1.3098
2023-04-14 01:30:36 - training - INFO - Epoch [2/5][21/438] lr: 3.6e-05, eta: 13:07:09.584580, loss: 1.2864
2023-04-14 01:30:44 - training - INFO - Epoch [2/5][31/438] lr: 3.6e-05, eta: 8:59:56.768709, loss: 1.7251
2023-04-14 01:30:52 - training - INFO - Epoch [2/5][41/438] lr: 3.5e-05, eta: 6:53:05.018720, loss: 0.7239
2023-04-14 01:31:00 - training - INFO - Epoch [2/5][51/438] lr: 3.5e-05, eta: 5:36:04.040706, loss: 1.5622
2023-04-14 01:31:08 - training - INFO - Epoch [2/5][61/438] lr: 3.5e-05, eta: 4:44:08.367752, loss: 1.2691
2023-04-14 01:31:16 - training - INFO - Epoch [2/5][71/438] lr: 3.5e-05, eta: 4:06:56.918909, loss: 0.6834
2023-04-14 01:31:23 - training - INFO - Epoch [2/5][81/438] lr: 3.5e-05, eta: 3:38:46.825146, loss: 0.6604
2023-04-14 01:31:31 - training - INFO - Epoch [2/5][91/438] lr: 3.4e-05, eta: 3:16:46.728070, loss: 0.7166
2023-04-14 01:31:39 - training - INFO - Epoch [2/5][101/438] lr: 3.4e-05, eta: 2:59:06.693380, loss: 1.2882
2023-04-14 01:31:46 - training - INFO - Epoch [2/5][111/438] lr: 3.4e-05, eta: 2:44:36.626298, loss: 1.0576
2023-04-14 01:31:54 - training - INFO - Epoch [2/5][121/438] lr: 3.4e-05, eta: 2:32:32.728405, loss: 0.8419
2023-04-14 01:32:02 - training - INFO - Epoch [2/5][131/438] lr: 3.4e-05, eta: 2:22:18.926257, loss: 2.1895
2023-04-14 01:32:10 - training - INFO - Epoch [2/5][141/438] lr: 3.3e-05, eta: 2:13:30.231513, loss: 1.5494
2023-04-14 01:32:18 - training - INFO - Epoch [2/5][151/438] lr: 3.3e-05, eta: 2:05:49.273121, loss: 1.0906
2023-04-14 01:32:26 - training - INFO - Epoch [2/5][161/438] lr: 3.3e-05, eta: 1:59:03.889868, loss: 1.1768
2023-04-14 01:32:34 - training - INFO - Epoch [2/5][171/438] lr: 3.3e-05, eta: 1:53:03.924798, loss: 0.9012
2023-04-14 01:32:42 - training - INFO - Epoch [2/5][181/438] lr: 3.3e-05, eta: 1:47:45.110666, loss: 0.8513
2023-04-14 01:32:50 - training - INFO - Epoch [2/5][191/438] lr: 3.2e-05, eta: 1:43:03.532687, loss: 0.7950
2023-04-14 01:32:58 - training - INFO - Epoch [2/5][201/438] lr: 3.2e-05, eta: 1:38:44.666124, loss: 0.9435
2023-04-14 01:33:06 - training - INFO - Epoch [2/5][211/438] lr: 3.2e-05, eta: 1:34:50.970720, loss: 1.3655
2023-04-14 01:33:14 - training - INFO - Epoch [2/5][221/438] lr: 3.2e-05, eta: 1:31:17.102323, loss: 1.1248
2023-04-14 01:33:22 - training - INFO - Epoch [2/5][231/438] lr: 3.2e-05, eta: 1:28:01.003635, loss: 0.8708
2023-04-14 01:33:30 - training - INFO - Epoch [2/5][241/438] lr: 3.1e-05, eta: 1:24:58.301395, loss: 0.8510
2023-04-14 01:33:37 - training - INFO - Epoch [2/5][251/438] lr: 3.1e-05, eta: 1:22:10.334080, loss: 0.9274
2023-04-14 01:33:45 - training - INFO - Epoch [2/5][261/438] lr: 3.1e-05, eta: 1:19:35.650377, loss: 1.6260
2023-04-14 01:33:53 - training - INFO - Epoch [2/5][271/438] lr: 3.1e-05, eta: 1:17:11.560232, loss: 0.9641
2023-04-14 01:34:01 - training - INFO - Epoch [2/5][281/438] lr: 3.0e-05, eta: 1:14:56.487235, loss: 0.5790
2023-04-14 01:34:09 - training - INFO - Epoch [2/5][291/438] lr: 3.0e-05, eta: 1:12:50.730804, loss: 0.9890
2023-04-14 01:34:17 - training - INFO - Epoch [2/5][301/438] lr: 3.0e-05, eta: 1:10:51.600635, loss: 0.8214
2023-04-14 01:34:24 - training - INFO - Epoch [2/5][311/438] lr: 3.0e-05, eta: 1:08:59.021741, loss: 0.6434
2023-04-14 01:34:32 - training - INFO - Epoch [2/5][321/438] lr: 3.0e-05, eta: 1:07:14.795331, loss: 1.7557
2023-04-14 01:34:40 - training - INFO - Epoch [2/5][331/438] lr: 2.9e-05, eta: 1:05:36.337691, loss: 1.4140
2023-04-14 01:34:48 - training - INFO - Epoch [2/5][341/438] lr: 2.9e-05, eta: 1:04:02.194265, loss: 1.2704
2023-04-14 01:34:55 - training - INFO - Epoch [2/5][351/438] lr: 2.9e-05, eta: 1:02:33.010971, loss: 1.8205
2023-04-14 01:35:03 - training - INFO - Epoch [2/5][361/438] lr: 2.9e-05, eta: 1:01:08.730743, loss: 1.0175
2023-04-14 01:35:11 - training - INFO - Epoch [2/5][371/438] lr: 2.9e-05, eta: 0:59:49.310827, loss: 1.1129
2023-04-14 01:35:19 - training - INFO - Epoch [2/5][381/438] lr: 2.8e-05, eta: 0:58:33.148551, loss: 0.9352
2023-04-14 01:35:27 - training - INFO - Epoch [2/5][391/438] lr: 2.8e-05, eta: 0:57:22.361314, loss: 0.6913
2023-04-14 01:35:35 - training - INFO - Epoch [2/5][401/438] lr: 2.8e-05, eta: 0:56:13.107619, loss: 1.1297
2023-04-14 01:35:43 - training - INFO - Epoch [2/5][411/438] lr: 2.8e-05, eta: 0:55:07.024017, loss: 1.1652
2023-04-14 01:35:51 - training - INFO - Epoch [2/5][421/438] lr: 2.8e-05, eta: 0:54:02.929031, loss: 0.6905
2023-04-14 01:35:59 - training - INFO - Epoch [2/5][431/438] lr: 2.7e-05, eta: 0:53:02.009892, loss: 0.6798
2023-04-14 01:36:53 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.1117, Validation Metrics: {'exact_match': 76.49186256781194, 'f1': 80.05710160743146}, Test Metrics: {'exact_match': 79.45945945945945, 'f1': 83.10625805362646}
2023-04-14 01:36:54 - training - INFO - Epoch [3/5][1/438] lr: 2.7e-05, eta: 21 days, 3:49:50.967317, loss: 0.6439
2023-04-14 01:37:02 - training - INFO - Epoch [3/5][11/438] lr: 2.7e-05, eta: 1 day, 22:22:37.015186, loss: 0.3616
2023-04-14 01:37:10 - training - INFO - Epoch [3/5][21/438] lr: 2.7e-05, eta: 1 day, 0:24:17.054172, loss: 0.4815
2023-04-14 01:37:17 - training - INFO - Epoch [3/5][31/438] lr: 2.7e-05, eta: 16:36:17.994744, loss: 0.8821
2023-04-14 01:37:25 - training - INFO - Epoch [3/5][41/438] lr: 2.6e-05, eta: 12:36:50.512553, loss: 0.5217
2023-04-14 01:37:33 - training - INFO - Epoch [3/5][51/438] lr: 2.6e-05, eta: 10:11:04.023609, loss: 1.0182
2023-04-14 01:37:41 - training - INFO - Epoch [3/5][61/438] lr: 2.6e-05, eta: 8:33:04.228662, loss: 0.6223
2023-04-14 01:37:49 - training - INFO - Epoch [3/5][71/438] lr: 2.6e-05, eta: 7:22:31.228925, loss: 0.8511
2023-04-14 01:37:56 - training - INFO - Epoch [3/5][81/438] lr: 2.6e-05, eta: 6:29:22.188093, loss: 0.9396
2023-04-14 01:38:04 - training - INFO - Epoch [3/5][91/438] lr: 2.5e-05, eta: 5:47:53.698608, loss: 0.8614
2023-04-14 01:38:12 - training - INFO - Epoch [3/5][101/438] lr: 2.5e-05, eta: 5:14:40.845758, loss: 0.6777
2023-04-14 01:38:20 - training - INFO - Epoch [3/5][111/438] lr: 2.5e-05, eta: 4:47:21.072156, loss: 0.7782
2023-04-14 01:38:27 - training - INFO - Epoch [3/5][121/438] lr: 2.5e-05, eta: 4:24:34.309395, loss: 0.8076
2023-04-14 01:38:35 - training - INFO - Epoch [3/5][131/438] lr: 2.5e-05, eta: 4:05:16.677792, loss: 0.8447
2023-04-14 01:38:43 - training - INFO - Epoch [3/5][141/438] lr: 2.4e-05, eta: 3:48:40.491261, loss: 0.7521
2023-04-14 01:38:51 - training - INFO - Epoch [3/5][151/438] lr: 2.4e-05, eta: 3:34:12.854851, loss: 0.9814
2023-04-14 01:38:59 - training - INFO - Epoch [3/5][161/438] lr: 2.4e-05, eta: 3:21:32.953624, loss: 1.2137
2023-04-14 01:39:06 - training - INFO - Epoch [3/5][171/438] lr: 2.4e-05, eta: 3:10:21.121599, loss: 0.5231
2023-04-14 01:39:14 - training - INFO - Epoch [3/5][181/438] lr: 2.3e-05, eta: 3:00:21.651274, loss: 0.9660
2023-04-14 01:39:22 - training - INFO - Epoch [3/5][191/438] lr: 2.3e-05, eta: 2:51:26.220317, loss: 0.6129
2023-04-14 01:39:30 - training - INFO - Epoch [3/5][201/438] lr: 2.3e-05, eta: 2:43:23.220102, loss: 0.7611
2023-04-14 01:39:37 - training - INFO - Epoch [3/5][211/438] lr: 2.3e-05, eta: 2:36:02.985430, loss: 0.7202
2023-04-14 01:39:45 - training - INFO - Epoch [3/5][221/438] lr: 2.3e-05, eta: 2:29:24.419882, loss: 1.0502
2023-04-14 01:39:53 - training - INFO - Epoch [3/5][231/438] lr: 2.2e-05, eta: 2:23:20.241162, loss: 1.0577
2023-04-14 01:40:01 - training - INFO - Epoch [3/5][241/438] lr: 2.2e-05, eta: 2:17:43.149963, loss: 1.5169
2023-04-14 01:40:09 - training - INFO - Epoch [3/5][251/438] lr: 2.2e-05, eta: 2:12:35.763536, loss: 0.6922
2023-04-14 01:40:17 - training - INFO - Epoch [3/5][261/438] lr: 2.2e-05, eta: 2:07:53.654592, loss: 1.3166
2023-04-14 01:40:25 - training - INFO - Epoch [3/5][271/438] lr: 2.2e-05, eta: 2:03:26.079217, loss: 0.6871
2023-04-14 01:40:33 - training - INFO - Epoch [3/5][281/438] lr: 2.1e-05, eta: 1:59:18.184936, loss: 0.4067
2023-04-14 01:40:41 - training - INFO - Epoch [3/5][291/438] lr: 2.1e-05, eta: 1:55:28.218549, loss: 0.9596
2023-04-14 01:40:49 - training - INFO - Epoch [3/5][301/438] lr: 2.1e-05, eta: 1:51:52.374489, loss: 1.2835
2023-04-14 01:40:57 - training - INFO - Epoch [3/5][311/438] lr: 2.1e-05, eta: 1:48:29.763557, loss: 0.7377
2023-04-14 01:41:04 - training - INFO - Epoch [3/5][321/438] lr: 2.1e-05, eta: 1:45:18.616143, loss: 1.3733
2023-04-14 01:41:12 - training - INFO - Epoch [3/5][331/438] lr: 2.0e-05, eta: 1:42:18.516527, loss: 0.9246
2023-04-14 01:41:20 - training - INFO - Epoch [3/5][341/438] lr: 2.0e-05, eta: 1:39:28.427778, loss: 0.6755
2023-04-14 01:41:28 - training - INFO - Epoch [3/5][351/438] lr: 2.0e-05, eta: 1:36:47.333964, loss: 0.8889
2023-04-14 01:41:35 - training - INFO - Epoch [3/5][361/438] lr: 2.0e-05, eta: 1:34:15.008282, loss: 0.6338
2023-04-14 01:41:43 - training - INFO - Epoch [3/5][371/438] lr: 2.0e-05, eta: 1:31:51.251675, loss: 0.7685
2023-04-14 01:41:51 - training - INFO - Epoch [3/5][381/438] lr: 1.9e-05, eta: 1:29:34.148256, loss: 1.1776
2023-04-14 01:41:59 - training - INFO - Epoch [3/5][391/438] lr: 1.9e-05, eta: 1:27:22.966022, loss: 0.7201
2023-04-14 01:42:07 - training - INFO - Epoch [3/5][401/438] lr: 1.9e-05, eta: 1:25:19.490061, loss: 0.9541
2023-04-14 01:42:15 - training - INFO - Epoch [3/5][411/438] lr: 1.9e-05, eta: 1:23:21.964488, loss: 0.4887
2023-04-14 01:42:22 - training - INFO - Epoch [3/5][421/438] lr: 1.9e-05, eta: 1:21:28.362612, loss: 0.5183
2023-04-14 01:42:30 - training - INFO - Epoch [3/5][431/438] lr: 1.8e-05, eta: 1:19:39.090424, loss: 0.5011
2023-04-14 01:43:25 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 0.8551, Validation Metrics: {'exact_match': 79.56600361663652, 'f1': 83.43748545892764}, Test Metrics: {'exact_match': 82.88288288288288, 'f1': 86.9814049225814}
2023-04-14 01:43:25 - training - INFO - Epoch [4/5][1/438] lr: 1.8e-05, eta: 31 days, 1:41:17.431262, loss: 0.4907
2023-04-14 01:43:34 - training - INFO - Epoch [4/5][11/438] lr: 1.8e-05, eta: 2 days, 19:55:57.345608, loss: 0.8619
2023-04-14 01:43:43 - training - INFO - Epoch [4/5][21/438] lr: 1.8e-05, eta: 1 day, 11:40:33.352716, loss: 0.7269
2023-04-14 01:43:50 - training - INFO - Epoch [4/5][31/438] lr: 1.8e-05, eta: 1 day, 0:12:33.458408, loss: 0.7751
2023-04-14 01:43:58 - training - INFO - Epoch [4/5][41/438] lr: 1.7e-05, eta: 18:19:48.369503, loss: 0.5019
2023-04-14 01:44:06 - training - INFO - Epoch [4/5][51/438] lr: 1.7e-05, eta: 14:45:38.287176, loss: 0.3698
2023-04-14 01:44:14 - training - INFO - Epoch [4/5][61/438] lr: 1.7e-05, eta: 12:21:27.913995, loss: 0.9118
2023-04-14 01:44:22 - training - INFO - Epoch [4/5][71/438] lr: 1.7e-05, eta: 10:37:53.909869, loss: 0.7528
2023-04-14 01:44:29 - training - INFO - Epoch [4/5][81/438] lr: 1.6e-05, eta: 9:19:56.055759, loss: 0.9146
2023-04-14 01:44:37 - training - INFO - Epoch [4/5][91/438] lr: 1.6e-05, eta: 8:19:08.267526, loss: 0.8492
2023-04-14 01:44:45 - training - INFO - Epoch [4/5][101/438] lr: 1.6e-05, eta: 7:30:15.626925, loss: 0.4936
2023-04-14 01:44:53 - training - INFO - Epoch [4/5][111/438] lr: 1.6e-05, eta: 6:50:07.330902, loss: 0.6026
2023-04-14 01:45:01 - training - INFO - Epoch [4/5][121/438] lr: 1.6e-05, eta: 6:16:39.275269, loss: 1.1981
2023-04-14 01:45:08 - training - INFO - Epoch [4/5][131/438] lr: 1.5e-05, eta: 5:48:12.217961, loss: 0.4855
2023-04-14 01:45:16 - training - INFO - Epoch [4/5][141/438] lr: 1.5e-05, eta: 5:23:47.575059, loss: 0.9274
2023-04-14 01:45:24 - training - INFO - Epoch [4/5][151/438] lr: 1.5e-05, eta: 5:02:43.579198, loss: 0.4701
2023-04-14 01:45:32 - training - INFO - Epoch [4/5][161/438] lr: 1.5e-05, eta: 4:44:12.937458, loss: 0.6108
2023-04-14 01:45:40 - training - INFO - Epoch [4/5][171/438] lr: 1.5e-05, eta: 4:27:49.644990, loss: 0.7492
2023-04-14 01:45:48 - training - INFO - Epoch [4/5][181/438] lr: 1.4e-05, eta: 4:13:13.757614, loss: 0.7683
2023-04-14 01:45:56 - training - INFO - Epoch [4/5][191/438] lr: 1.4e-05, eta: 4:00:07.148822, loss: 0.6080
2023-04-14 01:46:03 - training - INFO - Epoch [4/5][201/438] lr: 1.4e-05, eta: 3:48:18.970974, loss: 0.9893
2023-04-14 01:46:11 - training - INFO - Epoch [4/5][211/438] lr: 1.4e-05, eta: 3:37:35.957750, loss: 1.0375
2023-04-14 01:46:19 - training - INFO - Epoch [4/5][221/438] lr: 1.4e-05, eta: 3:27:50.265731, loss: 0.6380
2023-04-14 01:46:26 - training - INFO - Epoch [4/5][231/438] lr: 1.3e-05, eta: 3:18:55.183992, loss: 0.9663
2023-04-14 01:46:34 - training - INFO - Epoch [4/5][241/438] lr: 1.3e-05, eta: 3:10:44.225905, loss: 0.8417
2023-04-14 01:46:42 - training - INFO - Epoch [4/5][251/438] lr: 1.3e-05, eta: 3:03:11.772176, loss: 0.8552
2023-04-14 01:46:50 - training - INFO - Epoch [4/5][261/438] lr: 1.3e-05, eta: 2:56:12.374466, loss: 0.7861
2023-04-14 01:46:57 - training - INFO - Epoch [4/5][271/438] lr: 1.3e-05, eta: 2:49:43.818284, loss: 0.5583
2023-04-14 01:47:05 - training - INFO - Epoch [4/5][281/438] lr: 1.2e-05, eta: 2:43:43.427650, loss: 0.6638
2023-04-14 01:47:13 - training - INFO - Epoch [4/5][291/438] lr: 1.2e-05, eta: 2:38:09.584052, loss: 0.7029
2023-04-14 01:47:21 - training - INFO - Epoch [4/5][301/438] lr: 1.2e-05, eta: 2:32:55.248911, loss: 0.4142
2023-04-14 01:47:29 - training - INFO - Epoch [4/5][311/438] lr: 1.2e-05, eta: 2:28:00.697031, loss: 0.8373
2023-04-14 01:47:37 - training - INFO - Epoch [4/5][321/438] lr: 1.2e-05, eta: 2:23:25.958151, loss: 1.0527
2023-04-14 01:47:45 - training - INFO - Epoch [4/5][331/438] lr: 1.1e-05, eta: 2:19:04.820484, loss: 1.0638
2023-04-14 01:47:53 - training - INFO - Epoch [4/5][341/438] lr: 1.1e-05, eta: 2:15:00.223083, loss: 1.0038
2023-04-14 01:48:01 - training - INFO - Epoch [4/5][351/438] lr: 1.1e-05, eta: 2:11:08.498037, loss: 0.6852
2023-04-14 01:48:09 - training - INFO - Epoch [4/5][361/438] lr: 1.1e-05, eta: 2:07:28.208586, loss: 0.5928
2023-04-14 01:48:17 - training - INFO - Epoch [4/5][371/438] lr: 1.0e-05, eta: 2:04:00.597672, loss: 1.0881
2023-04-14 01:48:24 - training - INFO - Epoch [4/5][381/438] lr: 1.0e-05, eta: 2:00:42.505164, loss: 0.6120
2023-04-14 01:48:32 - training - INFO - Epoch [4/5][391/438] lr: 1.0e-05, eta: 1:57:33.938367, loss: 0.6518
2023-04-14 01:48:40 - training - INFO - Epoch [4/5][401/438] lr: 9.8e-06, eta: 1:54:33.919425, loss: 0.7078
2023-04-14 01:48:48 - training - INFO - Epoch [4/5][411/438] lr: 9.6e-06, eta: 1:51:44.640051, loss: 0.5320
2023-04-14 01:48:56 - training - INFO - Epoch [4/5][421/438] lr: 9.4e-06, eta: 1:49:02.243168, loss: 0.5826
2023-04-14 01:49:04 - training - INFO - Epoch [4/5][431/438] lr: 9.2e-06, eta: 1:46:27.372268, loss: 0.5094
2023-04-14 01:49:59 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 0.7171, Validation Metrics: {'exact_match': 78.6618444846293, 'f1': 81.22490643406587}, Test Metrics: {'exact_match': 81.8018018018018, 'f1': 84.96554796554797}
2023-04-14 01:50:00 - training - INFO - Epoch [5/5][1/438] lr: 9.1e-06, eta: 41 days, 1:27:13.693010, loss: 0.6866
2023-04-14 01:50:08 - training - INFO - Epoch [5/5][11/438] lr: 8.9e-06, eta: 3 days, 17:36:13.254910, loss: 0.6166
2023-04-14 01:50:15 - training - INFO - Epoch [5/5][21/438] lr: 8.6e-06, eta: 1 day, 22:56:39.513354, loss: 0.3509
2023-04-14 01:50:23 - training - INFO - Epoch [5/5][31/438] lr: 8.4e-06, eta: 1 day, 7:48:14.527043, loss: 0.3537
2023-04-14 01:50:31 - training - INFO - Epoch [5/5][41/438] lr: 8.2e-06, eta: 1 day, 0:03:17.715750, loss: 0.8918
2023-04-14 01:50:39 - training - INFO - Epoch [5/5][51/438] lr: 8.0e-06, eta: 19:20:14.931450, loss: 0.6254
2023-04-14 01:50:47 - training - INFO - Epoch [5/5][61/438] lr: 7.8e-06, eta: 16:09:59.208374, loss: 0.8216
2023-04-14 01:50:54 - training - INFO - Epoch [5/5][71/438] lr: 7.6e-06, eta: 13:53:20.167685, loss: 0.3351
2023-04-14 01:51:02 - training - INFO - Epoch [5/5][81/438] lr: 7.4e-06, eta: 12:10:21.206928, loss: 0.5197
2023-04-14 01:51:10 - training - INFO - Epoch [5/5][91/438] lr: 7.2e-06, eta: 10:50:03.943345, loss: 0.4640
2023-04-14 01:51:18 - training - INFO - Epoch [5/5][101/438] lr: 7.0e-06, eta: 9:45:35.910432, loss: 0.7163
2023-04-14 01:51:25 - training - INFO - Epoch [5/5][111/438] lr: 6.8e-06, eta: 8:52:38.354736, loss: 0.5017
2023-04-14 01:51:33 - training - INFO - Epoch [5/5][121/438] lr: 6.6e-06, eta: 8:08:25.655316, loss: 0.7673
2023-04-14 01:51:41 - training - INFO - Epoch [5/5][131/438] lr: 6.4e-06, eta: 7:31:00.518686, loss: 0.7273
2023-04-14 01:51:49 - training - INFO - Epoch [5/5][141/438] lr: 6.2e-06, eta: 6:58:51.745179, loss: 0.4654
2023-04-14 01:51:56 - training - INFO - Epoch [5/5][151/438] lr: 5.9e-06, eta: 6:30:56.884368, loss: 0.5433
2023-04-14 01:52:04 - training - INFO - Epoch [5/5][161/438] lr: 5.7e-06, eta: 6:06:29.703445, loss: 0.5967
2023-04-14 01:52:12 - training - INFO - Epoch [5/5][171/438] lr: 5.5e-06, eta: 5:44:57.265674, loss: 0.7925
2023-04-14 01:52:20 - training - INFO - Epoch [5/5][181/438] lr: 5.3e-06, eta: 5:25:41.472685, loss: 0.6864
2023-04-14 01:52:28 - training - INFO - Epoch [5/5][191/438] lr: 5.1e-06, eta: 5:08:29.752494, loss: 0.4734
2023-04-14 01:52:35 - training - INFO - Epoch [5/5][201/438] lr: 4.9e-06, eta: 4:52:56.703495, loss: 0.6555
2023-04-14 01:52:43 - training - INFO - Epoch [5/5][211/438] lr: 4.7e-06, eta: 4:38:53.155461, loss: 0.6537
2023-04-14 01:52:51 - training - INFO - Epoch [5/5][221/438] lr: 4.5e-06, eta: 4:26:06.500891, loss: 0.5037
2023-04-14 01:52:59 - training - INFO - Epoch [5/5][231/438] lr: 4.3e-06, eta: 4:14:25.162716, loss: 0.4295
2023-04-14 01:53:07 - training - INFO - Epoch [5/5][241/438] lr: 4.1e-06, eta: 4:03:41.259621, loss: 0.6520
2023-04-14 01:53:15 - training - INFO - Epoch [5/5][251/438] lr: 3.9e-06, eta: 3:53:47.003277, loss: 0.6547
2023-04-14 01:53:23 - training - INFO - Epoch [5/5][261/438] lr: 3.7e-06, eta: 3:44:37.215057, loss: 0.5983
2023-04-14 01:53:30 - training - INFO - Epoch [5/5][271/438] lr: 3.5e-06, eta: 3:36:06.512209, loss: 0.3465
2023-04-14 01:53:38 - training - INFO - Epoch [5/5][281/438] lr: 3.3e-06, eta: 3:28:15.895929, loss: 0.5301
2023-04-14 01:53:46 - training - INFO - Epoch [5/5][291/438] lr: 3.0e-06, eta: 3:20:55.567923, loss: 0.4084
2023-04-14 01:53:54 - training - INFO - Epoch [5/5][301/438] lr: 2.8e-06, eta: 3:14:01.364857, loss: 0.3853
2023-04-14 01:54:02 - training - INFO - Epoch [5/5][311/438] lr: 2.6e-06, eta: 3:07:34.787225, loss: 1.0149
2023-04-14 01:54:10 - training - INFO - Epoch [5/5][321/438] lr: 2.4e-06, eta: 3:01:33.036630, loss: 0.5090
2023-04-14 01:54:18 - training - INFO - Epoch [5/5][331/438] lr: 2.2e-06, eta: 2:55:50.905079, loss: 0.3565
2023-04-14 01:54:25 - training - INFO - Epoch [5/5][341/438] lr: 2.0e-06, eta: 2:50:27.662144, loss: 0.5503
2023-04-14 01:54:33 - training - INFO - Epoch [5/5][351/438] lr: 1.8e-06, eta: 2:45:23.435256, loss: 0.8561
2023-04-14 01:54:41 - training - INFO - Epoch [5/5][361/438] lr: 1.6e-06, eta: 2:40:37.311930, loss: 0.5152
2023-04-14 01:54:49 - training - INFO - Epoch [5/5][371/438] lr: 1.4e-06, eta: 2:36:04.288398, loss: 0.7296
2023-04-14 01:54:57 - training - INFO - Epoch [5/5][381/438] lr: 1.2e-06, eta: 2:31:44.859810, loss: 0.4369
2023-04-14 01:55:04 - training - INFO - Epoch [5/5][391/438] lr: 9.7e-07, eta: 2:27:38.193246, loss: 0.7321
2023-04-14 01:55:12 - training - INFO - Epoch [5/5][401/438] lr: 7.7e-07, eta: 2:23:43.888812, loss: 0.3717
2023-04-14 01:55:20 - training - INFO - Epoch [5/5][411/438] lr: 5.6e-07, eta: 2:20:00.251205, loss: 0.5259
2023-04-14 01:55:28 - training - INFO - Epoch [5/5][421/438] lr: 3.5e-07, eta: 2:16:27.285800, loss: 0.3399
2023-04-14 01:55:35 - training - INFO - Epoch [5/5][431/438] lr: 1.5e-07, eta: 2:13:04.370127, loss: 0.6824
2023-04-14 01:56:30 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 0.6307, Validation Metrics: {'exact_match': 79.20433996383363, 'f1': 81.83986290026155}, Test Metrics: {'exact_match': 82.34234234234235, 'f1': 85.20334620334621}
2023-04-14 01:56:54 - training - INFO - Final Test - Train Loss: 0.6307, Test Metrics: {'exact_match': 82.34234234234235, 'f1': 85.20334620334621}
