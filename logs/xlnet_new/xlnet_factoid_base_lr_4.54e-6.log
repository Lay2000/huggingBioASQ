2023-04-14 02:31:51 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1380cc367820a3f3/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'xlnet-base-cased'}, 'data': {'task_type': 'factoid', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 4.54e-06, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/xlnet_factoid_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 591.94it/s]
Map:   0%|          | 0/4429 [00:00<?, ? examples/s]Map:  23%|██▎       | 1000/4429 [00:00<00:02, 1230.11 examples/s]Map:  45%|████▌     | 2000/4429 [00:01<00:01, 1430.81 examples/s]Map:  68%|██████▊   | 3000/4429 [00:02<00:00, 1492.58 examples/s]Map:  90%|█████████ | 4000/4429 [00:02<00:00, 1518.37 examples/s]Map: 100%|██████████| 4429/4429 [00:02<00:00, 1514.68 examples/s]                                                                 Map:   0%|          | 0/553 [00:00<?, ? examples/s]Map: 100%|██████████| 553/553 [00:00<00:00, 1239.64 examples/s]                                                               Map:   0%|          | 0/555 [00:00<?, ? examples/s]Map: 100%|██████████| 555/555 [00:00<00:00, 1247.29 examples/s]                                                               Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForQuestionAnsweringSimple: ['lm_loss.bias', 'lm_loss.weight']
- This IS expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForQuestionAnsweringSimple were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-14 02:32:51 - training - INFO - First Test - Val Metrics:{'exact_match': 0.5424954792043399, 'f1': 5.423327858099389} Test Metrics: {'exact_match': 0.36036036036036034, 'f1': 6.186024174878664}
2023-04-14 02:32:52 - training - INFO - Epoch [1/5][1/438] lr: 4.5e-06, eta: 1 day, 6:14:25.449440, loss: 6.8915
2023-04-14 02:33:00 - training - INFO - Epoch [1/5][11/438] lr: 4.5e-06, eta: 3:09:35.545765, loss: 6.4430
2023-04-14 02:33:07 - training - INFO - Epoch [1/5][21/438] lr: 4.5e-06, eta: 1:51:51.534531, loss: 6.0032
2023-04-14 02:33:15 - training - INFO - Epoch [1/5][31/438] lr: 4.5e-06, eta: 1:24:11.341053, loss: 5.3876
2023-04-14 02:33:23 - training - INFO - Epoch [1/5][41/438] lr: 4.5e-06, eta: 1:10:10.492720, loss: 5.3706
2023-04-14 02:33:30 - training - INFO - Epoch [1/5][51/438] lr: 4.4e-06, eta: 1:01:27.811398, loss: 5.4345
2023-04-14 02:33:38 - training - INFO - Epoch [1/5][61/438] lr: 4.4e-06, eta: 0:55:37.799362, loss: 5.5825
2023-04-14 02:33:46 - training - INFO - Epoch [1/5][71/438] lr: 4.4e-06, eta: 0:51:25.463186, loss: 4.7715
2023-04-14 02:33:53 - training - INFO - Epoch [1/5][81/438] lr: 4.4e-06, eta: 0:48:08.275500, loss: 4.5921
2023-04-14 02:34:01 - training - INFO - Epoch [1/5][91/438] lr: 4.4e-06, eta: 0:45:36.409627, loss: 4.3530
2023-04-14 02:34:09 - training - INFO - Epoch [1/5][101/438] lr: 4.3e-06, eta: 0:43:33.086231, loss: 4.7465
2023-04-14 02:34:16 - training - INFO - Epoch [1/5][111/438] lr: 4.3e-06, eta: 0:41:49.404975, loss: 4.2644
2023-04-14 02:34:24 - training - INFO - Epoch [1/5][121/438] lr: 4.3e-06, eta: 0:40:21.630015, loss: 3.5135
2023-04-14 02:34:32 - training - INFO - Epoch [1/5][131/438] lr: 4.3e-06, eta: 0:39:07.192053, loss: 4.7409
2023-04-14 02:34:39 - training - INFO - Epoch [1/5][141/438] lr: 4.2e-06, eta: 0:38:01.375041, loss: 4.3432
2023-04-14 02:34:47 - training - INFO - Epoch [1/5][151/438] lr: 4.2e-06, eta: 0:37:07.876648, loss: 4.4720
2023-04-14 02:34:55 - training - INFO - Epoch [1/5][161/438] lr: 4.2e-06, eta: 0:36:18.107152, loss: 3.7387
2023-04-14 02:35:03 - training - INFO - Epoch [1/5][171/438] lr: 4.2e-06, eta: 0:35:32.059962, loss: 3.5025
2023-04-14 02:35:11 - training - INFO - Epoch [1/5][181/438] lr: 4.2e-06, eta: 0:34:52.696949, loss: 3.9933
2023-04-14 02:35:19 - training - INFO - Epoch [1/5][191/438] lr: 4.1e-06, eta: 0:34:14.772100, loss: 3.7542
2023-04-14 02:35:27 - training - INFO - Epoch [1/5][201/438] lr: 4.1e-06, eta: 0:33:41.004999, loss: 3.8058
2023-04-14 02:35:35 - training - INFO - Epoch [1/5][211/438] lr: 4.1e-06, eta: 0:33:10.058652, loss: 3.6792
2023-04-14 02:35:42 - training - INFO - Epoch [1/5][221/438] lr: 4.1e-06, eta: 0:32:40.348214, loss: 4.0805
2023-04-14 02:35:50 - training - INFO - Epoch [1/5][231/438] lr: 4.1e-06, eta: 0:32:12.970767, loss: 3.2469
2023-04-14 02:35:58 - training - INFO - Epoch [1/5][241/438] lr: 4.0e-06, eta: 0:31:46.862620, loss: 4.2338
2023-04-14 02:36:06 - training - INFO - Epoch [1/5][251/438] lr: 4.0e-06, eta: 0:31:22.852377, loss: 4.2501
2023-04-14 02:36:14 - training - INFO - Epoch [1/5][261/438] lr: 4.0e-06, eta: 0:30:59.808699, loss: 4.0179
2023-04-14 02:36:22 - training - INFO - Epoch [1/5][271/438] lr: 4.0e-06, eta: 0:30:38.144854, loss: 4.0442
2023-04-14 02:36:30 - training - INFO - Epoch [1/5][281/438] lr: 4.0e-06, eta: 0:30:15.877071, loss: 3.9695
2023-04-14 02:36:38 - training - INFO - Epoch [1/5][291/438] lr: 3.9e-06, eta: 0:29:56.761638, loss: 4.3832
2023-04-14 02:36:45 - training - INFO - Epoch [1/5][301/438] lr: 3.9e-06, eta: 0:29:36.281481, loss: 3.9603
2023-04-14 02:36:53 - training - INFO - Epoch [1/5][311/438] lr: 3.9e-06, eta: 0:29:17.470038, loss: 3.8779
2023-04-14 02:37:01 - training - INFO - Epoch [1/5][321/438] lr: 3.9e-06, eta: 0:28:57.960672, loss: 3.3375
2023-04-14 02:37:09 - training - INFO - Epoch [1/5][331/438] lr: 3.9e-06, eta: 0:28:40.482192, loss: 3.4989
2023-04-14 02:37:16 - training - INFO - Epoch [1/5][341/438] lr: 3.8e-06, eta: 0:28:22.777382, loss: 3.4688
2023-04-14 02:37:24 - training - INFO - Epoch [1/5][351/438] lr: 3.8e-06, eta: 0:28:05.916123, loss: 3.1573
2023-04-14 02:37:32 - training - INFO - Epoch [1/5][361/438] lr: 3.8e-06, eta: 0:27:49.534977, loss: 3.8500
2023-04-14 02:37:40 - training - INFO - Epoch [1/5][371/438] lr: 3.8e-06, eta: 0:27:33.903922, loss: 3.8384
2023-04-14 02:37:48 - training - INFO - Epoch [1/5][381/438] lr: 3.8e-06, eta: 0:27:19.894680, loss: 3.1937
2023-04-14 02:37:56 - training - INFO - Epoch [1/5][391/438] lr: 3.7e-06, eta: 0:27:04.788438, loss: 3.4565
2023-04-14 02:38:04 - training - INFO - Epoch [1/5][401/438] lr: 3.7e-06, eta: 0:26:52.490104, loss: 3.0496
2023-04-14 02:38:12 - training - INFO - Epoch [1/5][411/438] lr: 3.7e-06, eta: 0:26:38.193114, loss: 2.5955
2023-04-14 02:38:19 - training - INFO - Epoch [1/5][421/438] lr: 3.7e-06, eta: 0:26:24.052819, loss: 3.2496
2023-04-14 02:38:27 - training - INFO - Epoch [1/5][431/438] lr: 3.6e-06, eta: 0:26:09.594398, loss: 2.7010
2023-04-14 02:39:21 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 4.2045, Validation Metrics: {'exact_match': 43.037974683544306, 'f1': 51.276199794503995}, Test Metrics: {'exact_match': 47.027027027027025, 'f1': 55.70632415369257}
2023-04-14 02:39:22 - training - INFO - Epoch [2/5][1/438] lr: 3.6e-06, eta: 11 days, 3:14:16.592959, loss: 3.2197
2023-04-14 02:39:30 - training - INFO - Epoch [2/5][11/438] lr: 3.6e-06, eta: 1 day, 0:36:31.701055, loss: 2.9628
2023-04-14 02:39:37 - training - INFO - Epoch [2/5][21/438] lr: 3.6e-06, eta: 13:02:48.660387, loss: 2.7739
2023-04-14 02:39:45 - training - INFO - Epoch [2/5][31/438] lr: 3.6e-06, eta: 8:57:00.112852, loss: 3.2500
2023-04-14 02:39:53 - training - INFO - Epoch [2/5][41/438] lr: 3.5e-06, eta: 6:51:13.296508, loss: 2.4252
2023-04-14 02:40:01 - training - INFO - Epoch [2/5][51/438] lr: 3.5e-06, eta: 5:34:38.003709, loss: 3.0031
2023-04-14 02:40:09 - training - INFO - Epoch [2/5][61/438] lr: 3.5e-06, eta: 4:43:01.329800, loss: 2.5701
2023-04-14 02:40:17 - training - INFO - Epoch [2/5][71/438] lr: 3.5e-06, eta: 4:05:56.355770, loss: 2.8211
2023-04-14 02:40:24 - training - INFO - Epoch [2/5][81/438] lr: 3.5e-06, eta: 3:37:50.430486, loss: 1.8936
2023-04-14 02:40:32 - training - INFO - Epoch [2/5][91/438] lr: 3.4e-06, eta: 3:15:59.580332, loss: 1.7987
2023-04-14 02:40:40 - training - INFO - Epoch [2/5][101/438] lr: 3.4e-06, eta: 2:58:32.519429, loss: 3.0195
2023-04-14 02:40:48 - training - INFO - Epoch [2/5][111/438] lr: 3.4e-06, eta: 2:44:04.875810, loss: 2.5611
2023-04-14 02:40:56 - training - INFO - Epoch [2/5][121/438] lr: 3.4e-06, eta: 2:32:04.697593, loss: 2.2491
2023-04-14 02:41:04 - training - INFO - Epoch [2/5][131/438] lr: 3.4e-06, eta: 2:21:48.352166, loss: 3.2220
2023-04-14 02:41:12 - training - INFO - Epoch [2/5][141/438] lr: 3.3e-06, eta: 2:13:00.637806, loss: 3.4613
2023-04-14 02:41:19 - training - INFO - Epoch [2/5][151/438] lr: 3.3e-06, eta: 2:05:19.187676, loss: 2.3605
2023-04-14 02:41:27 - training - INFO - Epoch [2/5][161/438] lr: 3.3e-06, eta: 1:58:33.426462, loss: 2.6924
2023-04-14 02:41:35 - training - INFO - Epoch [2/5][171/438] lr: 3.3e-06, eta: 1:52:39.373758, loss: 2.3158
2023-04-14 02:41:44 - training - INFO - Epoch [2/5][181/438] lr: 3.3e-06, eta: 1:47:30.219958, loss: 2.1155
2023-04-14 02:41:51 - training - INFO - Epoch [2/5][191/438] lr: 3.2e-06, eta: 1:42:44.832042, loss: 2.5996
2023-04-14 02:41:59 - training - INFO - Epoch [2/5][201/438] lr: 3.2e-06, eta: 1:38:26.458818, loss: 2.2628
2023-04-14 02:42:07 - training - INFO - Epoch [2/5][211/438] lr: 3.2e-06, eta: 1:34:31.871391, loss: 2.5673
2023-04-14 02:42:15 - training - INFO - Epoch [2/5][221/438] lr: 3.2e-06, eta: 1:30:56.736956, loss: 2.4007
2023-04-14 02:42:23 - training - INFO - Epoch [2/5][231/438] lr: 3.2e-06, eta: 1:27:41.168760, loss: 2.0698
2023-04-14 02:42:30 - training - INFO - Epoch [2/5][241/438] lr: 3.1e-06, eta: 1:24:39.016040, loss: 1.7770
2023-04-14 02:42:39 - training - INFO - Epoch [2/5][251/438] lr: 3.1e-06, eta: 1:21:53.794410, loss: 2.5605
2023-04-14 02:42:46 - training - INFO - Epoch [2/5][261/438] lr: 3.1e-06, eta: 1:19:18.443697, loss: 2.8680
2023-04-14 02:42:54 - training - INFO - Epoch [2/5][271/438] lr: 3.1e-06, eta: 1:16:56.144905, loss: 2.2372
2023-04-14 02:43:02 - training - INFO - Epoch [2/5][281/438] lr: 3.0e-06, eta: 1:14:43.021149, loss: 1.7447
2023-04-14 02:43:10 - training - INFO - Epoch [2/5][291/438] lr: 3.0e-06, eta: 1:12:38.130939, loss: 2.6343
2023-04-14 02:43:18 - training - INFO - Epoch [2/5][301/438] lr: 3.0e-06, eta: 1:10:41.356588, loss: 1.5631
2023-04-14 02:43:26 - training - INFO - Epoch [2/5][311/438] lr: 3.0e-06, eta: 1:08:51.347905, loss: 1.7931
2023-04-14 02:43:34 - training - INFO - Epoch [2/5][321/438] lr: 3.0e-06, eta: 1:07:06.405390, loss: 3.3503
2023-04-14 02:43:42 - training - INFO - Epoch [2/5][331/438] lr: 2.9e-06, eta: 1:05:29.351569, loss: 2.6196
2023-04-14 02:43:50 - training - INFO - Epoch [2/5][341/438] lr: 2.9e-06, eta: 1:03:56.160978, loss: 2.5791
2023-04-14 02:43:58 - training - INFO - Epoch [2/5][351/438] lr: 2.9e-06, eta: 1:02:28.393242, loss: 2.4593
2023-04-14 02:44:06 - training - INFO - Epoch [2/5][361/438] lr: 2.9e-06, eta: 1:01:03.479684, loss: 2.0445
2023-04-14 02:44:13 - training - INFO - Epoch [2/5][371/438] lr: 2.9e-06, eta: 0:59:43.302670, loss: 2.2843
2023-04-14 02:44:21 - training - INFO - Epoch [2/5][381/438] lr: 2.8e-06, eta: 0:58:27.075738, loss: 1.7827
2023-04-14 02:44:29 - training - INFO - Epoch [2/5][391/438] lr: 2.8e-06, eta: 0:57:13.897019, loss: 1.8933
2023-04-14 02:44:37 - training - INFO - Epoch [2/5][401/438] lr: 2.8e-06, eta: 0:56:04.461382, loss: 2.2284
2023-04-14 02:44:45 - training - INFO - Epoch [2/5][411/438] lr: 2.8e-06, eta: 0:54:58.691181, loss: 2.5266
2023-04-14 02:44:53 - training - INFO - Epoch [2/5][421/438] lr: 2.8e-06, eta: 0:53:56.038776, loss: 2.1636
2023-04-14 02:45:00 - training - INFO - Epoch [2/5][431/438] lr: 2.7e-06, eta: 0:52:54.894737, loss: 1.7915
2023-04-14 02:45:55 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 2.3681, Validation Metrics: {'exact_match': 54.611211573236886, 'f1': 63.336490523989475}, Test Metrics: {'exact_match': 57.11711711711712, 'f1': 67.66487172802962}
2023-04-14 02:45:55 - training - INFO - Epoch [3/5][1/438] lr: 2.7e-06, eta: 21 days, 2:31:05.608041, loss: 1.4446
2023-04-14 02:46:03 - training - INFO - Epoch [3/5][11/438] lr: 2.7e-06, eta: 1 day, 22:15:22.644610, loss: 2.0068
2023-04-14 02:46:11 - training - INFO - Epoch [3/5][21/438] lr: 2.7e-06, eta: 1 day, 0:20:40.791858, loss: 2.1832
2023-04-14 02:46:19 - training - INFO - Epoch [3/5][31/438] lr: 2.7e-06, eta: 16:33:41.657236, loss: 2.5862
2023-04-14 02:46:26 - training - INFO - Epoch [3/5][41/438] lr: 2.6e-06, eta: 12:34:26.718665, loss: 1.6235
2023-04-14 02:46:34 - training - INFO - Epoch [3/5][51/438] lr: 2.6e-06, eta: 10:09:04.506984, loss: 2.1753
2023-04-14 02:46:42 - training - INFO - Epoch [3/5][61/438] lr: 2.6e-06, eta: 8:31:26.869492, loss: 1.2646
2023-04-14 02:46:49 - training - INFO - Epoch [3/5][71/438] lr: 2.6e-06, eta: 7:21:09.119794, loss: 1.9704
2023-04-14 02:46:57 - training - INFO - Epoch [3/5][81/438] lr: 2.6e-06, eta: 6:28:12.302160, loss: 1.6898
2023-04-14 02:47:05 - training - INFO - Epoch [3/5][91/438] lr: 2.5e-06, eta: 5:46:50.938508, loss: 1.6974
2023-04-14 02:47:12 - training - INFO - Epoch [3/5][101/438] lr: 2.5e-06, eta: 5:13:40.204177, loss: 1.7762
2023-04-14 02:47:20 - training - INFO - Epoch [3/5][111/438] lr: 2.5e-06, eta: 4:46:26.022315, loss: 2.8559
2023-04-14 02:47:28 - training - INFO - Epoch [3/5][121/438] lr: 2.5e-06, eta: 4:23:39.029853, loss: 1.8119
2023-04-14 02:47:35 - training - INFO - Epoch [3/5][131/438] lr: 2.5e-06, eta: 4:04:21.865153, loss: 1.4213
2023-04-14 02:47:43 - training - INFO - Epoch [3/5][141/438] lr: 2.4e-06, eta: 3:47:50.610405, loss: 1.6892
2023-04-14 02:47:51 - training - INFO - Epoch [3/5][151/438] lr: 2.4e-06, eta: 3:33:27.674689, loss: 2.1836
2023-04-14 02:47:59 - training - INFO - Epoch [3/5][161/438] lr: 2.4e-06, eta: 3:20:52.704351, loss: 3.5078
2023-04-14 02:48:07 - training - INFO - Epoch [3/5][171/438] lr: 2.4e-06, eta: 3:09:44.123424, loss: 1.3071
2023-04-14 02:48:15 - training - INFO - Epoch [3/5][181/438] lr: 2.3e-06, eta: 2:59:54.602098, loss: 1.6339
2023-04-14 02:48:23 - training - INFO - Epoch [3/5][191/438] lr: 2.3e-06, eta: 2:50:58.592138, loss: 1.5698
2023-04-14 02:48:30 - training - INFO - Epoch [3/5][201/438] lr: 2.3e-06, eta: 2:42:55.121499, loss: 1.2730
2023-04-14 02:48:38 - training - INFO - Epoch [3/5][211/438] lr: 2.3e-06, eta: 2:35:37.299989, loss: 1.8241
2023-04-14 02:48:46 - training - INFO - Epoch [3/5][221/438] lr: 2.3e-06, eta: 2:28:58.740184, loss: 2.2706
2023-04-14 02:48:53 - training - INFO - Epoch [3/5][231/438] lr: 2.2e-06, eta: 2:22:53.195208, loss: 2.0305
2023-04-14 02:49:01 - training - INFO - Epoch [3/5][241/438] lr: 2.2e-06, eta: 2:17:19.023292, loss: 2.5305
2023-04-14 02:49:09 - training - INFO - Epoch [3/5][251/438] lr: 2.2e-06, eta: 2:12:10.387843, loss: 2.3196
2023-04-14 02:49:17 - training - INFO - Epoch [3/5][261/438] lr: 2.2e-06, eta: 2:07:24.231555, loss: 2.7716
2023-04-14 02:49:24 - training - INFO - Epoch [3/5][271/438] lr: 2.2e-06, eta: 2:02:58.125144, loss: 1.6779
2023-04-14 02:49:32 - training - INFO - Epoch [3/5][281/438] lr: 2.1e-06, eta: 1:58:51.584930, loss: 1.3092
2023-04-14 02:49:40 - training - INFO - Epoch [3/5][291/438] lr: 2.1e-06, eta: 1:55:00.533028, loss: 1.3920
2023-04-14 02:49:48 - training - INFO - Epoch [3/5][301/438] lr: 2.1e-06, eta: 1:51:25.773591, loss: 2.3421
2023-04-14 02:49:56 - training - INFO - Epoch [3/5][311/438] lr: 2.1e-06, eta: 1:48:03.705585, loss: 1.0219
2023-04-14 02:50:04 - training - INFO - Epoch [3/5][321/438] lr: 2.1e-06, eta: 1:44:54.560244, loss: 2.1593
2023-04-14 02:50:11 - training - INFO - Epoch [3/5][331/438] lr: 2.0e-06, eta: 1:41:56.372119, loss: 1.5414
2023-04-14 02:50:20 - training - INFO - Epoch [3/5][341/438] lr: 2.0e-06, eta: 1:39:08.698948, loss: 1.0663
2023-04-14 02:50:28 - training - INFO - Epoch [3/5][351/438] lr: 2.0e-06, eta: 1:36:30.137475, loss: 1.9967
2023-04-14 02:50:35 - training - INFO - Epoch [3/5][361/438] lr: 2.0e-06, eta: 1:33:59.127075, loss: 1.3181
2023-04-14 02:50:43 - training - INFO - Epoch [3/5][371/438] lr: 2.0e-06, eta: 1:31:34.953435, loss: 1.7857
2023-04-14 02:50:51 - training - INFO - Epoch [3/5][381/438] lr: 1.9e-06, eta: 1:29:17.952279, loss: 1.8536
2023-04-14 02:50:59 - training - INFO - Epoch [3/5][391/438] lr: 1.9e-06, eta: 1:27:07.852623, loss: 1.4491
2023-04-14 02:51:07 - training - INFO - Epoch [3/5][401/438] lr: 1.9e-06, eta: 1:25:04.516131, loss: 1.8799
2023-04-14 02:51:14 - training - INFO - Epoch [3/5][411/438] lr: 1.9e-06, eta: 1:23:06.113598, loss: 0.8717
2023-04-14 02:51:23 - training - INFO - Epoch [3/5][421/438] lr: 1.9e-06, eta: 1:21:14.573257, loss: 1.2137
2023-04-14 02:51:30 - training - INFO - Epoch [3/5][431/438] lr: 1.8e-06, eta: 1:19:25.973561, loss: 1.3060
2023-04-14 02:52:24 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 1.8744, Validation Metrics: {'exact_match': 62.0253164556962, 'f1': 70.02255343833983}, Test Metrics: {'exact_match': 63.06306306306306, 'f1': 72.09473215789006}
2023-04-14 02:52:25 - training - INFO - Epoch [4/5][1/438] lr: 1.8e-06, eta: 30 days, 23:21:45.309625, loss: 1.4060
2023-04-14 02:52:33 - training - INFO - Epoch [4/5][11/438] lr: 1.8e-06, eta: 2 days, 19:41:18.526581, loss: 2.0480
2023-04-14 02:52:40 - training - INFO - Epoch [4/5][21/438] lr: 1.8e-06, eta: 1 day, 11:30:46.024389, loss: 1.2066
2023-04-14 02:52:48 - training - INFO - Epoch [4/5][31/438] lr: 1.8e-06, eta: 1 day, 0:05:42.412875, loss: 1.5104
2023-04-14 02:52:56 - training - INFO - Epoch [4/5][41/438] lr: 1.7e-06, eta: 18:14:50.589020, loss: 1.2624
2023-04-14 02:53:03 - training - INFO - Epoch [4/5][51/438] lr: 1.7e-06, eta: 14:41:29.222016, loss: 1.2746
2023-04-14 02:53:11 - training - INFO - Epoch [4/5][61/438] lr: 1.7e-06, eta: 12:18:04.238952, loss: 1.6292
2023-04-14 02:53:19 - training - INFO - Epoch [4/5][71/438] lr: 1.7e-06, eta: 10:34:58.090082, loss: 2.2445
2023-04-14 02:53:27 - training - INFO - Epoch [4/5][81/438] lr: 1.6e-06, eta: 9:17:22.893852, loss: 1.6109
2023-04-14 02:53:35 - training - INFO - Epoch [4/5][91/438] lr: 1.6e-06, eta: 8:16:45.506140, loss: 1.9018
2023-04-14 02:53:42 - training - INFO - Epoch [4/5][101/438] lr: 1.6e-06, eta: 7:28:07.838617, loss: 1.6286
2023-04-14 02:53:50 - training - INFO - Epoch [4/5][111/438] lr: 1.6e-06, eta: 6:48:17.617914, loss: 2.0242
2023-04-14 02:53:58 - training - INFO - Epoch [4/5][121/438] lr: 1.6e-06, eta: 6:14:55.361813, loss: 1.7858
2023-04-14 02:54:06 - training - INFO - Epoch [4/5][131/438] lr: 1.5e-06, eta: 5:46:40.991907, loss: 1.7235
2023-04-14 02:54:14 - training - INFO - Epoch [4/5][141/438] lr: 1.5e-06, eta: 5:22:26.887488, loss: 2.2777
2023-04-14 02:54:22 - training - INFO - Epoch [4/5][151/438] lr: 1.5e-06, eta: 5:01:25.469186, loss: 1.5401
2023-04-14 02:54:30 - training - INFO - Epoch [4/5][161/438] lr: 1.5e-06, eta: 4:42:57.178656, loss: 2.0157
2023-04-14 02:54:37 - training - INFO - Epoch [4/5][171/438] lr: 1.5e-06, eta: 4:26:36.086763, loss: 1.4811
2023-04-14 02:54:45 - training - INFO - Epoch [4/5][181/438] lr: 1.4e-06, eta: 4:12:03.299975, loss: 1.1732
2023-04-14 02:54:53 - training - INFO - Epoch [4/5][191/438] lr: 1.4e-06, eta: 3:58:59.856485, loss: 1.4866
2023-04-14 02:55:00 - training - INFO - Epoch [4/5][201/438] lr: 1.4e-06, eta: 3:47:14.378199, loss: 1.4754
2023-04-14 02:55:08 - training - INFO - Epoch [4/5][211/438] lr: 1.4e-06, eta: 3:36:35.620019, loss: 2.0438
2023-04-14 02:55:16 - training - INFO - Epoch [4/5][221/438] lr: 1.4e-06, eta: 3:26:54.883668, loss: 1.6689
2023-04-14 02:55:24 - training - INFO - Epoch [4/5][231/438] lr: 1.3e-06, eta: 3:18:02.866938, loss: 2.0275
2023-04-14 02:55:31 - training - INFO - Epoch [4/5][241/438] lr: 1.3e-06, eta: 3:09:54.247698, loss: 1.6226
2023-04-14 02:55:39 - training - INFO - Epoch [4/5][251/438] lr: 1.3e-06, eta: 3:02:25.600708, loss: 2.3285
2023-04-14 02:55:47 - training - INFO - Epoch [4/5][261/438] lr: 1.3e-06, eta: 2:55:28.833078, loss: 1.8868
2023-04-14 02:55:55 - training - INFO - Epoch [4/5][271/438] lr: 1.3e-06, eta: 2:49:03.615234, loss: 2.4109
2023-04-14 02:56:03 - training - INFO - Epoch [4/5][281/438] lr: 1.2e-06, eta: 2:43:04.417235, loss: 1.4227
2023-04-14 02:56:10 - training - INFO - Epoch [4/5][291/438] lr: 1.2e-06, eta: 2:37:29.507556, loss: 1.6210
2023-04-14 02:56:18 - training - INFO - Epoch [4/5][301/438] lr: 1.2e-06, eta: 2:32:16.441295, loss: 1.0327
2023-04-14 02:56:26 - training - INFO - Epoch [4/5][311/438] lr: 1.2e-06, eta: 2:27:23.564233, loss: 1.5913
2023-04-14 02:56:34 - training - INFO - Epoch [4/5][321/438] lr: 1.2e-06, eta: 2:22:49.054746, loss: 1.6108
2023-04-14 02:56:42 - training - INFO - Epoch [4/5][331/438] lr: 1.1e-06, eta: 2:18:31.754451, loss: 2.1561
2023-04-14 02:56:50 - training - INFO - Epoch [4/5][341/438] lr: 1.1e-06, eta: 2:14:27.431068, loss: 1.4494
2023-04-14 02:56:58 - training - INFO - Epoch [4/5][351/438] lr: 1.1e-06, eta: 2:10:36.091179, loss: 1.7213
2023-04-14 02:57:06 - training - INFO - Epoch [4/5][361/438] lr: 1.1e-06, eta: 2:06:57.640509, loss: 1.4422
2023-04-14 02:57:14 - training - INFO - Epoch [4/5][371/438] lr: 1.0e-06, eta: 2:03:30.049386, loss: 1.5489
2023-04-14 02:57:22 - training - INFO - Epoch [4/5][381/438] lr: 1.0e-06, eta: 2:00:12.618675, loss: 1.4635
2023-04-14 02:57:29 - training - INFO - Epoch [4/5][391/438] lr: 1.0e-06, eta: 1:57:05.298287, loss: 1.5796
2023-04-14 02:57:37 - training - INFO - Epoch [4/5][401/438] lr: 9.8e-07, eta: 1:54:07.842961, loss: 1.8151
2023-04-14 02:57:45 - training - INFO - Epoch [4/5][411/438] lr: 9.6e-07, eta: 1:51:16.761342, loss: 1.5332
2023-04-14 02:57:53 - training - INFO - Epoch [4/5][421/438] lr: 9.4e-07, eta: 1:48:33.928554, loss: 1.6495
2023-04-14 02:58:01 - training - INFO - Epoch [4/5][431/438] lr: 9.2e-07, eta: 1:46:00.097214, loss: 2.0574
2023-04-14 02:58:55 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 1.6488, Validation Metrics: {'exact_match': 63.11030741410488, 'f1': 71.77379596715922}, Test Metrics: {'exact_match': 64.68468468468468, 'f1': 73.60416466732256}
2023-04-14 02:58:56 - training - INFO - Epoch [5/5][1/438] lr: 9.1e-07, eta: 40 days, 21:17:00.769238, loss: 1.4530
2023-04-14 02:59:04 - training - INFO - Epoch [5/5][11/438] lr: 8.9e-07, eta: 3 days, 17:13:47.970816, loss: 1.9078
2023-04-14 02:59:12 - training - INFO - Epoch [5/5][21/438] lr: 8.6e-07, eta: 1 day, 22:45:17.213193, loss: 1.6883
2023-04-14 02:59:20 - training - INFO - Epoch [5/5][31/438] lr: 8.4e-07, eta: 1 day, 7:40:21.820470, loss: 1.1941
2023-04-14 02:59:27 - training - INFO - Epoch [5/5][41/438] lr: 8.2e-07, eta: 23:56:50.588443, loss: 1.3849
2023-04-14 02:59:35 - training - INFO - Epoch [5/5][51/438] lr: 8.0e-07, eta: 19:15:00.680265, loss: 1.4762
2023-04-14 02:59:42 - training - INFO - Epoch [5/5][61/438] lr: 7.8e-07, eta: 16:05:37.926849, loss: 1.2102
2023-04-14 02:59:50 - training - INFO - Epoch [5/5][71/438] lr: 7.6e-07, eta: 13:49:38.130389, loss: 1.6656
2023-04-14 02:59:58 - training - INFO - Epoch [5/5][81/438] lr: 7.4e-07, eta: 12:07:16.182249, loss: 1.3699
2023-04-14 03:00:06 - training - INFO - Epoch [5/5][91/438] lr: 7.2e-07, eta: 10:47:16.726510, loss: 1.4074
2023-04-14 03:00:14 - training - INFO - Epoch [5/5][101/438] lr: 7.0e-07, eta: 9:43:03.227511, loss: 1.3091
2023-04-14 03:00:22 - training - INFO - Epoch [5/5][111/438] lr: 6.8e-07, eta: 8:50:29.874615, loss: 2.5182
2023-04-14 03:00:30 - training - INFO - Epoch [5/5][121/438] lr: 6.6e-07, eta: 8:06:35.762450, loss: 1.9907
2023-04-14 03:00:38 - training - INFO - Epoch [5/5][131/438] lr: 6.4e-07, eta: 7:29:21.624916, loss: 1.8311
2023-04-14 03:00:46 - training - INFO - Epoch [5/5][141/438] lr: 6.2e-07, eta: 6:57:21.945705, loss: 1.4539
2023-04-14 03:00:53 - training - INFO - Epoch [5/5][151/438] lr: 5.9e-07, eta: 6:29:33.275173, loss: 1.5935
2023-04-14 03:01:01 - training - INFO - Epoch [5/5][161/438] lr: 5.7e-07, eta: 6:05:16.105528, loss: 1.1212
2023-04-14 03:01:09 - training - INFO - Epoch [5/5][171/438] lr: 5.5e-07, eta: 5:43:44.214216, loss: 1.6193
2023-04-14 03:01:17 - training - INFO - Epoch [5/5][181/438] lr: 5.3e-07, eta: 5:24:35.069208, loss: 1.6511
2023-04-14 03:01:25 - training - INFO - Epoch [5/5][191/438] lr: 5.1e-07, eta: 5:07:24.403185, loss: 2.0164
2023-04-14 03:01:32 - training - INFO - Epoch [5/5][201/438] lr: 4.9e-07, eta: 4:51:55.408482, loss: 1.2612
2023-04-14 03:01:40 - training - INFO - Epoch [5/5][211/438] lr: 4.7e-07, eta: 4:37:53.646931, loss: 1.8979
2023-04-14 03:01:48 - training - INFO - Epoch [5/5][221/438] lr: 4.5e-07, eta: 4:25:09.173456, loss: 1.0908
2023-04-14 03:01:56 - training - INFO - Epoch [5/5][231/438] lr: 4.3e-07, eta: 4:13:28.183242, loss: 1.1855
2023-04-14 03:02:03 - training - INFO - Epoch [5/5][241/438] lr: 4.1e-07, eta: 4:02:44.808785, loss: 1.5960
2023-04-14 03:02:12 - training - INFO - Epoch [5/5][251/438] lr: 3.9e-07, eta: 3:52:55.739995, loss: 0.9178
2023-04-14 03:02:20 - training - INFO - Epoch [5/5][261/438] lr: 3.7e-07, eta: 3:43:50.784027, loss: 1.3672
2023-04-14 03:02:28 - training - INFO - Epoch [5/5][271/438] lr: 3.5e-07, eta: 3:35:25.161597, loss: 1.1739
2023-04-14 03:02:35 - training - INFO - Epoch [5/5][281/438] lr: 3.3e-07, eta: 3:27:32.897613, loss: 1.2592
2023-04-14 03:02:43 - training - INFO - Epoch [5/5][291/438] lr: 3.0e-07, eta: 3:20:12.170076, loss: 1.2700
2023-04-14 03:02:51 - training - INFO - Epoch [5/5][301/438] lr: 2.8e-07, eta: 3:13:19.497061, loss: 1.3792
2023-04-14 03:02:59 - training - INFO - Epoch [5/5][311/438] lr: 2.6e-07, eta: 3:06:54.747614, loss: 1.4953
2023-04-14 03:03:07 - training - INFO - Epoch [5/5][321/438] lr: 2.4e-07, eta: 3:00:53.454948, loss: 1.4371
2023-04-14 03:03:14 - training - INFO - Epoch [5/5][331/438] lr: 2.2e-07, eta: 2:55:13.053980, loss: 1.4159
2023-04-14 03:03:22 - training - INFO - Epoch [5/5][341/438] lr: 2.0e-07, eta: 2:49:52.741930, loss: 1.4411
2023-04-14 03:03:30 - training - INFO - Epoch [5/5][351/438] lr: 1.8e-07, eta: 2:44:50.184297, loss: 1.8192
2023-04-14 03:03:38 - training - INFO - Epoch [5/5][361/438] lr: 1.6e-07, eta: 2:40:03.266924, loss: 1.0344
2023-04-14 03:03:46 - training - INFO - Epoch [5/5][371/438] lr: 1.4e-07, eta: 2:35:32.719653, loss: 1.6424
2023-04-14 03:03:54 - training - INFO - Epoch [5/5][381/438] lr: 1.2e-07, eta: 2:31:14.846691, loss: 1.6288
2023-04-14 03:04:02 - training - INFO - Epoch [5/5][391/438] lr: 9.7e-08, eta: 2:27:10.441872, loss: 2.4536
2023-04-14 03:04:10 - training - INFO - Epoch [5/5][401/438] lr: 7.7e-08, eta: 2:23:17.617347, loss: 1.0213
2023-04-14 03:04:17 - training - INFO - Epoch [5/5][411/438] lr: 5.6e-08, eta: 2:19:34.651395, loss: 1.9875
2023-04-14 03:04:25 - training - INFO - Epoch [5/5][421/438] lr: 3.5e-08, eta: 2:16:03.172561, loss: 0.7614
2023-04-14 03:04:33 - training - INFO - Epoch [5/5][431/438] lr: 1.5e-08, eta: 2:12:40.752034, loss: 1.5778
2023-04-14 03:05:28 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 1.5517, Validation Metrics: {'exact_match': 64.01446654611212, 'f1': 72.16086218605184}, Test Metrics: {'exact_match': 67.38738738738739, 'f1': 75.65684592000383}
2023-04-14 03:05:53 - training - INFO - Final Test - Train Loss: 1.5517, Test Metrics: {'exact_match': 67.38738738738739, 'f1': 75.65684592000383}
