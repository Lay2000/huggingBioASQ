2023-04-14 02:17:00 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-44a167365c0b341b/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'xlnet-base-cased'}, 'data': {'task_type': 'list', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 1e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/xlnet_list_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 334.07it/s]
Map:   0%|          | 0/6878 [00:00<?, ? examples/s]Map:  15%|█▍        | 1000/6878 [00:00<00:04, 1354.09 examples/s]Map:  29%|██▉       | 2000/6878 [00:01<00:03, 1425.67 examples/s]Map:  44%|████▎     | 3000/6878 [00:02<00:02, 1434.86 examples/s]Map:  58%|█████▊    | 4000/6878 [00:02<00:02, 1437.12 examples/s]Map:  73%|███████▎  | 5000/6878 [00:03<00:01, 1450.01 examples/s]Map:  87%|████████▋ | 6000/6878 [00:04<00:00, 1455.01 examples/s]Map: 100%|██████████| 6878/6878 [00:04<00:00, 1355.14 examples/s]                                                                 Map:   0%|          | 0/859 [00:00<?, ? examples/s]Map: 100%|██████████| 859/859 [00:00<00:00, 1170.74 examples/s]                                                               Map:   0%|          | 0/861 [00:00<?, ? examples/s]Map: 100%|██████████| 861/861 [00:00<00:00, 1252.34 examples/s]                                                               Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForQuestionAnsweringSimple: ['lm_loss.weight', 'lm_loss.bias']
- This IS expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForQuestionAnsweringSimple were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-14 02:18:30 - training - INFO - First Test - Val Metrics:{'exact_match': 0.11641443538998836, 'f1': 3.5108483369545644} Test Metrics: {'exact_match': 0.8130081300813008, 'f1': 4.255093738648391}
2023-04-14 02:18:31 - training - INFO - Epoch [1/5][1/690] lr: 1.0e-05, eta: 3 days, 2:03:30.090157, loss: 7.2861
2023-04-14 02:18:39 - training - INFO - Epoch [1/5][11/690] lr: 1.0e-05, eta: 7:21:45.576650, loss: 5.9646
2023-04-14 02:18:46 - training - INFO - Epoch [1/5][21/690] lr: 9.9e-06, eta: 4:11:42.444141, loss: 5.1912
2023-04-14 02:18:54 - training - INFO - Epoch [1/5][31/690] lr: 9.9e-06, eta: 3:03:42.756849, loss: 4.9343
2023-04-14 02:19:02 - training - INFO - Epoch [1/5][41/690] lr: 9.9e-06, eta: 2:29:16.090710, loss: 5.1635
2023-04-14 02:19:09 - training - INFO - Epoch [1/5][51/690] lr: 9.9e-06, eta: 2:07:58.738683, loss: 4.7283
2023-04-14 02:19:17 - training - INFO - Epoch [1/5][61/690] lr: 9.8e-06, eta: 1:53:42.304397, loss: 4.3855
2023-04-14 02:19:24 - training - INFO - Epoch [1/5][71/690] lr: 9.8e-06, eta: 1:43:22.374135, loss: 4.5933
2023-04-14 02:19:32 - training - INFO - Epoch [1/5][81/690] lr: 9.8e-06, eta: 1:35:35.382231, loss: 4.1315
2023-04-14 02:19:39 - training - INFO - Epoch [1/5][91/690] lr: 9.7e-06, eta: 1:29:28.830778, loss: 4.8126
2023-04-14 02:19:47 - training - INFO - Epoch [1/5][101/690] lr: 9.7e-06, eta: 1:24:36.635234, loss: 4.1670
2023-04-14 02:19:55 - training - INFO - Epoch [1/5][111/690] lr: 9.7e-06, eta: 1:20:36.955536, loss: 4.0388
2023-04-14 02:20:02 - training - INFO - Epoch [1/5][121/690] lr: 9.6e-06, eta: 1:17:16.571278, loss: 4.3566
2023-04-14 02:20:10 - training - INFO - Epoch [1/5][131/690] lr: 9.6e-06, eta: 1:14:23.865817, loss: 3.0950
2023-04-14 02:20:18 - training - INFO - Epoch [1/5][141/690] lr: 9.6e-06, eta: 1:11:51.309336, loss: 3.8079
2023-04-14 02:20:25 - training - INFO - Epoch [1/5][151/690] lr: 9.6e-06, eta: 1:09:39.542688, loss: 3.3311
2023-04-14 02:20:33 - training - INFO - Epoch [1/5][161/690] lr: 9.5e-06, eta: 1:07:48.377885, loss: 4.2706
2023-04-14 02:20:41 - training - INFO - Epoch [1/5][171/690] lr: 9.5e-06, eta: 1:06:05.517672, loss: 3.6607
2023-04-14 02:20:48 - training - INFO - Epoch [1/5][181/690] lr: 9.5e-06, eta: 1:04:32.794107, loss: 3.4514
2023-04-14 02:20:56 - training - INFO - Epoch [1/5][191/690] lr: 9.4e-06, eta: 1:03:11.621629, loss: 3.4231
2023-04-14 02:21:04 - training - INFO - Epoch [1/5][201/690] lr: 9.4e-06, eta: 1:01:57.674748, loss: 3.4786
2023-04-14 02:21:11 - training - INFO - Epoch [1/5][211/690] lr: 9.4e-06, eta: 1:00:45.737425, loss: 2.7862
2023-04-14 02:21:19 - training - INFO - Epoch [1/5][221/690] lr: 9.4e-06, eta: 0:59:40.951313, loss: 2.9265
2023-04-14 02:21:27 - training - INFO - Epoch [1/5][231/690] lr: 9.3e-06, eta: 0:58:42.426159, loss: 3.6618
2023-04-14 02:21:34 - training - INFO - Epoch [1/5][241/690] lr: 9.3e-06, eta: 0:57:45.758508, loss: 3.3305
2023-04-14 02:21:42 - training - INFO - Epoch [1/5][251/690] lr: 9.3e-06, eta: 0:56:53.822447, loss: 2.7297
2023-04-14 02:21:49 - training - INFO - Epoch [1/5][261/690] lr: 9.2e-06, eta: 0:56:04.764924, loss: 2.4287
2023-04-14 02:21:57 - training - INFO - Epoch [1/5][271/690] lr: 9.2e-06, eta: 0:55:19.597633, loss: 2.0903
2023-04-14 02:22:05 - training - INFO - Epoch [1/5][281/690] lr: 9.2e-06, eta: 0:54:38.425570, loss: 3.0247
2023-04-14 02:22:12 - training - INFO - Epoch [1/5][291/690] lr: 9.2e-06, eta: 0:54:00.315819, loss: 2.6411
2023-04-14 02:22:20 - training - INFO - Epoch [1/5][301/690] lr: 9.1e-06, eta: 0:53:21.783538, loss: 2.4200
2023-04-14 02:22:28 - training - INFO - Epoch [1/5][311/690] lr: 9.1e-06, eta: 0:52:46.199435, loss: 2.4729
2023-04-14 02:22:35 - training - INFO - Epoch [1/5][321/690] lr: 9.1e-06, eta: 0:52:11.731617, loss: 3.1440
2023-04-14 02:22:43 - training - INFO - Epoch [1/5][331/690] lr: 9.0e-06, eta: 0:51:38.539360, loss: 2.6308
2023-04-14 02:22:50 - training - INFO - Epoch [1/5][341/690] lr: 9.0e-06, eta: 0:51:07.432670, loss: 2.2104
2023-04-14 02:22:58 - training - INFO - Epoch [1/5][351/690] lr: 9.0e-06, eta: 0:50:37.853631, loss: 1.9184
2023-04-14 02:23:05 - training - INFO - Epoch [1/5][361/690] lr: 9.0e-06, eta: 0:50:08.188671, loss: 2.8479
2023-04-14 02:23:13 - training - INFO - Epoch [1/5][371/690] lr: 8.9e-06, eta: 0:49:40.281102, loss: 3.3709
2023-04-14 02:23:21 - training - INFO - Epoch [1/5][381/690] lr: 8.9e-06, eta: 0:49:13.728360, loss: 2.9648
2023-04-14 02:23:28 - training - INFO - Epoch [1/5][391/690] lr: 8.9e-06, eta: 0:48:49.910200, loss: 2.2797
2023-04-14 02:23:36 - training - INFO - Epoch [1/5][401/690] lr: 8.8e-06, eta: 0:48:27.151373, loss: 2.7933
2023-04-14 02:23:44 - training - INFO - Epoch [1/5][411/690] lr: 8.8e-06, eta: 0:48:04.333134, loss: 2.5943
2023-04-14 02:23:51 - training - INFO - Epoch [1/5][421/690] lr: 8.8e-06, eta: 0:47:40.548223, loss: 2.2200
2023-04-14 02:23:59 - training - INFO - Epoch [1/5][431/690] lr: 8.8e-06, eta: 0:47:18.231337, loss: 2.7556
2023-04-14 02:24:07 - training - INFO - Epoch [1/5][441/690] lr: 8.7e-06, eta: 0:46:56.815170, loss: 3.8441
2023-04-14 02:24:14 - training - INFO - Epoch [1/5][451/690] lr: 8.7e-06, eta: 0:46:36.399556, loss: 3.1355
2023-04-14 02:24:22 - training - INFO - Epoch [1/5][461/690] lr: 8.7e-06, eta: 0:46:16.347595, loss: 2.7387
2023-04-14 02:24:30 - training - INFO - Epoch [1/5][471/690] lr: 8.6e-06, eta: 0:45:56.197611, loss: 2.5435
2023-04-14 02:24:37 - training - INFO - Epoch [1/5][481/690] lr: 8.6e-06, eta: 0:45:37.073596, loss: 3.1655
2023-04-14 02:24:45 - training - INFO - Epoch [1/5][491/690] lr: 8.6e-06, eta: 0:45:18.486562, loss: 3.1357
2023-04-14 02:24:53 - training - INFO - Epoch [1/5][501/690] lr: 8.5e-06, eta: 0:45:00.915375, loss: 2.6501
2023-04-14 02:25:00 - training - INFO - Epoch [1/5][511/690] lr: 8.5e-06, eta: 0:44:43.571510, loss: 2.1142
2023-04-14 02:25:08 - training - INFO - Epoch [1/5][521/690] lr: 8.5e-06, eta: 0:44:26.133966, loss: 1.8616
2023-04-14 02:25:16 - training - INFO - Epoch [1/5][531/690] lr: 8.5e-06, eta: 0:44:08.525460, loss: 2.4327
2023-04-14 02:25:23 - training - INFO - Epoch [1/5][541/690] lr: 8.4e-06, eta: 0:43:51.530853, loss: 2.6790
2023-04-14 02:25:31 - training - INFO - Epoch [1/5][551/690] lr: 8.4e-06, eta: 0:43:34.889303, loss: 2.6478
2023-04-14 02:25:39 - training - INFO - Epoch [1/5][561/690] lr: 8.4e-06, eta: 0:43:19.611759, loss: 2.6236
2023-04-14 02:25:46 - training - INFO - Epoch [1/5][571/690] lr: 8.3e-06, eta: 0:43:03.303668, loss: 1.7620
2023-04-14 02:25:54 - training - INFO - Epoch [1/5][581/690] lr: 8.3e-06, eta: 0:42:48.440691, loss: 2.4193
2023-04-14 02:26:02 - training - INFO - Epoch [1/5][591/690] lr: 8.3e-06, eta: 0:42:33.204219, loss: 2.4160
2023-04-14 02:26:09 - training - INFO - Epoch [1/5][601/690] lr: 8.3e-06, eta: 0:42:18.701165, loss: 1.8457
2023-04-14 02:26:17 - training - INFO - Epoch [1/5][611/690] lr: 8.2e-06, eta: 0:42:03.252098, loss: 2.5132
2023-04-14 02:26:25 - training - INFO - Epoch [1/5][621/690] lr: 8.2e-06, eta: 0:41:51.040203, loss: 1.9554
2023-04-14 02:26:33 - training - INFO - Epoch [1/5][631/690] lr: 8.2e-06, eta: 0:41:36.117378, loss: 2.2409
2023-04-14 02:26:40 - training - INFO - Epoch [1/5][641/690] lr: 8.1e-06, eta: 0:41:21.400375, loss: 2.9727
2023-04-14 02:26:48 - training - INFO - Epoch [1/5][651/690] lr: 8.1e-06, eta: 0:41:07.402470, loss: 2.5863
2023-04-14 02:26:56 - training - INFO - Epoch [1/5][661/690] lr: 8.1e-06, eta: 0:40:55.655931, loss: 2.7609
2023-04-14 02:27:04 - training - INFO - Epoch [1/5][671/690] lr: 8.1e-06, eta: 0:40:43.088375, loss: 1.5566
2023-04-14 02:27:11 - training - INFO - Epoch [1/5][681/690] lr: 8.0e-06, eta: 0:40:29.429223, loss: 2.5123
2023-04-14 02:28:34 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 3.0173, Validation Metrics: {'exact_match': 26.30966239813737, 'f1': 34.35105876950019}, Test Metrics: {'exact_match': 29.849012775842045, 'f1': 38.060216259642374}
2023-04-14 02:28:35 - training - INFO - Epoch [2/5][1/690] lr: 8.0e-06, eta: 27 days, 4:12:46.469435, loss: 1.2493
2023-04-14 02:28:42 - training - INFO - Epoch [2/5][11/690] lr: 8.0e-06, eta: 2 days, 11:46:26.122505, loss: 2.0239
2023-04-14 02:28:50 - training - INFO - Epoch [2/5][21/690] lr: 7.9e-06, eta: 1 day, 7:33:59.048820, loss: 2.2635
2023-04-14 02:28:57 - training - INFO - Epoch [2/5][31/690] lr: 7.9e-06, eta: 21:33:27.125401, loss: 0.9455
2023-04-14 02:29:05 - training - INFO - Epoch [2/5][41/690] lr: 7.9e-06, eta: 16:25:41.193314, loss: 2.5685
2023-04-14 02:29:13 - training - INFO - Epoch [2/5][51/690] lr: 7.9e-06, eta: 13:18:35.835561, loss: 2.5259
2023-04-14 02:29:20 - training - INFO - Epoch [2/5][61/690] lr: 7.8e-06, eta: 11:12:50.242460, loss: 1.5375
2023-04-14 02:29:28 - training - INFO - Epoch [2/5][71/690] lr: 7.8e-06, eta: 9:42:31.173076, loss: 2.4331
2023-04-14 02:29:36 - training - INFO - Epoch [2/5][81/690] lr: 7.8e-06, eta: 8:34:19.164060, loss: 2.9881
2023-04-14 02:29:43 - training - INFO - Epoch [2/5][91/690] lr: 7.7e-06, eta: 7:41:01.328051, loss: 2.0664
2023-04-14 02:29:51 - training - INFO - Epoch [2/5][101/690] lr: 7.7e-06, eta: 6:58:17.821276, loss: 1.7517
2023-04-14 02:29:58 - training - INFO - Epoch [2/5][111/690] lr: 7.7e-06, eta: 6:23:18.310776, loss: 1.8708
2023-04-14 02:30:06 - training - INFO - Epoch [2/5][121/690] lr: 7.6e-06, eta: 5:54:02.382290, loss: 1.7995
2023-04-14 02:30:13 - training - INFO - Epoch [2/5][131/690] lr: 7.6e-06, eta: 5:29:13.011905, loss: 2.5792
2023-04-14 02:30:21 - training - INFO - Epoch [2/5][141/690] lr: 7.6e-06, eta: 5:07:56.632059, loss: 2.1433
2023-04-14 02:30:29 - training - INFO - Epoch [2/5][151/690] lr: 7.6e-06, eta: 4:49:29.591292, loss: 1.5317
2023-04-14 02:30:37 - training - INFO - Epoch [2/5][161/690] lr: 7.5e-06, eta: 4:33:23.946702, loss: 2.3905
2023-04-14 02:30:44 - training - INFO - Epoch [2/5][171/690] lr: 7.5e-06, eta: 4:19:02.955129, loss: 1.9259
2023-04-14 02:30:52 - training - INFO - Epoch [2/5][181/690] lr: 7.5e-06, eta: 4:06:15.363498, loss: 1.2685
2023-04-14 02:31:00 - training - INFO - Epoch [2/5][191/690] lr: 7.4e-06, eta: 3:54:49.465232, loss: 1.8917
2023-04-14 02:31:07 - training - INFO - Epoch [2/5][201/690] lr: 7.4e-06, eta: 3:44:33.382068, loss: 1.2039
2023-04-14 02:31:15 - training - INFO - Epoch [2/5][211/690] lr: 7.4e-06, eta: 3:35:12.837086, loss: 2.2103
2023-04-14 02:31:23 - training - INFO - Epoch [2/5][221/690] lr: 7.4e-06, eta: 3:26:43.250945, loss: 2.0927
2023-04-14 02:31:31 - training - INFO - Epoch [2/5][231/690] lr: 7.3e-06, eta: 3:18:58.817121, loss: 1.9410
2023-04-14 02:31:38 - training - INFO - Epoch [2/5][241/690] lr: 7.3e-06, eta: 3:11:52.463995, loss: 1.9733
2023-04-14 02:31:46 - training - INFO - Epoch [2/5][251/690] lr: 7.3e-06, eta: 3:05:15.866006, loss: 2.0741
2023-04-14 02:31:54 - training - INFO - Epoch [2/5][261/690] lr: 7.2e-06, eta: 2:59:10.932195, loss: 1.8499
2023-04-14 02:32:01 - training - INFO - Epoch [2/5][271/690] lr: 7.2e-06, eta: 2:53:30.055128, loss: 1.7041
2023-04-14 02:32:09 - training - INFO - Epoch [2/5][281/690] lr: 7.2e-06, eta: 2:48:12.675566, loss: 1.8282
2023-04-14 02:32:16 - training - INFO - Epoch [2/5][291/690] lr: 7.2e-06, eta: 2:43:17.291010, loss: 1.7336
2023-04-14 02:32:24 - training - INFO - Epoch [2/5][301/690] lr: 7.1e-06, eta: 2:38:43.892282, loss: 1.7056
2023-04-14 02:32:32 - training - INFO - Epoch [2/5][311/690] lr: 7.1e-06, eta: 2:34:28.751308, loss: 1.4405
2023-04-14 02:32:40 - training - INFO - Epoch [2/5][321/690] lr: 7.1e-06, eta: 2:30:24.868314, loss: 2.2936
2023-04-14 02:32:47 - training - INFO - Epoch [2/5][331/690] lr: 7.0e-06, eta: 2:26:36.325441, loss: 1.5641
2023-04-14 02:32:55 - training - INFO - Epoch [2/5][341/690] lr: 7.0e-06, eta: 2:23:00.662787, loss: 2.3469
2023-04-14 02:33:03 - training - INFO - Epoch [2/5][351/690] lr: 7.0e-06, eta: 2:19:40.935600, loss: 1.5610
2023-04-14 02:33:11 - training - INFO - Epoch [2/5][361/690] lr: 7.0e-06, eta: 2:16:27.935075, loss: 2.2687
2023-04-14 02:33:18 - training - INFO - Epoch [2/5][371/690] lr: 6.9e-06, eta: 2:13:24.079109, loss: 1.5889
2023-04-14 02:33:26 - training - INFO - Epoch [2/5][381/690] lr: 6.9e-06, eta: 2:10:30.458361, loss: 1.7864
2023-04-14 02:33:34 - training - INFO - Epoch [2/5][391/690] lr: 6.9e-06, eta: 2:07:45.480802, loss: 1.6361
2023-04-14 02:33:41 - training - INFO - Epoch [2/5][401/690] lr: 6.8e-06, eta: 2:05:06.543481, loss: 1.5239
2023-04-14 02:33:49 - training - INFO - Epoch [2/5][411/690] lr: 6.8e-06, eta: 2:02:36.422208, loss: 2.3751
2023-04-14 02:33:56 - training - INFO - Epoch [2/5][421/690] lr: 6.8e-06, eta: 2:00:12.288291, loss: 1.3599
2023-04-14 02:34:04 - training - INFO - Epoch [2/5][431/690] lr: 6.8e-06, eta: 1:57:56.780539, loss: 1.5696
2023-04-14 02:34:12 - training - INFO - Epoch [2/5][441/690] lr: 6.7e-06, eta: 1:55:45.274503, loss: 1.7824
2023-04-14 02:34:19 - training - INFO - Epoch [2/5][451/690] lr: 6.7e-06, eta: 1:53:38.793311, loss: 1.2072
2023-04-14 02:34:27 - training - INFO - Epoch [2/5][461/690] lr: 6.7e-06, eta: 1:51:39.194887, loss: 2.0212
2023-04-14 02:34:35 - training - INFO - Epoch [2/5][471/690] lr: 6.6e-06, eta: 1:49:43.497651, loss: 2.2940
2023-04-14 02:34:42 - training - INFO - Epoch [2/5][481/690] lr: 6.6e-06, eta: 1:47:51.357098, loss: 2.3768
2023-04-14 02:34:50 - training - INFO - Epoch [2/5][491/690] lr: 6.6e-06, eta: 1:46:03.974562, loss: 1.5135
2023-04-14 02:34:57 - training - INFO - Epoch [2/5][501/690] lr: 6.5e-06, eta: 1:44:20.004495, loss: 1.8147
2023-04-14 02:35:05 - training - INFO - Epoch [2/5][511/690] lr: 6.5e-06, eta: 1:42:40.387937, loss: 2.2367
2023-04-14 02:35:13 - training - INFO - Epoch [2/5][521/690] lr: 6.5e-06, eta: 1:41:04.520861, loss: 2.3430
2023-04-14 02:35:20 - training - INFO - Epoch [2/5][531/690] lr: 6.5e-06, eta: 1:39:31.261107, loss: 2.6610
2023-04-14 02:35:28 - training - INFO - Epoch [2/5][541/690] lr: 6.4e-06, eta: 1:38:02.009636, loss: 2.0416
2023-04-14 02:35:35 - training - INFO - Epoch [2/5][551/690] lr: 6.4e-06, eta: 1:36:35.295233, loss: 1.4954
2023-04-14 02:35:43 - training - INFO - Epoch [2/5][561/690] lr: 6.4e-06, eta: 1:35:12.075909, loss: 1.7341
2023-04-14 02:35:51 - training - INFO - Epoch [2/5][571/690] lr: 6.3e-06, eta: 1:33:51.136865, loss: 1.4505
2023-04-14 02:35:58 - training - INFO - Epoch [2/5][581/690] lr: 6.3e-06, eta: 1:32:33.245007, loss: 1.5127
2023-04-14 02:36:06 - training - INFO - Epoch [2/5][591/690] lr: 6.3e-06, eta: 1:31:17.046339, loss: 2.3595
2023-04-14 02:36:14 - training - INFO - Epoch [2/5][601/690] lr: 6.3e-06, eta: 1:30:03.097161, loss: 1.8448
2023-04-14 02:36:21 - training - INFO - Epoch [2/5][611/690] lr: 6.2e-06, eta: 1:28:51.094073, loss: 1.9734
2023-04-14 02:36:29 - training - INFO - Epoch [2/5][621/690] lr: 6.2e-06, eta: 1:27:41.674074, loss: 1.8892
2023-04-14 02:36:37 - training - INFO - Epoch [2/5][631/690] lr: 6.2e-06, eta: 1:26:34.853200, loss: 2.4477
2023-04-14 02:36:44 - training - INFO - Epoch [2/5][641/690] lr: 6.1e-06, eta: 1:25:30.034565, loss: 1.6571
2023-04-14 02:36:52 - training - INFO - Epoch [2/5][651/690] lr: 6.1e-06, eta: 1:24:26.100432, loss: 1.8578
2023-04-14 02:37:00 - training - INFO - Epoch [2/5][661/690] lr: 6.1e-06, eta: 1:23:24.163250, loss: 1.9129
2023-04-14 02:37:07 - training - INFO - Epoch [2/5][671/690] lr: 6.1e-06, eta: 1:22:22.921151, loss: 2.2176
2023-04-14 02:37:15 - training - INFO - Epoch [2/5][681/690] lr: 6.0e-06, eta: 1:21:24.156030, loss: 1.9037
2023-04-14 02:38:38 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.9333, Validation Metrics: {'exact_match': 29.68568102444703, 'f1': 37.79559459039521}, Test Metrics: {'exact_match': 30.197444831591174, 'f1': 38.67803833114348}
2023-04-14 02:38:38 - training - INFO - Epoch [3/5][1/690] lr: 6.0e-06, eta: 51 days, 6:36:14.399915, loss: 1.5398
2023-04-14 02:38:46 - training - INFO - Epoch [3/5][11/690] lr: 6.0e-06, eta: 4 days, 16:12:34.249373, loss: 1.8512
2023-04-14 02:38:54 - training - INFO - Epoch [3/5][21/690] lr: 5.9e-06, eta: 2 days, 10:57:14.381982, loss: 2.1303
2023-04-14 02:39:01 - training - INFO - Epoch [3/5][31/690] lr: 5.9e-06, eta: 1 day, 16:03:03.855907, loss: 1.7326
2023-04-14 02:39:09 - training - INFO - Epoch [3/5][41/690] lr: 5.9e-06, eta: 1 day, 6:22:29.545298, loss: 1.6687
2023-04-14 02:39:16 - training - INFO - Epoch [3/5][51/690] lr: 5.9e-06, eta: 1 day, 0:29:05.658579, loss: 1.7045
2023-04-14 02:39:24 - training - INFO - Epoch [3/5][61/690] lr: 5.8e-06, eta: 20:31:50.470548, loss: 1.8322
2023-04-14 02:39:32 - training - INFO - Epoch [3/5][71/690] lr: 5.8e-06, eta: 17:41:21.522677, loss: 1.5858
2023-04-14 02:39:40 - training - INFO - Epoch [3/5][81/690] lr: 5.8e-06, eta: 15:32:59.651007, loss: 1.8653
2023-04-14 02:39:48 - training - INFO - Epoch [3/5][91/690] lr: 5.7e-06, eta: 13:52:52.121797, loss: 1.7260
2023-04-14 02:39:55 - training - INFO - Epoch [3/5][101/690] lr: 5.7e-06, eta: 12:32:23.307662, loss: 1.6940
2023-04-14 02:40:03 - training - INFO - Epoch [3/5][111/690] lr: 5.7e-06, eta: 11:26:21.072345, loss: 1.1651
2023-04-14 02:40:11 - training - INFO - Epoch [3/5][121/690] lr: 5.6e-06, eta: 10:31:16.286733, loss: 1.1503
2023-04-14 02:40:18 - training - INFO - Epoch [3/5][131/690] lr: 5.6e-06, eta: 9:44:34.893290, loss: 2.0265
2023-04-14 02:40:26 - training - INFO - Epoch [3/5][141/690] lr: 5.6e-06, eta: 9:04:27.212379, loss: 2.0443
2023-04-14 02:40:34 - training - INFO - Epoch [3/5][151/690] lr: 5.6e-06, eta: 8:29:41.086695, loss: 2.1728
2023-04-14 02:40:41 - training - INFO - Epoch [3/5][161/690] lr: 5.5e-06, eta: 7:59:08.658939, loss: 1.8039
2023-04-14 02:40:49 - training - INFO - Epoch [3/5][171/690] lr: 5.5e-06, eta: 7:32:10.373862, loss: 2.3859
2023-04-14 02:40:56 - training - INFO - Epoch [3/5][181/690] lr: 5.5e-06, eta: 7:08:12.153039, loss: 1.3874
2023-04-14 02:41:04 - training - INFO - Epoch [3/5][191/690] lr: 5.4e-06, eta: 6:46:42.358897, loss: 1.5074
2023-04-14 02:41:12 - training - INFO - Epoch [3/5][201/690] lr: 5.4e-06, eta: 6:27:22.455774, loss: 1.4431
2023-04-14 02:41:19 - training - INFO - Epoch [3/5][211/690] lr: 5.4e-06, eta: 6:09:48.669091, loss: 1.3906
2023-04-14 02:41:27 - training - INFO - Epoch [3/5][221/690] lr: 5.4e-06, eta: 5:53:52.622087, loss: 2.9229
2023-04-14 02:41:35 - training - INFO - Epoch [3/5][231/690] lr: 5.3e-06, eta: 5:39:15.913044, loss: 1.4564
2023-04-14 02:41:42 - training - INFO - Epoch [3/5][241/690] lr: 5.3e-06, eta: 5:25:51.567361, loss: 1.6400
2023-04-14 02:41:50 - training - INFO - Epoch [3/5][251/690] lr: 5.3e-06, eta: 5:13:30.004836, loss: 1.4831
2023-04-14 02:41:57 - training - INFO - Epoch [3/5][261/690] lr: 5.2e-06, eta: 5:02:06.929745, loss: 1.6319
2023-04-14 02:42:05 - training - INFO - Epoch [3/5][271/690] lr: 5.2e-06, eta: 4:51:31.382535, loss: 2.0694
2023-04-14 02:42:12 - training - INFO - Epoch [3/5][281/690] lr: 5.2e-06, eta: 4:41:40.362563, loss: 1.9826
2023-04-14 02:42:20 - training - INFO - Epoch [3/5][291/690] lr: 5.2e-06, eta: 4:32:31.884315, loss: 2.0081
2023-04-14 02:42:28 - training - INFO - Epoch [3/5][301/690] lr: 5.1e-06, eta: 4:23:56.720923, loss: 2.0313
2023-04-14 02:42:35 - training - INFO - Epoch [3/5][311/690] lr: 5.1e-06, eta: 4:15:54.948991, loss: 1.8016
2023-04-14 02:42:43 - training - INFO - Epoch [3/5][321/690] lr: 5.1e-06, eta: 4:08:23.380065, loss: 1.4569
2023-04-14 02:42:50 - training - INFO - Epoch [3/5][331/690] lr: 5.0e-06, eta: 4:01:17.356254, loss: 1.5108
2023-04-14 02:42:58 - training - INFO - Epoch [3/5][341/690] lr: 5.0e-06, eta: 3:54:38.074312, loss: 2.0312
2023-04-14 02:43:06 - training - INFO - Epoch [3/5][351/690] lr: 5.0e-06, eta: 3:48:20.973405, loss: 1.2139
2023-04-14 02:43:13 - training - INFO - Epoch [3/5][361/690] lr: 5.0e-06, eta: 3:42:22.530841, loss: 1.5469
2023-04-14 02:43:21 - training - INFO - Epoch [3/5][371/690] lr: 4.9e-06, eta: 3:36:44.753826, loss: 2.2936
2023-04-14 02:43:28 - training - INFO - Epoch [3/5][381/690] lr: 4.9e-06, eta: 3:31:22.731501, loss: 1.9238
2023-04-14 02:43:36 - training - INFO - Epoch [3/5][391/690] lr: 4.9e-06, eta: 3:26:16.854714, loss: 1.1483
2023-04-14 02:43:44 - training - INFO - Epoch [3/5][401/690] lr: 4.8e-06, eta: 3:21:27.376326, loss: 1.4176
2023-04-14 02:43:51 - training - INFO - Epoch [3/5][411/690] lr: 4.8e-06, eta: 3:16:49.724184, loss: 1.8251
2023-04-14 02:43:59 - training - INFO - Epoch [3/5][421/690] lr: 4.8e-06, eta: 3:12:27.617237, loss: 1.6051
2023-04-14 02:44:06 - training - INFO - Epoch [3/5][431/690] lr: 4.8e-06, eta: 3:08:15.591519, loss: 1.7395
2023-04-14 02:44:14 - training - INFO - Epoch [3/5][441/690] lr: 4.7e-06, eta: 3:04:13.796202, loss: 2.1152
2023-04-14 02:44:22 - training - INFO - Epoch [3/5][451/690] lr: 4.7e-06, eta: 3:00:23.678904, loss: 1.6834
2023-04-14 02:44:29 - training - INFO - Epoch [3/5][461/690] lr: 4.7e-06, eta: 2:56:43.360929, loss: 1.8666
2023-04-14 02:44:37 - training - INFO - Epoch [3/5][471/690] lr: 4.6e-06, eta: 2:53:13.576092, loss: 1.6038
2023-04-14 02:44:45 - training - INFO - Epoch [3/5][481/690] lr: 4.6e-06, eta: 2:49:50.231490, loss: 2.0499
2023-04-14 02:44:52 - training - INFO - Epoch [3/5][491/690] lr: 4.6e-06, eta: 2:46:35.513836, loss: 1.3913
2023-04-14 02:45:00 - training - INFO - Epoch [3/5][501/690] lr: 4.5e-06, eta: 2:43:29.367813, loss: 1.5645
2023-04-14 02:45:08 - training - INFO - Epoch [3/5][511/690] lr: 4.5e-06, eta: 2:40:29.354295, loss: 1.7410
2023-04-14 02:45:16 - training - INFO - Epoch [3/5][521/690] lr: 4.5e-06, eta: 2:37:34.888154, loss: 1.8704
2023-04-14 02:45:23 - training - INFO - Epoch [3/5][531/690] lr: 4.5e-06, eta: 2:34:46.982397, loss: 2.0586
2023-04-14 02:45:31 - training - INFO - Epoch [3/5][541/690] lr: 4.4e-06, eta: 2:32:04.587575, loss: 1.4256
2023-04-14 02:45:38 - training - INFO - Epoch [3/5][551/690] lr: 4.4e-06, eta: 2:29:28.207248, loss: 1.8789
2023-04-14 02:45:46 - training - INFO - Epoch [3/5][561/690] lr: 4.4e-06, eta: 2:26:56.517306, loss: 1.3548
2023-04-14 02:45:54 - training - INFO - Epoch [3/5][571/690] lr: 4.3e-06, eta: 2:24:32.037430, loss: 1.9938
2023-04-14 02:46:01 - training - INFO - Epoch [3/5][581/690] lr: 4.3e-06, eta: 2:22:10.457949, loss: 0.9611
2023-04-14 02:46:09 - training - INFO - Epoch [3/5][591/690] lr: 4.3e-06, eta: 2:19:53.683779, loss: 1.2860
2023-04-14 02:46:16 - training - INFO - Epoch [3/5][601/690] lr: 4.3e-06, eta: 2:17:40.826497, loss: 1.8250
2023-04-14 02:46:24 - training - INFO - Epoch [3/5][611/690] lr: 4.2e-06, eta: 2:15:32.500035, loss: 1.3485
2023-04-14 02:46:32 - training - INFO - Epoch [3/5][621/690] lr: 4.2e-06, eta: 2:13:28.683996, loss: 1.4033
2023-04-14 02:46:39 - training - INFO - Epoch [3/5][631/690] lr: 4.2e-06, eta: 2:11:27.818529, loss: 1.4900
2023-04-14 02:46:47 - training - INFO - Epoch [3/5][641/690] lr: 4.1e-06, eta: 2:09:29.927147, loss: 1.6257
2023-04-14 02:46:54 - training - INFO - Epoch [3/5][651/690] lr: 4.1e-06, eta: 2:07:35.752026, loss: 1.4211
2023-04-14 02:47:02 - training - INFO - Epoch [3/5][661/690] lr: 4.1e-06, eta: 2:05:45.452637, loss: 1.8799
2023-04-14 02:47:10 - training - INFO - Epoch [3/5][671/690] lr: 4.1e-06, eta: 2:03:57.887898, loss: 1.3808
2023-04-14 02:47:17 - training - INFO - Epoch [3/5][681/690] lr: 4.0e-06, eta: 2:02:13.167621, loss: 1.2014
2023-04-14 02:48:40 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 1.6699, Validation Metrics: {'exact_match': 31.548311990686845, 'f1': 39.8965437234971}, Test Metrics: {'exact_match': 32.40418118466899, 'f1': 40.6314432077908}
2023-04-14 02:48:41 - training - INFO - Epoch [4/5][1/690] lr: 4.0e-06, eta: 75 days, 7:50:51.618300, loss: 1.5321
2023-04-14 02:48:49 - training - INFO - Epoch [4/5][11/690] lr: 4.0e-06, eta: 6 days, 20:32:56.075819, loss: 1.0509
2023-04-14 02:48:56 - training - INFO - Epoch [4/5][21/690] lr: 3.9e-06, eta: 3 days, 14:17:13.833519, loss: 1.1513
2023-04-14 02:49:04 - training - INFO - Epoch [4/5][31/690] lr: 3.9e-06, eta: 2 days, 10:30:37.820380, loss: 1.4169
2023-04-14 02:49:11 - training - INFO - Epoch [4/5][41/690] lr: 3.9e-06, eta: 1 day, 20:17:16.250526, loss: 2.1789
2023-04-14 02:49:19 - training - INFO - Epoch [4/5][51/690] lr: 3.9e-06, eta: 1 day, 11:38:18.983703, loss: 0.9739
2023-04-14 02:49:26 - training - INFO - Epoch [4/5][61/690] lr: 3.8e-06, eta: 1 day, 5:49:30.633511, loss: 1.2412
2023-04-14 02:49:34 - training - INFO - Epoch [4/5][71/690] lr: 3.8e-06, eta: 1 day, 1:39:02.177024, loss: 0.9088
2023-04-14 02:49:42 - training - INFO - Epoch [4/5][81/690] lr: 3.8e-06, eta: 22:30:27.003702, loss: 1.7797
2023-04-14 02:49:49 - training - INFO - Epoch [4/5][91/690] lr: 3.7e-06, eta: 20:03:05.662416, loss: 1.7200
2023-04-14 02:49:57 - training - INFO - Epoch [4/5][101/690] lr: 3.7e-06, eta: 18:04:57.309415, loss: 2.1003
2023-04-14 02:50:04 - training - INFO - Epoch [4/5][111/690] lr: 3.7e-06, eta: 16:27:59.637690, loss: 1.4334
2023-04-14 02:50:12 - training - INFO - Epoch [4/5][121/690] lr: 3.6e-06, eta: 15:07:05.118581, loss: 1.5222
2023-04-14 02:50:20 - training - INFO - Epoch [4/5][131/690] lr: 3.6e-06, eta: 13:58:31.725300, loss: 1.5443
2023-04-14 02:50:27 - training - INFO - Epoch [4/5][141/690] lr: 3.6e-06, eta: 12:59:41.387889, loss: 1.7571
2023-04-14 02:50:35 - training - INFO - Epoch [4/5][151/690] lr: 3.6e-06, eta: 12:08:36.823862, loss: 1.8588
2023-04-14 02:50:42 - training - INFO - Epoch [4/5][161/690] lr: 3.5e-06, eta: 11:23:50.811107, loss: 1.5613
2023-04-14 02:50:50 - training - INFO - Epoch [4/5][171/690] lr: 3.5e-06, eta: 10:44:27.089418, loss: 1.5894
2023-04-14 02:50:58 - training - INFO - Epoch [4/5][181/690] lr: 3.5e-06, eta: 10:09:20.433889, loss: 0.8702
2023-04-14 02:51:06 - training - INFO - Epoch [4/5][191/690] lr: 3.4e-06, eta: 9:37:53.367894, loss: 1.3105
2023-04-14 02:51:14 - training - INFO - Epoch [4/5][201/690] lr: 3.4e-06, eta: 9:09:30.637566, loss: 1.0021
2023-04-14 02:51:21 - training - INFO - Epoch [4/5][211/690] lr: 3.4e-06, eta: 8:43:48.191906, loss: 1.7062
2023-04-14 02:51:29 - training - INFO - Epoch [4/5][221/690] lr: 3.4e-06, eta: 8:20:27.368662, loss: 1.5223
2023-04-14 02:51:36 - training - INFO - Epoch [4/5][231/690] lr: 3.3e-06, eta: 7:59:03.651687, loss: 1.6004
2023-04-14 02:51:44 - training - INFO - Epoch [4/5][241/690] lr: 3.3e-06, eta: 7:39:29.853944, loss: 1.8091
2023-04-14 02:51:52 - training - INFO - Epoch [4/5][251/690] lr: 3.3e-06, eta: 7:21:25.135208, loss: 1.4582
2023-04-14 02:52:00 - training - INFO - Epoch [4/5][261/690] lr: 3.2e-06, eta: 7:04:45.642915, loss: 1.4544
2023-04-14 02:52:07 - training - INFO - Epoch [4/5][271/690] lr: 3.2e-06, eta: 6:49:18.572929, loss: 1.2798
2023-04-14 02:52:15 - training - INFO - Epoch [4/5][281/690] lr: 3.2e-06, eta: 6:34:55.712643, loss: 0.9969
2023-04-14 02:52:23 - training - INFO - Epoch [4/5][291/690] lr: 3.2e-06, eta: 6:21:31.823019, loss: 1.5202
2023-04-14 02:52:30 - training - INFO - Epoch [4/5][301/690] lr: 3.1e-06, eta: 6:09:01.837663, loss: 0.8397
2023-04-14 02:52:38 - training - INFO - Epoch [4/5][311/690] lr: 3.1e-06, eta: 5:57:20.063719, loss: 1.1844
2023-04-14 02:52:46 - training - INFO - Epoch [4/5][321/690] lr: 3.1e-06, eta: 5:46:19.282230, loss: 1.5808
2023-04-14 02:52:53 - training - INFO - Epoch [4/5][331/690] lr: 3.0e-06, eta: 5:35:59.544216, loss: 1.5344
2023-04-14 02:53:01 - training - INFO - Epoch [4/5][341/690] lr: 3.0e-06, eta: 5:26:14.282654, loss: 1.7614
2023-04-14 02:53:08 - training - INFO - Epoch [4/5][351/690] lr: 3.0e-06, eta: 5:17:02.198127, loss: 1.4752
2023-04-14 02:53:16 - training - INFO - Epoch [4/5][361/690] lr: 3.0e-06, eta: 5:08:21.889845, loss: 1.0864
2023-04-14 02:53:24 - training - INFO - Epoch [4/5][371/690] lr: 2.9e-06, eta: 5:00:07.891743, loss: 1.8087
2023-04-14 02:53:31 - training - INFO - Epoch [4/5][381/690] lr: 2.9e-06, eta: 4:52:19.258275, loss: 1.1283
2023-04-14 02:53:39 - training - INFO - Epoch [4/5][391/690] lr: 2.9e-06, eta: 4:44:55.126671, loss: 1.2212
2023-04-14 02:53:46 - training - INFO - Epoch [4/5][401/690] lr: 2.8e-06, eta: 4:37:51.983833, loss: 1.6287
2023-04-14 02:53:54 - training - INFO - Epoch [4/5][411/690] lr: 2.8e-06, eta: 4:31:08.563218, loss: 1.4679
2023-04-14 02:54:02 - training - INFO - Epoch [4/5][421/690] lr: 2.8e-06, eta: 4:24:44.945323, loss: 1.6408
2023-04-14 02:54:09 - training - INFO - Epoch [4/5][431/690] lr: 2.8e-06, eta: 4:18:38.780049, loss: 1.2925
2023-04-14 02:54:17 - training - INFO - Epoch [4/5][441/690] lr: 2.7e-06, eta: 4:12:48.426171, loss: 1.1991
2023-04-14 02:54:25 - training - INFO - Epoch [4/5][451/690] lr: 2.7e-06, eta: 4:07:15.408215, loss: 1.7117
2023-04-14 02:54:32 - training - INFO - Epoch [4/5][461/690] lr: 2.7e-06, eta: 4:01:54.784263, loss: 1.7571
2023-04-14 02:54:40 - training - INFO - Epoch [4/5][471/690] lr: 2.6e-06, eta: 3:56:46.484583, loss: 1.8440
2023-04-14 02:54:48 - training - INFO - Epoch [4/5][481/690] lr: 2.6e-06, eta: 3:51:52.000657, loss: 1.6107
2023-04-14 02:54:55 - training - INFO - Epoch [4/5][491/690] lr: 2.6e-06, eta: 3:47:08.840346, loss: 1.9717
2023-04-14 02:55:03 - training - INFO - Epoch [4/5][501/690] lr: 2.5e-06, eta: 3:42:35.852907, loss: 1.9992
2023-04-14 02:55:10 - training - INFO - Epoch [4/5][511/690] lr: 2.5e-06, eta: 3:38:14.141395, loss: 1.8713
2023-04-14 02:55:18 - training - INFO - Epoch [4/5][521/690] lr: 2.5e-06, eta: 3:34:02.065766, loss: 1.3692
2023-04-14 02:55:26 - training - INFO - Epoch [4/5][531/690] lr: 2.5e-06, eta: 3:29:58.520760, loss: 1.1074
2023-04-14 02:55:33 - training - INFO - Epoch [4/5][541/690] lr: 2.4e-06, eta: 3:26:04.241969, loss: 1.6539
2023-04-14 02:55:41 - training - INFO - Epoch [4/5][551/690] lr: 2.4e-06, eta: 3:22:19.437843, loss: 1.3845
2023-04-14 02:55:49 - training - INFO - Epoch [4/5][561/690] lr: 2.4e-06, eta: 3:18:40.302900, loss: 1.7694
2023-04-14 02:55:56 - training - INFO - Epoch [4/5][571/690] lr: 2.3e-06, eta: 3:15:09.059982, loss: 0.9722
2023-04-14 02:56:04 - training - INFO - Epoch [4/5][581/690] lr: 2.3e-06, eta: 3:11:45.840469, loss: 1.6750
2023-04-14 02:56:12 - training - INFO - Epoch [4/5][591/690] lr: 2.3e-06, eta: 3:08:29.026092, loss: 1.3996
2023-04-14 02:56:19 - training - INFO - Epoch [4/5][601/690] lr: 2.3e-06, eta: 3:05:18.686887, loss: 0.5884
2023-04-14 02:56:27 - training - INFO - Epoch [4/5][611/690] lr: 2.2e-06, eta: 3:02:14.110405, loss: 1.2700
2023-04-14 02:56:35 - training - INFO - Epoch [4/5][621/690] lr: 2.2e-06, eta: 2:59:14.375604, loss: 1.5755
2023-04-14 02:56:42 - training - INFO - Epoch [4/5][631/690] lr: 2.2e-06, eta: 2:56:20.876885, loss: 1.7397
2023-04-14 02:56:50 - training - INFO - Epoch [4/5][641/690] lr: 2.1e-06, eta: 2:53:32.634347, loss: 1.2947
2023-04-14 02:56:58 - training - INFO - Epoch [4/5][651/690] lr: 2.1e-06, eta: 2:50:49.820442, loss: 1.3792
2023-04-14 02:57:05 - training - INFO - Epoch [4/5][661/690] lr: 2.1e-06, eta: 2:48:10.719138, loss: 1.0998
2023-04-14 02:57:13 - training - INFO - Epoch [4/5][671/690] lr: 2.1e-06, eta: 2:45:36.239467, loss: 1.0667
2023-04-14 02:57:21 - training - INFO - Epoch [4/5][681/690] lr: 2.0e-06, eta: 2:43:06.053043, loss: 1.5822
2023-04-14 02:58:44 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 1.5117, Validation Metrics: {'exact_match': 31.19906868451688, 'f1': 40.04594012058231}, Test Metrics: {'exact_match': 35.07549361207898, 'f1': 43.30074107077591}
2023-04-14 02:58:45 - training - INFO - Epoch [5/5][1/690] lr: 2.0e-06, eta: 99 days, 10:13:54.729776, loss: 1.1875
2023-04-14 02:58:52 - training - INFO - Epoch [5/5][11/690] lr: 2.0e-06, eta: 9 days, 0:57:42.474852, loss: 0.8666
2023-04-14 02:59:00 - training - INFO - Epoch [5/5][21/690] lr: 1.9e-06, eta: 4 days, 17:39:59.468112, loss: 1.3798
2023-04-14 02:59:08 - training - INFO - Epoch [5/5][31/690] lr: 1.9e-06, eta: 3 days, 5:00:44.791941, loss: 1.0190
2023-04-14 02:59:15 - training - INFO - Epoch [5/5][41/690] lr: 1.9e-06, eta: 2 days, 10:13:56.700448, loss: 1.6444
2023-04-14 02:59:23 - training - INFO - Epoch [5/5][51/690] lr: 1.9e-06, eta: 1 day, 22:48:53.986797, loss: 1.2591
2023-04-14 02:59:30 - training - INFO - Epoch [5/5][61/690] lr: 1.8e-06, eta: 1 day, 15:08:24.375053, loss: 0.9742
2023-04-14 02:59:38 - training - INFO - Epoch [5/5][71/690] lr: 1.8e-06, eta: 1 day, 9:37:48.525889, loss: 1.2893
2023-04-14 02:59:45 - training - INFO - Epoch [5/5][81/690] lr: 1.8e-06, eta: 1 day, 5:28:42.826200, loss: 1.4308
2023-04-14 02:59:53 - training - INFO - Epoch [5/5][91/690] lr: 1.7e-06, eta: 1 day, 2:14:17.780636, loss: 2.4733
2023-04-14 03:00:01 - training - INFO - Epoch [5/5][101/690] lr: 1.7e-06, eta: 23:38:34.520194, loss: 1.1770
2023-04-14 03:00:08 - training - INFO - Epoch [5/5][111/690] lr: 1.7e-06, eta: 21:30:47.674269, loss: 1.7303
2023-04-14 03:00:16 - training - INFO - Epoch [5/5][121/690] lr: 1.6e-06, eta: 19:44:05.270925, loss: 1.3274
2023-04-14 03:00:24 - training - INFO - Epoch [5/5][131/690] lr: 1.6e-06, eta: 18:13:42.687175, loss: 1.5461
2023-04-14 03:00:32 - training - INFO - Epoch [5/5][141/690] lr: 1.6e-06, eta: 16:56:02.980965, loss: 1.6597
2023-04-14 03:00:39 - training - INFO - Epoch [5/5][151/690] lr: 1.6e-06, eta: 15:48:38.409069, loss: 1.9777
2023-04-14 03:00:47 - training - INFO - Epoch [5/5][161/690] lr: 1.5e-06, eta: 14:49:35.717395, loss: 1.2358
2023-04-14 03:00:54 - training - INFO - Epoch [5/5][171/690] lr: 1.5e-06, eta: 13:57:27.300909, loss: 1.5683
2023-04-14 03:01:02 - training - INFO - Epoch [5/5][181/690] lr: 1.5e-06, eta: 13:11:05.249083, loss: 1.1525
2023-04-14 03:01:10 - training - INFO - Epoch [5/5][191/690] lr: 1.4e-06, eta: 12:29:32.427104, loss: 2.1410
2023-04-14 03:01:17 - training - INFO - Epoch [5/5][201/690] lr: 1.4e-06, eta: 11:52:08.895351, loss: 1.1816
2023-04-14 03:01:25 - training - INFO - Epoch [5/5][211/690] lr: 1.4e-06, eta: 11:18:18.384812, loss: 1.4270
2023-04-14 03:01:33 - training - INFO - Epoch [5/5][221/690] lr: 1.4e-06, eta: 10:47:28.005359, loss: 1.5766
2023-04-14 03:01:40 - training - INFO - Epoch [5/5][231/690] lr: 1.3e-06, eta: 10:19:17.815101, loss: 2.0174
2023-04-14 03:01:48 - training - INFO - Epoch [5/5][241/690] lr: 1.3e-06, eta: 9:53:25.809281, loss: 1.7761
2023-04-14 03:01:56 - training - INFO - Epoch [5/5][251/690] lr: 1.3e-06, eta: 9:29:38.407109, loss: 1.8803
2023-04-14 03:02:03 - training - INFO - Epoch [5/5][261/690] lr: 1.2e-06, eta: 9:07:40.093800, loss: 1.6819
2023-04-14 03:02:11 - training - INFO - Epoch [5/5][271/690] lr: 1.2e-06, eta: 8:47:18.743180, loss: 1.2209
2023-04-14 03:02:18 - training - INFO - Epoch [5/5][281/690] lr: 1.2e-06, eta: 8:28:22.109857, loss: 0.9510
2023-04-14 03:02:26 - training - INFO - Epoch [5/5][291/690] lr: 1.2e-06, eta: 8:10:42.821382, loss: 2.0418
2023-04-14 03:02:34 - training - INFO - Epoch [5/5][301/690] lr: 1.1e-06, eta: 7:54:12.877672, loss: 1.6101
2023-04-14 03:02:41 - training - INFO - Epoch [5/5][311/690] lr: 1.1e-06, eta: 7:38:46.747947, loss: 1.4187
2023-04-14 03:02:49 - training - INFO - Epoch [5/5][321/690] lr: 1.1e-06, eta: 7:24:19.862250, loss: 1.8557
2023-04-14 03:02:56 - training - INFO - Epoch [5/5][331/690] lr: 1.0e-06, eta: 7:10:44.130689, loss: 1.1991
2023-04-14 03:03:04 - training - INFO - Epoch [5/5][341/690] lr: 1.0e-06, eta: 6:57:55.633282, loss: 1.3804
2023-04-14 03:03:12 - training - INFO - Epoch [5/5][351/690] lr: 9.8e-07, eta: 6:45:50.566044, loss: 0.9265
2023-04-14 03:03:20 - training - INFO - Epoch [5/5][361/690] lr: 9.5e-07, eta: 6:34:25.869993, loss: 1.4740
2023-04-14 03:03:27 - training - INFO - Epoch [5/5][371/690] lr: 9.2e-07, eta: 6:23:36.159274, loss: 1.0566
2023-04-14 03:03:35 - training - INFO - Epoch [5/5][381/690] lr: 9.0e-07, eta: 6:13:20.775243, loss: 1.2532
2023-04-14 03:03:42 - training - INFO - Epoch [5/5][391/690] lr: 8.7e-07, eta: 6:03:36.295501, loss: 1.5285
2023-04-14 03:03:50 - training - INFO - Epoch [5/5][401/690] lr: 8.4e-07, eta: 5:54:19.731810, loss: 1.3300
2023-04-14 03:03:57 - training - INFO - Epoch [5/5][411/690] lr: 8.1e-07, eta: 5:45:30.283224, loss: 1.2935
2023-04-14 03:04:05 - training - INFO - Epoch [5/5][421/690] lr: 7.8e-07, eta: 5:37:06.504922, loss: 1.4872
2023-04-14 03:04:13 - training - INFO - Epoch [5/5][431/690] lr: 7.5e-07, eta: 5:29:05.358916, loss: 2.0899
2023-04-14 03:04:20 - training - INFO - Epoch [5/5][441/690] lr: 7.2e-07, eta: 5:21:25.502457, loss: 1.1888
2023-04-14 03:04:28 - training - INFO - Epoch [5/5][451/690] lr: 6.9e-07, eta: 5:14:05.760985, loss: 1.1437
2023-04-14 03:04:35 - training - INFO - Epoch [5/5][461/690] lr: 6.6e-07, eta: 5:07:04.620438, loss: 1.5870
2023-04-14 03:04:43 - training - INFO - Epoch [5/5][471/690] lr: 6.3e-07, eta: 5:00:21.397941, loss: 1.2381
2023-04-14 03:04:51 - training - INFO - Epoch [5/5][481/690] lr: 6.1e-07, eta: 4:53:55.242448, loss: 1.5032
2023-04-14 03:04:59 - training - INFO - Epoch [5/5][491/690] lr: 5.8e-07, eta: 4:47:44.039903, loss: 1.4600
2023-04-14 03:05:07 - training - INFO - Epoch [5/5][501/690] lr: 5.5e-07, eta: 4:41:49.975911, loss: 1.5298
2023-04-14 03:05:14 - training - INFO - Epoch [5/5][511/690] lr: 5.2e-07, eta: 4:36:06.569895, loss: 1.4751
2023-04-14 03:05:22 - training - INFO - Epoch [5/5][521/690] lr: 4.9e-07, eta: 4:30:36.785553, loss: 1.3674
2023-04-14 03:05:30 - training - INFO - Epoch [5/5][531/690] lr: 4.6e-07, eta: 4:25:18.544656, loss: 1.0024
2023-04-14 03:05:37 - training - INFO - Epoch [5/5][541/690] lr: 4.3e-07, eta: 4:20:12.640817, loss: 0.7371
2023-04-14 03:05:45 - training - INFO - Epoch [5/5][551/690] lr: 4.0e-07, eta: 4:15:17.359330, loss: 1.5011
2023-04-14 03:05:53 - training - INFO - Epoch [5/5][561/690] lr: 3.7e-07, eta: 4:10:31.244547, loss: 1.3053
2023-04-14 03:06:00 - training - INFO - Epoch [5/5][571/690] lr: 3.4e-07, eta: 4:05:55.548686, loss: 1.3228
2023-04-14 03:06:08 - training - INFO - Epoch [5/5][581/690] lr: 3.2e-07, eta: 4:01:28.728293, loss: 1.3832
2023-04-14 03:06:16 - training - INFO - Epoch [5/5][591/690] lr: 2.9e-07, eta: 3:57:10.603884, loss: 1.9408
2023-04-14 03:06:23 - training - INFO - Epoch [5/5][601/690] lr: 2.6e-07, eta: 3:53:01.011660, loss: 1.7388
2023-04-14 03:06:31 - training - INFO - Epoch [5/5][611/690] lr: 2.3e-07, eta: 3:48:59.613044, loss: 1.7792
2023-04-14 03:06:38 - training - INFO - Epoch [5/5][621/690] lr: 2.0e-07, eta: 3:45:05.145267, loss: 1.3653
2023-04-14 03:06:46 - training - INFO - Epoch [5/5][631/690] lr: 1.7e-07, eta: 3:41:17.718339, loss: 1.7972
2023-04-14 03:06:54 - training - INFO - Epoch [5/5][641/690] lr: 1.4e-07, eta: 3:37:37.715152, loss: 1.8373
2023-04-14 03:07:01 - training - INFO - Epoch [5/5][651/690] lr: 1.1e-07, eta: 3:34:04.896498, loss: 1.1405
2023-04-14 03:07:09 - training - INFO - Epoch [5/5][661/690] lr: 8.4e-08, eta: 3:30:37.854269, loss: 1.1765
2023-04-14 03:07:17 - training - INFO - Epoch [5/5][671/690] lr: 5.5e-08, eta: 3:27:16.753098, loss: 1.4334
2023-04-14 03:07:24 - training - INFO - Epoch [5/5][681/690] lr: 2.6e-08, eta: 3:24:00.732777, loss: 1.0933
2023-04-14 03:08:53 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 1.4309, Validation Metrics: {'exact_match': 31.431897555296857, 'f1': 39.44047651162628}, Test Metrics: {'exact_match': 34.61091753774681, 'f1': 42.87870279159478}
2023-04-14 03:09:33 - training - INFO - Final Test - Train Loss: 1.4309, Test Metrics: {'exact_match': 34.61091753774681, 'f1': 42.87870279159478}
