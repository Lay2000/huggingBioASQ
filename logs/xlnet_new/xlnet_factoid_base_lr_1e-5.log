2023-04-14 01:57:15 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1380cc367820a3f3/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'xlnet-base-cased'}, 'data': {'task_type': 'factoid', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 1e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/xlnet_factoid_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 611.80it/s]
Map:   0%|          | 0/4429 [00:00<?, ? examples/s]Map:  23%|██▎       | 1000/4429 [00:00<00:02, 1243.08 examples/s]Map:  45%|████▌     | 2000/4429 [00:01<00:01, 1447.28 examples/s]Map:  68%|██████▊   | 3000/4429 [00:02<00:00, 1495.15 examples/s]Map:  90%|█████████ | 4000/4429 [00:02<00:00, 1511.06 examples/s]Map: 100%|██████████| 4429/4429 [00:02<00:00, 1517.73 examples/s]                                                                 Map:   0%|          | 0/553 [00:00<?, ? examples/s]Map: 100%|██████████| 553/553 [00:00<00:00, 1251.30 examples/s]                                                               Map:   0%|          | 0/555 [00:00<?, ? examples/s]Map: 100%|██████████| 555/555 [00:00<00:00, 1227.59 examples/s]                                                               Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForQuestionAnsweringSimple: ['lm_loss.weight', 'lm_loss.bias']
- This IS expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForQuestionAnsweringSimple were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-14 01:58:15 - training - INFO - First Test - Val Metrics:{'exact_match': 0.5424954792043399, 'f1': 5.423327858099389} Test Metrics: {'exact_match': 0.36036036036036034, 'f1': 6.186024174878664}
2023-04-14 01:58:16 - training - INFO - Epoch [1/5][1/438] lr: 1.0e-05, eta: 1 day, 6:23:24.315570, loss: 6.8915
2023-04-14 01:58:24 - training - INFO - Epoch [1/5][11/438] lr: 9.9e-06, eta: 3:10:44.153759, loss: 5.9125
2023-04-14 01:58:32 - training - INFO - Epoch [1/5][21/438] lr: 9.9e-06, eta: 1:53:14.633259, loss: 5.5594
2023-04-14 01:58:40 - training - INFO - Epoch [1/5][31/438] lr: 9.9e-06, eta: 1:25:30.714529, loss: 4.9341
2023-04-14 01:58:47 - training - INFO - Epoch [1/5][41/438] lr: 9.8e-06, eta: 1:11:02.425454, loss: 4.8262
2023-04-14 01:58:55 - training - INFO - Epoch [1/5][51/438] lr: 9.8e-06, eta: 1:02:19.806210, loss: 4.9310
2023-04-14 01:59:03 - training - INFO - Epoch [1/5][61/438] lr: 9.7e-06, eta: 0:56:20.958450, loss: 5.1027
2023-04-14 01:59:11 - training - INFO - Epoch [1/5][71/438] lr: 9.7e-06, eta: 0:52:07.150273, loss: 4.0770
2023-04-14 01:59:19 - training - INFO - Epoch [1/5][81/438] lr: 9.6e-06, eta: 0:48:49.801710, loss: 4.3622
2023-04-14 01:59:26 - training - INFO - Epoch [1/5][91/438] lr: 9.6e-06, eta: 0:46:13.219790, loss: 3.8833
2023-04-14 01:59:34 - training - INFO - Epoch [1/5][101/438] lr: 9.5e-06, eta: 0:44:11.154078, loss: 4.1864
2023-04-14 01:59:43 - training - INFO - Epoch [1/5][111/438] lr: 9.5e-06, eta: 0:42:36.124263, loss: 3.6184
2023-04-14 01:59:50 - training - INFO - Epoch [1/5][121/438] lr: 9.4e-06, eta: 0:41:02.836219, loss: 3.0304
2023-04-14 01:59:58 - training - INFO - Epoch [1/5][131/438] lr: 9.4e-06, eta: 0:39:44.151103, loss: 3.9239
2023-04-14 02:00:06 - training - INFO - Epoch [1/5][141/438] lr: 9.4e-06, eta: 0:38:38.822565, loss: 3.5338
2023-04-14 02:00:13 - training - INFO - Epoch [1/5][151/438] lr: 9.3e-06, eta: 0:37:40.178486, loss: 3.1791
2023-04-14 02:00:21 - training - INFO - Epoch [1/5][161/438] lr: 9.3e-06, eta: 0:36:48.878966, loss: 2.7117
2023-04-14 02:00:29 - training - INFO - Epoch [1/5][171/438] lr: 9.2e-06, eta: 0:36:00.493539, loss: 2.7982
2023-04-14 02:00:38 - training - INFO - Epoch [1/5][181/438] lr: 9.2e-06, eta: 0:35:32.095448, loss: 3.1179
2023-04-14 02:00:48 - training - INFO - Epoch [1/5][191/438] lr: 9.1e-06, eta: 0:35:08.763091, loss: 2.1074
2023-04-14 02:00:57 - training - INFO - Epoch [1/5][201/438] lr: 9.1e-06, eta: 0:34:44.820075, loss: 2.1512
2023-04-14 02:01:06 - training - INFO - Epoch [1/5][211/438] lr: 9.0e-06, eta: 0:34:24.186055, loss: 2.4519
2023-04-14 02:01:15 - training - INFO - Epoch [1/5][221/438] lr: 9.0e-06, eta: 0:33:59.257858, loss: 3.3748
2023-04-14 02:01:24 - training - INFO - Epoch [1/5][231/438] lr: 8.9e-06, eta: 0:33:37.867950, loss: 2.1567
2023-04-14 02:01:33 - training - INFO - Epoch [1/5][241/438] lr: 8.9e-06, eta: 0:33:16.600427, loss: 2.8726
2023-04-14 02:01:43 - training - INFO - Epoch [1/5][251/438] lr: 8.9e-06, eta: 0:33:02.191225, loss: 2.9478
2023-04-14 02:01:52 - training - INFO - Epoch [1/5][261/438] lr: 8.8e-06, eta: 0:32:42.834660, loss: 2.0910
2023-04-14 02:02:01 - training - INFO - Epoch [1/5][271/438] lr: 8.8e-06, eta: 0:32:23.693692, loss: 3.0179
2023-04-14 02:02:10 - training - INFO - Epoch [1/5][281/438] lr: 8.7e-06, eta: 0:32:09.340395, loss: 2.4026
2023-04-14 02:02:19 - training - INFO - Epoch [1/5][291/438] lr: 8.7e-06, eta: 0:31:54.598386, loss: 2.9999
2023-04-14 02:02:29 - training - INFO - Epoch [1/5][301/438] lr: 8.6e-06, eta: 0:31:38.922917, loss: 2.2681
2023-04-14 02:02:38 - training - INFO - Epoch [1/5][311/438] lr: 8.6e-06, eta: 0:31:24.915092, loss: 2.4490
2023-04-14 02:02:46 - training - INFO - Epoch [1/5][321/438] lr: 8.5e-06, eta: 0:31:02.243565, loss: 2.5644
2023-04-14 02:02:54 - training - INFO - Epoch [1/5][331/438] lr: 8.5e-06, eta: 0:30:39.478641, loss: 2.2021
2023-04-14 02:03:01 - training - INFO - Epoch [1/5][341/438] lr: 8.4e-06, eta: 0:30:18.310298, loss: 2.0084
2023-04-14 02:03:09 - training - INFO - Epoch [1/5][351/438] lr: 8.4e-06, eta: 0:29:58.021563, loss: 1.8443
2023-04-14 02:03:17 - training - INFO - Epoch [1/5][361/438] lr: 8.4e-06, eta: 0:29:37.740446, loss: 2.3132
2023-04-14 02:03:25 - training - INFO - Epoch [1/5][371/438] lr: 8.3e-06, eta: 0:29:17.596017, loss: 2.5021
2023-04-14 02:03:32 - training - INFO - Epoch [1/5][381/438] lr: 8.3e-06, eta: 0:28:58.110717, loss: 1.8061
2023-04-14 02:03:40 - training - INFO - Epoch [1/5][391/438] lr: 8.2e-06, eta: 0:28:40.923400, loss: 2.3830
2023-04-14 02:03:48 - training - INFO - Epoch [1/5][401/438] lr: 8.2e-06, eta: 0:28:22.691484, loss: 2.4183
2023-04-14 02:03:56 - training - INFO - Epoch [1/5][411/438] lr: 8.1e-06, eta: 0:28:07.171578, loss: 1.8328
2023-04-14 02:04:03 - training - INFO - Epoch [1/5][421/438] lr: 8.1e-06, eta: 0:27:49.769714, loss: 1.8887
2023-04-14 02:04:11 - training - INFO - Epoch [1/5][431/438] lr: 8.0e-06, eta: 0:27:33.577853, loss: 1.5867
2023-04-14 02:05:06 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 3.1486, Validation Metrics: {'exact_match': 57.68535262206148, 'f1': 64.69472810058348}, Test Metrics: {'exact_match': 58.91891891891892, 'f1': 69.39548595864386}
2023-04-14 02:05:07 - training - INFO - Epoch [2/5][1/438] lr: 8.0e-06, eta: 11 days, 16:10:36.249533, loss: 2.1555
2023-04-14 02:05:15 - training - INFO - Epoch [2/5][11/438] lr: 7.9e-06, eta: 1 day, 1:46:51.903274, loss: 1.9419
2023-04-14 02:05:22 - training - INFO - Epoch [2/5][21/438] lr: 7.9e-06, eta: 13:39:42.528321, loss: 1.7679
2023-04-14 02:05:30 - training - INFO - Epoch [2/5][31/438] lr: 7.9e-06, eta: 9:21:35.072564, loss: 1.6330
2023-04-14 02:05:38 - training - INFO - Epoch [2/5][41/438] lr: 7.8e-06, eta: 7:09:24.096673, loss: 1.4497
2023-04-14 02:05:45 - training - INFO - Epoch [2/5][51/438] lr: 7.8e-06, eta: 5:49:05.579970, loss: 1.9986
2023-04-14 02:05:53 - training - INFO - Epoch [2/5][61/438] lr: 7.7e-06, eta: 4:54:59.658658, loss: 1.7473
2023-04-14 02:06:01 - training - INFO - Epoch [2/5][71/438] lr: 7.7e-06, eta: 4:16:09.725748, loss: 1.4159
2023-04-14 02:06:09 - training - INFO - Epoch [2/5][81/438] lr: 7.6e-06, eta: 3:46:47.795250, loss: 1.0296
2023-04-14 02:06:16 - training - INFO - Epoch [2/5][91/438] lr: 7.6e-06, eta: 3:23:52.652952, loss: 1.0224
2023-04-14 02:06:24 - training - INFO - Epoch [2/5][101/438] lr: 7.5e-06, eta: 3:05:32.360382, loss: 1.7019
2023-04-14 02:06:32 - training - INFO - Epoch [2/5][111/438] lr: 7.5e-06, eta: 2:50:26.143620, loss: 1.5706
2023-04-14 02:06:40 - training - INFO - Epoch [2/5][121/438] lr: 7.4e-06, eta: 2:37:51.718549, loss: 1.5982
2023-04-14 02:06:48 - training - INFO - Epoch [2/5][131/438] lr: 7.4e-06, eta: 2:27:11.289844, loss: 2.7125
2023-04-14 02:06:56 - training - INFO - Epoch [2/5][141/438] lr: 7.4e-06, eta: 2:17:56.400711, loss: 2.4412
2023-04-14 02:07:03 - training - INFO - Epoch [2/5][151/438] lr: 7.3e-06, eta: 2:09:52.611459, loss: 1.9119
2023-04-14 02:07:11 - training - INFO - Epoch [2/5][161/438] lr: 7.3e-06, eta: 2:02:48.481907, loss: 2.0445
2023-04-14 02:07:19 - training - INFO - Epoch [2/5][171/438] lr: 7.2e-06, eta: 1:56:35.507922, loss: 1.3730
2023-04-14 02:07:26 - training - INFO - Epoch [2/5][181/438] lr: 7.2e-06, eta: 1:51:03.909252, loss: 1.3064
2023-04-14 02:07:34 - training - INFO - Epoch [2/5][191/438] lr: 7.1e-06, eta: 1:46:04.756030, loss: 1.3924
2023-04-14 02:07:42 - training - INFO - Epoch [2/5][201/438] lr: 7.1e-06, eta: 1:41:35.901123, loss: 1.5433
2023-04-14 02:07:50 - training - INFO - Epoch [2/5][211/438] lr: 7.0e-06, eta: 1:37:30.123879, loss: 1.6265
2023-04-14 02:07:58 - training - INFO - Epoch [2/5][221/438] lr: 7.0e-06, eta: 1:33:45.832707, loss: 1.7842
2023-04-14 02:08:05 - training - INFO - Epoch [2/5][231/438] lr: 6.9e-06, eta: 1:30:20.601975, loss: 1.2804
2023-04-14 02:08:14 - training - INFO - Epoch [2/5][241/438] lr: 6.9e-06, eta: 1:27:16.278901, loss: 1.0828
2023-04-14 02:08:21 - training - INFO - Epoch [2/5][251/438] lr: 6.9e-06, eta: 1:24:21.379456, loss: 1.4864
2023-04-14 02:08:29 - training - INFO - Epoch [2/5][261/438] lr: 6.8e-06, eta: 1:21:39.588627, loss: 2.2863
2023-04-14 02:08:37 - training - INFO - Epoch [2/5][271/438] lr: 6.8e-06, eta: 1:19:12.073432, loss: 1.4098
2023-04-14 02:08:45 - training - INFO - Epoch [2/5][281/438] lr: 6.7e-06, eta: 1:16:54.049182, loss: 0.8573
2023-04-14 02:08:53 - training - INFO - Epoch [2/5][291/438] lr: 6.7e-06, eta: 1:14:43.404171, loss: 1.8662
2023-04-14 02:09:01 - training - INFO - Epoch [2/5][301/438] lr: 6.6e-06, eta: 1:12:40.622381, loss: 1.1111
2023-04-14 02:09:09 - training - INFO - Epoch [2/5][311/438] lr: 6.6e-06, eta: 1:10:45.421995, loss: 1.3913
2023-04-14 02:09:17 - training - INFO - Epoch [2/5][321/438] lr: 6.5e-06, eta: 1:08:57.005334, loss: 2.5699
2023-04-14 02:09:24 - training - INFO - Epoch [2/5][331/438] lr: 6.5e-06, eta: 1:07:14.585841, loss: 2.1331
2023-04-14 02:09:32 - training - INFO - Epoch [2/5][341/438] lr: 6.4e-06, eta: 1:05:37.848582, loss: 2.0485
2023-04-14 02:09:40 - training - INFO - Epoch [2/5][351/438] lr: 6.4e-06, eta: 1:04:05.325093, loss: 1.8783
2023-04-14 02:09:48 - training - INFO - Epoch [2/5][361/438] lr: 6.4e-06, eta: 1:02:38.220055, loss: 1.3096
2023-04-14 02:09:56 - training - INFO - Epoch [2/5][371/438] lr: 6.3e-06, eta: 1:01:15.405916, loss: 1.4148
2023-04-14 02:10:03 - training - INFO - Epoch [2/5][381/438] lr: 6.3e-06, eta: 0:59:55.432725, loss: 1.3108
2023-04-14 02:10:11 - training - INFO - Epoch [2/5][391/438] lr: 6.2e-06, eta: 0:58:41.053172, loss: 1.1457
2023-04-14 02:10:19 - training - INFO - Epoch [2/5][401/438] lr: 6.2e-06, eta: 0:57:28.603419, loss: 1.7002
2023-04-14 02:10:27 - training - INFO - Epoch [2/5][411/438] lr: 6.1e-06, eta: 0:56:19.466676, loss: 1.9012
2023-04-14 02:10:35 - training - INFO - Epoch [2/5][421/438] lr: 6.1e-06, eta: 0:55:13.936691, loss: 1.6702
2023-04-14 02:10:43 - training - INFO - Epoch [2/5][431/438] lr: 6.0e-06, eta: 0:54:10.617928, loss: 1.0881
2023-04-14 02:11:37 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.6116, Validation Metrics: {'exact_match': 69.07775768535262, 'f1': 74.79441157819073}, Test Metrics: {'exact_match': 73.87387387387388, 'f1': 79.5596955596956}
2023-04-14 02:11:37 - training - INFO - Epoch [3/5][1/438] lr: 6.0e-06, eta: 21 days, 13:41:19.694206, loss: 0.9797
2023-04-14 02:11:45 - training - INFO - Epoch [3/5][11/438] lr: 5.9e-06, eta: 1 day, 23:16:55.694433, loss: 0.9773
2023-04-14 02:11:53 - training - INFO - Epoch [3/5][21/438] lr: 5.9e-06, eta: 1 day, 0:52:20.861886, loss: 1.3572
2023-04-14 02:12:01 - training - INFO - Epoch [3/5][31/438] lr: 5.9e-06, eta: 16:55:07.188447, loss: 1.5915
2023-04-14 02:12:08 - training - INFO - Epoch [3/5][41/438] lr: 5.8e-06, eta: 12:50:49.480004, loss: 1.1977
2023-04-14 02:12:16 - training - INFO - Epoch [3/5][51/438] lr: 5.8e-06, eta: 10:22:12.784098, loss: 1.6004
2023-04-14 02:12:24 - training - INFO - Epoch [3/5][61/438] lr: 5.7e-06, eta: 8:42:15.773789, loss: 0.8080
2023-04-14 02:12:32 - training - INFO - Epoch [3/5][71/438] lr: 5.7e-06, eta: 7:30:29.944929, loss: 1.6757
2023-04-14 02:12:39 - training - INFO - Epoch [3/5][81/438] lr: 5.6e-06, eta: 6:36:20.301561, loss: 1.2803
2023-04-14 02:12:47 - training - INFO - Epoch [3/5][91/438] lr: 5.6e-06, eta: 5:54:07.912526, loss: 1.0632
2023-04-14 02:12:55 - training - INFO - Epoch [3/5][101/438] lr: 5.5e-06, eta: 5:20:14.252247, loss: 1.0577
2023-04-14 02:13:03 - training - INFO - Epoch [3/5][111/438] lr: 5.5e-06, eta: 4:52:27.364989, loss: 1.8465
2023-04-14 02:13:11 - training - INFO - Epoch [3/5][121/438] lr: 5.4e-06, eta: 4:29:16.643066, loss: 1.1752
2023-04-14 02:13:19 - training - INFO - Epoch [3/5][131/438] lr: 5.4e-06, eta: 4:09:33.741883, loss: 1.0203
2023-04-14 02:13:27 - training - INFO - Epoch [3/5][141/438] lr: 5.4e-06, eta: 3:52:37.611786, loss: 0.9779
2023-04-14 02:13:34 - training - INFO - Epoch [3/5][151/438] lr: 5.3e-06, eta: 3:37:52.332811, loss: 1.5039
2023-04-14 02:13:42 - training - INFO - Epoch [3/5][161/438] lr: 5.3e-06, eta: 3:24:57.261750, loss: 2.6290
2023-04-14 02:13:50 - training - INFO - Epoch [3/5][171/438] lr: 5.2e-06, eta: 3:13:31.547622, loss: 1.0147
2023-04-14 02:13:57 - training - INFO - Epoch [3/5][181/438] lr: 5.2e-06, eta: 3:03:21.127298, loss: 1.1407
2023-04-14 02:14:05 - training - INFO - Epoch [3/5][191/438] lr: 5.1e-06, eta: 2:54:14.438166, loss: 0.8974
2023-04-14 02:14:13 - training - INFO - Epoch [3/5][201/438] lr: 5.1e-06, eta: 2:46:01.049241, loss: 0.9923
2023-04-14 02:14:21 - training - INFO - Epoch [3/5][211/438] lr: 5.0e-06, eta: 2:38:34.986483, loss: 1.1897
2023-04-14 02:14:28 - training - INFO - Epoch [3/5][221/438] lr: 5.0e-06, eta: 2:31:48.857846, loss: 1.8325
2023-04-14 02:14:36 - training - INFO - Epoch [3/5][231/438] lr: 4.9e-06, eta: 2:25:36.501366, loss: 1.4127
2023-04-14 02:14:44 - training - INFO - Epoch [3/5][241/438] lr: 4.9e-06, eta: 2:19:55.500706, loss: 2.0494
2023-04-14 02:14:52 - training - INFO - Epoch [3/5][251/438] lr: 4.9e-06, eta: 2:14:40.301628, loss: 1.4428
2023-04-14 02:15:00 - training - INFO - Epoch [3/5][261/438] lr: 4.8e-06, eta: 2:09:46.830951, loss: 2.0379
2023-04-14 02:15:07 - training - INFO - Epoch [3/5][271/438] lr: 4.8e-06, eta: 2:05:15.448784, loss: 1.2062
2023-04-14 02:15:15 - training - INFO - Epoch [3/5][281/438] lr: 4.7e-06, eta: 2:01:02.960401, loss: 0.8832
2023-04-14 02:15:23 - training - INFO - Epoch [3/5][291/438] lr: 4.7e-06, eta: 1:57:08.105949, loss: 1.1453
2023-04-14 02:15:31 - training - INFO - Epoch [3/5][301/438] lr: 4.6e-06, eta: 1:53:27.209845, loss: 1.8352
2023-04-14 02:15:39 - training - INFO - Epoch [3/5][311/438] lr: 4.6e-06, eta: 1:50:01.180665, loss: 0.7352
2023-04-14 02:15:47 - training - INFO - Epoch [3/5][321/438] lr: 4.5e-06, eta: 1:46:49.219656, loss: 1.8670
2023-04-14 02:15:55 - training - INFO - Epoch [3/5][331/438] lr: 4.5e-06, eta: 1:43:46.179531, loss: 1.0518
2023-04-14 02:16:03 - training - INFO - Epoch [3/5][341/438] lr: 4.4e-06, eta: 1:40:53.892256, loss: 0.7911
2023-04-14 02:16:10 - training - INFO - Epoch [3/5][351/438] lr: 4.4e-06, eta: 1:38:10.802496, loss: 1.0736
2023-04-14 02:16:18 - training - INFO - Epoch [3/5][361/438] lr: 4.4e-06, eta: 1:35:36.150038, loss: 0.8141
2023-04-14 02:16:26 - training - INFO - Epoch [3/5][371/438] lr: 4.3e-06, eta: 1:33:08.353628, loss: 0.9435
2023-04-14 02:16:34 - training - INFO - Epoch [3/5][381/438] lr: 4.3e-06, eta: 1:30:48.275649, loss: 1.5105
2023-04-14 02:16:41 - training - INFO - Epoch [3/5][391/438] lr: 4.2e-06, eta: 1:28:35.228254, loss: 0.9852
2023-04-14 02:16:49 - training - INFO - Epoch [3/5][401/438] lr: 4.2e-06, eta: 1:26:29.346933, loss: 1.4537
2023-04-14 02:16:57 - training - INFO - Epoch [3/5][411/438] lr: 4.1e-06, eta: 1:24:29.425947, loss: 0.5417
2023-04-14 02:17:05 - training - INFO - Epoch [3/5][421/438] lr: 4.1e-06, eta: 1:22:33.737776, loss: 0.6586
2023-04-14 02:17:13 - training - INFO - Epoch [3/5][431/438] lr: 4.0e-06, eta: 1:20:43.524353, loss: 0.8198
2023-04-14 02:18:07 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 1.2876, Validation Metrics: {'exact_match': 69.43942133815551, 'f1': 75.27346587731736}, Test Metrics: {'exact_match': 73.51351351351352, 'f1': 79.75539125539126}
2023-04-14 02:18:08 - training - INFO - Epoch [4/5][1/438] lr: 4.0e-06, eta: 31 days, 11:11:38.002592, loss: 0.9307
2023-04-14 02:18:16 - training - INFO - Epoch [4/5][11/438] lr: 3.9e-06, eta: 2 days, 20:46:10.270698, loss: 1.5571
2023-04-14 02:18:24 - training - INFO - Epoch [4/5][21/438] lr: 3.9e-06, eta: 1 day, 12:04:50.094327, loss: 0.8210
2023-04-14 02:18:32 - training - INFO - Epoch [4/5][31/438] lr: 3.9e-06, eta: 1 day, 0:28:51.139968, loss: 0.9661
2023-04-14 02:18:39 - training - INFO - Epoch [4/5][41/438] lr: 3.8e-06, eta: 18:32:07.814615, loss: 0.7125
2023-04-14 02:18:47 - training - INFO - Epoch [4/5][51/438] lr: 3.8e-06, eta: 14:55:40.441344, loss: 0.6979
2023-04-14 02:18:55 - training - INFO - Epoch [4/5][61/438] lr: 3.7e-06, eta: 12:29:45.772129, loss: 1.1605
2023-04-14 02:19:03 - training - INFO - Epoch [4/5][71/438] lr: 3.7e-06, eta: 10:44:59.799203, loss: 1.4348
2023-04-14 02:19:11 - training - INFO - Epoch [4/5][81/438] lr: 3.6e-06, eta: 9:26:04.668888, loss: 1.3271
2023-04-14 02:19:18 - training - INFO - Epoch [4/5][91/438] lr: 3.6e-06, eta: 8:24:24.588925, loss: 1.4634
2023-04-14 02:19:26 - training - INFO - Epoch [4/5][101/438] lr: 3.5e-06, eta: 7:35:00.184238, loss: 0.9851
2023-04-14 02:19:34 - training - INFO - Epoch [4/5][111/438] lr: 3.5e-06, eta: 6:54:25.987608, loss: 0.9728
2023-04-14 02:19:41 - training - INFO - Epoch [4/5][121/438] lr: 3.4e-06, eta: 6:20:32.321222, loss: 1.3805
2023-04-14 02:19:49 - training - INFO - Epoch [4/5][131/438] lr: 3.4e-06, eta: 5:51:48.312070, loss: 1.0418
2023-04-14 02:19:57 - training - INFO - Epoch [4/5][141/438] lr: 3.4e-06, eta: 5:27:08.233629, loss: 1.5404
2023-04-14 02:20:05 - training - INFO - Epoch [4/5][151/438] lr: 3.3e-06, eta: 5:05:44.000113, loss: 1.0159
2023-04-14 02:20:12 - training - INFO - Epoch [4/5][161/438] lr: 3.3e-06, eta: 4:46:55.979782, loss: 1.3077
2023-04-14 02:20:20 - training - INFO - Epoch [4/5][171/438] lr: 3.2e-06, eta: 4:30:20.270466, loss: 0.8391
2023-04-14 02:20:28 - training - INFO - Epoch [4/5][181/438] lr: 3.2e-06, eta: 4:15:37.985733, loss: 0.9477
2023-04-14 02:20:36 - training - INFO - Epoch [4/5][191/438] lr: 3.1e-06, eta: 4:02:25.279722, loss: 0.7905
2023-04-14 02:20:44 - training - INFO - Epoch [4/5][201/438] lr: 3.1e-06, eta: 3:50:30.306633, loss: 1.1516
2023-04-14 02:20:52 - training - INFO - Epoch [4/5][211/438] lr: 3.0e-06, eta: 3:39:43.150059, loss: 1.5068
2023-04-14 02:20:59 - training - INFO - Epoch [4/5][221/438] lr: 3.0e-06, eta: 3:29:51.695930, loss: 1.1458
2023-04-14 02:21:07 - training - INFO - Epoch [4/5][231/438] lr: 2.9e-06, eta: 3:20:50.647452, loss: 1.5919
2023-04-14 02:21:15 - training - INFO - Epoch [4/5][241/438] lr: 2.9e-06, eta: 3:12:33.406936, loss: 1.0916
2023-04-14 02:21:22 - training - INFO - Epoch [4/5][251/438] lr: 2.9e-06, eta: 3:04:55.403970, loss: 1.4814
2023-04-14 02:21:30 - training - INFO - Epoch [4/5][261/438] lr: 2.8e-06, eta: 2:57:51.118047, loss: 1.4051
2023-04-14 02:21:38 - training - INFO - Epoch [4/5][271/438] lr: 2.8e-06, eta: 2:51:19.620521, loss: 1.3140
2023-04-14 02:21:46 - training - INFO - Epoch [4/5][281/438] lr: 2.7e-06, eta: 2:45:16.481855, loss: 1.1543
2023-04-14 02:21:54 - training - INFO - Epoch [4/5][291/438] lr: 2.7e-06, eta: 2:39:36.478494, loss: 1.1405
2023-04-14 02:22:01 - training - INFO - Epoch [4/5][301/438] lr: 2.6e-06, eta: 2:34:18.162788, loss: 0.5450
2023-04-14 02:22:09 - training - INFO - Epoch [4/5][311/438] lr: 2.6e-06, eta: 2:29:19.519202, loss: 1.0388
2023-04-14 02:22:17 - training - INFO - Epoch [4/5][321/438] lr: 2.5e-06, eta: 2:24:39.107073, loss: 1.1064
2023-04-14 02:22:24 - training - INFO - Epoch [4/5][331/438] lr: 2.5e-06, eta: 2:20:15.142736, loss: 1.5499
2023-04-14 02:22:32 - training - INFO - Epoch [4/5][341/438] lr: 2.4e-06, eta: 2:16:05.936543, loss: 1.1201
2023-04-14 02:22:40 - training - INFO - Epoch [4/5][351/438] lr: 2.4e-06, eta: 2:12:12.776604, loss: 1.2051
2023-04-14 02:22:48 - training - INFO - Epoch [4/5][361/438] lr: 2.4e-06, eta: 2:08:30.048905, loss: 0.9925
2023-04-14 02:22:56 - training - INFO - Epoch [4/5][371/438] lr: 2.3e-06, eta: 2:04:59.467788, loss: 1.3001
2023-04-14 02:23:03 - training - INFO - Epoch [4/5][381/438] lr: 2.3e-06, eta: 2:01:39.052695, loss: 0.9936
2023-04-14 02:23:11 - training - INFO - Epoch [4/5][391/438] lr: 2.2e-06, eta: 1:58:28.931998, loss: 1.0643
2023-04-14 02:23:19 - training - INFO - Epoch [4/5][401/438] lr: 2.2e-06, eta: 1:55:27.133230, loss: 1.2621
2023-04-14 02:23:26 - training - INFO - Epoch [4/5][411/438] lr: 2.1e-06, eta: 1:52:34.060671, loss: 0.8136
2023-04-14 02:23:34 - training - INFO - Epoch [4/5][421/438] lr: 2.1e-06, eta: 1:49:49.348100, loss: 0.9204
2023-04-14 02:23:42 - training - INFO - Epoch [4/5][431/438] lr: 2.0e-06, eta: 1:47:11.702586, loss: 1.3573
2023-04-14 02:24:37 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 1.1066, Validation Metrics: {'exact_match': 71.24773960216999, 'f1': 75.76101830881191}, Test Metrics: {'exact_match': 76.93693693693693, 'f1': 81.99584599584601}
2023-04-14 02:24:37 - training - INFO - Epoch [5/5][1/438] lr: 2.0e-06, eta: 41 days, 7:51:38.266630, loss: 1.0238
2023-04-14 02:24:45 - training - INFO - Epoch [5/5][11/438] lr: 1.9e-06, eta: 3 days, 18:10:46.296677, loss: 1.2055
2023-04-14 02:24:53 - training - INFO - Epoch [5/5][21/438] lr: 1.9e-06, eta: 1 day, 23:14:44.061072, loss: 1.0304
2023-04-14 02:25:01 - training - INFO - Epoch [5/5][31/438] lr: 1.9e-06, eta: 1 day, 8:00:34.036359, loss: 0.8535
2023-04-14 02:25:09 - training - INFO - Epoch [5/5][41/438] lr: 1.8e-06, eta: 1 day, 0:12:34.040274, loss: 1.1416
2023-04-14 02:25:17 - training - INFO - Epoch [5/5][51/438] lr: 1.8e-06, eta: 19:27:45.627306, loss: 1.0686
2023-04-14 02:25:24 - training - INFO - Epoch [5/5][61/438] lr: 1.7e-06, eta: 16:16:11.446992, loss: 0.9660
2023-04-14 02:25:32 - training - INFO - Epoch [5/5][71/438] lr: 1.7e-06, eta: 13:58:46.805178, loss: 0.7652
2023-04-14 02:25:40 - training - INFO - Epoch [5/5][81/438] lr: 1.6e-06, eta: 12:15:07.581711, loss: 0.8345
2023-04-14 02:25:48 - training - INFO - Epoch [5/5][91/438] lr: 1.6e-06, eta: 10:54:14.253293, loss: 0.7845
2023-04-14 02:25:56 - training - INFO - Epoch [5/5][101/438] lr: 1.5e-06, eta: 9:49:21.614348, loss: 1.1404
2023-04-14 02:26:04 - training - INFO - Epoch [5/5][111/438] lr: 1.5e-06, eta: 8:56:09.772404, loss: 1.3185
2023-04-14 02:26:11 - training - INFO - Epoch [5/5][121/438] lr: 1.4e-06, eta: 8:11:42.413078, loss: 1.3172
2023-04-14 02:26:19 - training - INFO - Epoch [5/5][131/438] lr: 1.4e-06, eta: 7:33:59.725810, loss: 1.3885
2023-04-14 02:26:27 - training - INFO - Epoch [5/5][141/438] lr: 1.4e-06, eta: 7:01:36.230703, loss: 1.0083
2023-04-14 02:26:35 - training - INFO - Epoch [5/5][151/438] lr: 1.3e-06, eta: 6:33:35.814223, loss: 1.0508
2023-04-14 02:26:43 - training - INFO - Epoch [5/5][161/438] lr: 1.3e-06, eta: 6:08:56.596958, loss: 0.7681
2023-04-14 02:26:50 - training - INFO - Epoch [5/5][171/438] lr: 1.2e-06, eta: 5:47:11.498889, loss: 1.0607
2023-04-14 02:26:58 - training - INFO - Epoch [5/5][181/438] lr: 1.2e-06, eta: 5:27:49.697110, loss: 1.1636
2023-04-14 02:27:06 - training - INFO - Epoch [5/5][191/438] lr: 1.1e-06, eta: 5:10:27.731475, loss: 0.9619
2023-04-14 02:27:14 - training - INFO - Epoch [5/5][201/438] lr: 1.1e-06, eta: 4:54:48.228714, loss: 0.8776
2023-04-14 02:27:21 - training - INFO - Epoch [5/5][211/438] lr: 1.0e-06, eta: 4:40:36.338542, loss: 1.3347
2023-04-14 02:27:29 - training - INFO - Epoch [5/5][221/438] lr: 9.9e-07, eta: 4:27:44.622068, loss: 0.7448
2023-04-14 02:27:37 - training - INFO - Epoch [5/5][231/438] lr: 9.5e-07, eta: 4:15:57.202413, loss: 0.7178
2023-04-14 02:27:45 - training - INFO - Epoch [5/5][241/438] lr: 9.0e-07, eta: 4:05:07.536004, loss: 0.8929
2023-04-14 02:27:53 - training - INFO - Epoch [5/5][251/438] lr: 8.5e-07, eta: 3:55:10.159231, loss: 0.7074
2023-04-14 02:28:00 - training - INFO - Epoch [5/5][261/438] lr: 8.1e-07, eta: 3:45:55.877748, loss: 1.0354
2023-04-14 02:28:08 - training - INFO - Epoch [5/5][271/438] lr: 7.6e-07, eta: 3:37:23.076471, loss: 0.6080
2023-04-14 02:28:16 - training - INFO - Epoch [5/5][281/438] lr: 7.2e-07, eta: 3:29:26.612925, loss: 0.7709
2023-04-14 02:28:24 - training - INFO - Epoch [5/5][291/438] lr: 6.7e-07, eta: 3:22:01.497405, loss: 0.8255
2023-04-14 02:28:32 - training - INFO - Epoch [5/5][301/438] lr: 6.3e-07, eta: 3:15:06.983050, loss: 0.9094
2023-04-14 02:28:39 - training - INFO - Epoch [5/5][311/438] lr: 5.8e-07, eta: 3:08:37.666081, loss: 1.2767
2023-04-14 02:28:47 - training - INFO - Epoch [5/5][321/438] lr: 5.3e-07, eta: 3:02:31.627911, loss: 0.7338
2023-04-14 02:28:55 - training - INFO - Epoch [5/5][331/438] lr: 4.9e-07, eta: 2:56:46.624886, loss: 0.9033
2023-04-14 02:29:02 - training - INFO - Epoch [5/5][341/438] lr: 4.4e-07, eta: 2:51:21.440309, loss: 0.8019
2023-04-14 02:29:10 - training - INFO - Epoch [5/5][351/438] lr: 4.0e-07, eta: 2:46:15.087249, loss: 1.0467
2023-04-14 02:29:18 - training - INFO - Epoch [5/5][361/438] lr: 3.5e-07, eta: 2:41:26.288892, loss: 0.9001
2023-04-14 02:29:26 - training - INFO - Epoch [5/5][371/438] lr: 3.1e-07, eta: 2:36:51.627873, loss: 1.1309
2023-04-14 02:29:33 - training - INFO - Epoch [5/5][381/438] lr: 2.6e-07, eta: 2:32:31.240761, loss: 1.1268
2023-04-14 02:29:41 - training - INFO - Epoch [5/5][391/438] lr: 2.1e-07, eta: 2:28:24.276430, loss: 1.4486
2023-04-14 02:29:49 - training - INFO - Epoch [5/5][401/438] lr: 1.7e-07, eta: 2:24:28.812391, loss: 0.5992
2023-04-14 02:29:57 - training - INFO - Epoch [5/5][411/438] lr: 1.2e-07, eta: 2:20:43.173138, loss: 1.1397
2023-04-14 02:30:04 - training - INFO - Epoch [5/5][421/438] lr: 7.8e-08, eta: 2:17:08.705166, loss: 0.5187
2023-04-14 02:30:12 - training - INFO - Epoch [5/5][431/438] lr: 3.2e-08, eta: 2:13:43.773486, loss: 1.1144
2023-04-14 02:31:06 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 1.0350, Validation Metrics: {'exact_match': 71.79023508137432, 'f1': 76.16554948187836}, Test Metrics: {'exact_match': 78.1981981981982, 'f1': 82.73863273863276}
2023-04-14 02:31:30 - training - INFO - Final Test - Train Loss: 1.0350, Test Metrics: {'exact_match': 78.1981981981982, 'f1': 82.73863273863276}
