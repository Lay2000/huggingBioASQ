2023-04-14 01:23:34 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-44a167365c0b341b/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'xlnet-base-cased'}, 'data': {'task_type': 'list', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 4.54e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/xlnet_list_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 91.53it/s]
Map:   0%|          | 0/6878 [00:00<?, ? examples/s]Map:  15%|█▍        | 1000/6878 [00:00<00:04, 1380.18 examples/s]Map:  29%|██▉       | 2000/6878 [00:01<00:03, 1469.01 examples/s]Map:  44%|████▎     | 3000/6878 [00:02<00:02, 1465.62 examples/s]Map:  58%|█████▊    | 4000/6878 [00:02<00:01, 1492.58 examples/s]Map:  73%|███████▎  | 5000/6878 [00:03<00:01, 1497.34 examples/s]Map:  87%|████████▋ | 6000/6878 [00:04<00:00, 1518.93 examples/s]Map: 100%|██████████| 6878/6878 [00:04<00:00, 1389.86 examples/s]                                                                 Map:   0%|          | 0/859 [00:00<?, ? examples/s]Map: 100%|██████████| 859/859 [00:00<00:00, 1191.66 examples/s]                                                               Map:   0%|          | 0/861 [00:00<?, ? examples/s]Map: 100%|██████████| 861/861 [00:00<00:00, 1202.87 examples/s]                                                               Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForQuestionAnsweringSimple: ['lm_loss.weight', 'lm_loss.bias']
- This IS expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForQuestionAnsweringSimple were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-14 01:25:14 - training - INFO - First Test - Val Metrics:{'exact_match': 0.11641443538998836, 'f1': 3.5108483369545644} Test Metrics: {'exact_match': 0.8130081300813008, 'f1': 4.255093738648391}
2023-04-14 01:25:15 - training - INFO - Epoch [1/5][1/690] lr: 4.5e-05, eta: 3 days, 6:56:41.076592, loss: 7.2861
2023-04-14 01:25:23 - training - INFO - Epoch [1/5][11/690] lr: 4.5e-05, eta: 7:49:13.851521, loss: 5.2157
2023-04-14 01:25:30 - training - INFO - Epoch [1/5][21/690] lr: 4.5e-05, eta: 4:25:44.918580, loss: 4.1002
2023-04-14 01:25:38 - training - INFO - Epoch [1/5][31/690] lr: 4.5e-05, eta: 3:13:39.946741, loss: 4.4297
2023-04-14 01:25:46 - training - INFO - Epoch [1/5][41/690] lr: 4.5e-05, eta: 2:36:58.477243, loss: 4.1526
2023-04-14 01:25:54 - training - INFO - Epoch [1/5][51/690] lr: 4.5e-05, eta: 2:14:56.163075, loss: 4.0306
2023-04-14 01:26:02 - training - INFO - Epoch [1/5][61/690] lr: 4.5e-05, eta: 1:59:29.409166, loss: 3.5454
2023-04-14 01:26:09 - training - INFO - Epoch [1/5][71/690] lr: 4.4e-05, eta: 1:48:21.030429, loss: 2.4328
2023-04-14 01:26:17 - training - INFO - Epoch [1/5][81/690] lr: 4.4e-05, eta: 1:40:03.544524, loss: 2.6826
2023-04-14 01:26:25 - training - INFO - Epoch [1/5][91/690] lr: 4.4e-05, eta: 1:33:40.406442, loss: 3.4990
2023-04-14 01:26:33 - training - INFO - Epoch [1/5][101/690] lr: 4.4e-05, eta: 1:28:21.644497, loss: 2.0009
2023-04-14 01:26:40 - training - INFO - Epoch [1/5][111/690] lr: 4.4e-05, eta: 1:23:54.934863, loss: 3.2359
2023-04-14 01:26:48 - training - INFO - Epoch [1/5][121/690] lr: 4.4e-05, eta: 1:20:22.339465, loss: 2.7177
2023-04-14 01:26:56 - training - INFO - Epoch [1/5][131/690] lr: 4.4e-05, eta: 1:17:20.814983, loss: 2.3174
2023-04-14 01:27:04 - training - INFO - Epoch [1/5][141/690] lr: 4.4e-05, eta: 1:14:44.049063, loss: 2.4546
2023-04-14 01:27:11 - training - INFO - Epoch [1/5][151/690] lr: 4.3e-05, eta: 1:12:20.741725, loss: 2.1865
2023-04-14 01:27:19 - training - INFO - Epoch [1/5][161/690] lr: 4.3e-05, eta: 1:10:13.666171, loss: 3.7791
2023-04-14 01:27:27 - training - INFO - Epoch [1/5][171/690] lr: 4.3e-05, eta: 1:08:24.940752, loss: 2.9272
2023-04-14 01:27:34 - training - INFO - Epoch [1/5][181/690] lr: 4.3e-05, eta: 1:06:45.172262, loss: 2.1065
2023-04-14 01:27:42 - training - INFO - Epoch [1/5][191/690] lr: 4.3e-05, eta: 1:05:17.946987, loss: 2.4158
2023-04-14 01:27:50 - training - INFO - Epoch [1/5][201/690] lr: 4.3e-05, eta: 1:03:55.269054, loss: 2.5992
2023-04-14 01:27:57 - training - INFO - Epoch [1/5][211/690] lr: 4.3e-05, eta: 1:02:38.357455, loss: 2.1258
2023-04-14 01:28:05 - training - INFO - Epoch [1/5][221/690] lr: 4.2e-05, eta: 1:01:32.807102, loss: 2.4631
2023-04-14 01:28:13 - training - INFO - Epoch [1/5][231/690] lr: 4.2e-05, eta: 1:00:28.302288, loss: 2.7905
2023-04-14 01:28:21 - training - INFO - Epoch [1/5][241/690] lr: 4.2e-05, eta: 0:59:30.923856, loss: 2.5839
2023-04-14 01:28:28 - training - INFO - Epoch [1/5][251/690] lr: 4.2e-05, eta: 0:58:34.760494, loss: 2.0496
2023-04-14 01:28:36 - training - INFO - Epoch [1/5][261/690] lr: 4.2e-05, eta: 0:57:43.547388, loss: 2.0065
2023-04-14 01:28:44 - training - INFO - Epoch [1/5][271/690] lr: 4.2e-05, eta: 0:56:54.913590, loss: 1.3956
2023-04-14 01:28:51 - training - INFO - Epoch [1/5][281/690] lr: 4.2e-05, eta: 0:56:08.114608, loss: 2.4764
2023-04-14 01:28:59 - training - INFO - Epoch [1/5][291/690] lr: 4.2e-05, eta: 0:55:28.742547, loss: 2.6532
2023-04-14 01:29:07 - training - INFO - Epoch [1/5][301/690] lr: 4.1e-05, eta: 0:54:48.976199, loss: 2.0240
2023-04-14 01:29:15 - training - INFO - Epoch [1/5][311/690] lr: 4.1e-05, eta: 0:54:10.108044, loss: 1.6717
2023-04-14 01:29:22 - training - INFO - Epoch [1/5][321/690] lr: 4.1e-05, eta: 0:53:32.907264, loss: 2.4478
2023-04-14 01:29:30 - training - INFO - Epoch [1/5][331/690] lr: 4.1e-05, eta: 0:53:01.925825, loss: 2.0390
2023-04-14 01:29:38 - training - INFO - Epoch [1/5][341/690] lr: 4.1e-05, eta: 0:52:27.144321, loss: 1.6789
2023-04-14 01:29:45 - training - INFO - Epoch [1/5][351/690] lr: 4.1e-05, eta: 0:51:54.656148, loss: 1.4126
2023-04-14 01:29:53 - training - INFO - Epoch [1/5][361/690] lr: 4.1e-05, eta: 0:51:24.595086, loss: 2.4422
2023-04-14 01:30:01 - training - INFO - Epoch [1/5][371/690] lr: 4.1e-05, eta: 0:50:55.581126, loss: 2.6397
2023-04-14 01:30:08 - training - INFO - Epoch [1/5][381/690] lr: 4.0e-05, eta: 0:50:26.822733, loss: 1.9983
2023-04-14 01:30:16 - training - INFO - Epoch [1/5][391/690] lr: 4.0e-05, eta: 0:50:00.931003, loss: 2.2304
2023-04-14 01:30:24 - training - INFO - Epoch [1/5][401/690] lr: 4.0e-05, eta: 0:49:38.101603, loss: 2.1249
2023-04-14 01:30:32 - training - INFO - Epoch [1/5][411/690] lr: 4.0e-05, eta: 0:49:14.558346, loss: 2.2928
2023-04-14 01:30:40 - training - INFO - Epoch [1/5][421/690] lr: 4.0e-05, eta: 0:48:49.130841, loss: 1.4468
2023-04-14 01:30:47 - training - INFO - Epoch [1/5][431/690] lr: 4.0e-05, eta: 0:48:24.896895, loss: 2.1240
2023-04-14 01:30:55 - training - INFO - Epoch [1/5][441/690] lr: 4.0e-05, eta: 0:48:02.670144, loss: 3.0250
2023-04-14 01:31:03 - training - INFO - Epoch [1/5][451/690] lr: 3.9e-05, eta: 0:47:41.483854, loss: 1.9890
2023-04-14 01:31:11 - training - INFO - Epoch [1/5][461/690] lr: 3.9e-05, eta: 0:47:20.365997, loss: 2.2656
2023-04-14 01:31:18 - training - INFO - Epoch [1/5][471/690] lr: 3.9e-05, eta: 0:46:59.027700, loss: 2.0811
2023-04-14 01:31:26 - training - INFO - Epoch [1/5][481/690] lr: 3.9e-05, eta: 0:46:38.466578, loss: 2.3448
2023-04-14 01:31:34 - training - INFO - Epoch [1/5][491/690] lr: 3.9e-05, eta: 0:46:17.616259, loss: 2.2775
2023-04-14 01:31:42 - training - INFO - Epoch [1/5][501/690] lr: 3.9e-05, eta: 0:45:59.954355, loss: 2.2026
2023-04-14 01:31:49 - training - INFO - Epoch [1/5][511/690] lr: 3.9e-05, eta: 0:45:39.826909, loss: 1.7662
2023-04-14 01:31:57 - training - INFO - Epoch [1/5][521/690] lr: 3.9e-05, eta: 0:45:22.124730, loss: 1.6406
2023-04-14 01:32:04 - training - INFO - Epoch [1/5][531/690] lr: 3.8e-05, eta: 0:45:03.405579, loss: 1.6519
2023-04-14 01:32:12 - training - INFO - Epoch [1/5][541/690] lr: 3.8e-05, eta: 0:44:45.312445, loss: 2.1356
2023-04-14 01:32:20 - training - INFO - Epoch [1/5][551/690] lr: 3.8e-05, eta: 0:44:29.465877, loss: 2.2146
2023-04-14 01:32:28 - training - INFO - Epoch [1/5][561/690] lr: 3.8e-05, eta: 0:44:12.497793, loss: 1.9742
2023-04-14 01:32:35 - training - INFO - Epoch [1/5][571/690] lr: 3.8e-05, eta: 0:43:55.295529, loss: 1.3549
2023-04-14 01:32:43 - training - INFO - Epoch [1/5][581/690] lr: 3.8e-05, eta: 0:43:38.114557, loss: 1.7176
2023-04-14 01:32:50 - training - INFO - Epoch [1/5][591/690] lr: 3.8e-05, eta: 0:43:21.878694, loss: 2.0651
2023-04-14 01:32:58 - training - INFO - Epoch [1/5][601/690] lr: 3.7e-05, eta: 0:43:07.242427, loss: 1.3949
2023-04-14 01:33:06 - training - INFO - Epoch [1/5][611/690] lr: 3.7e-05, eta: 0:42:51.421411, loss: 2.0679
2023-04-14 01:33:14 - training - INFO - Epoch [1/5][621/690] lr: 3.7e-05, eta: 0:42:35.953407, loss: 1.8194
2023-04-14 01:33:21 - training - INFO - Epoch [1/5][631/690] lr: 3.7e-05, eta: 0:42:20.702682, loss: 1.6035
2023-04-14 01:33:29 - training - INFO - Epoch [1/5][641/690] lr: 3.7e-05, eta: 0:42:07.150558, loss: 2.4115
2023-04-14 01:33:37 - training - INFO - Epoch [1/5][651/690] lr: 3.7e-05, eta: 0:41:51.872982, loss: 2.0191
2023-04-14 01:33:45 - training - INFO - Epoch [1/5][661/690] lr: 3.7e-05, eta: 0:41:38.193759, loss: 2.5356
2023-04-14 01:33:53 - training - INFO - Epoch [1/5][671/690] lr: 3.7e-05, eta: 0:41:24.851187, loss: 1.1886
2023-04-14 01:34:01 - training - INFO - Epoch [1/5][681/690] lr: 3.6e-05, eta: 0:41:12.110589, loss: 2.3181
2023-04-14 01:35:24 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 2.3519, Validation Metrics: {'exact_match': 33.527357392316645, 'f1': 41.921542205730375}, Test Metrics: {'exact_match': 37.86295005807201, 'f1': 46.027736823629}
2023-04-14 01:35:24 - training - INFO - Epoch [2/5][1/690] lr: 3.6e-05, eta: 27 days, 14:40:03.923365, loss: 1.2466
2023-04-14 01:35:32 - training - INFO - Epoch [2/5][11/690] lr: 3.6e-05, eta: 2 days, 12:44:41.759396, loss: 1.2838
2023-04-14 01:35:40 - training - INFO - Epoch [2/5][21/690] lr: 3.6e-05, eta: 1 day, 8:04:30.488007, loss: 1.6476
2023-04-14 01:35:48 - training - INFO - Epoch [2/5][31/690] lr: 3.6e-05, eta: 21:54:08.167697, loss: 0.6081
2023-04-14 01:35:55 - training - INFO - Epoch [2/5][41/690] lr: 3.6e-05, eta: 16:41:14.434336, loss: 2.4694
2023-04-14 01:36:03 - training - INFO - Epoch [2/5][51/690] lr: 3.6e-05, eta: 13:31:13.319706, loss: 1.8204
2023-04-14 01:36:11 - training - INFO - Epoch [2/5][61/690] lr: 3.6e-05, eta: 11:23:17.580250, loss: 0.9425
2023-04-14 01:36:18 - training - INFO - Epoch [2/5][71/690] lr: 3.5e-05, eta: 9:51:22.010597, loss: 1.7320
2023-04-14 01:36:26 - training - INFO - Epoch [2/5][81/690] lr: 3.5e-05, eta: 8:42:15.864084, loss: 2.4935
2023-04-14 01:36:34 - training - INFO - Epoch [2/5][91/690] lr: 3.5e-05, eta: 7:48:09.247856, loss: 1.9117
2023-04-14 01:36:41 - training - INFO - Epoch [2/5][101/690] lr: 3.5e-05, eta: 7:04:46.084242, loss: 1.2357
2023-04-14 01:36:49 - training - INFO - Epoch [2/5][111/690] lr: 3.5e-05, eta: 6:29:09.112794, loss: 1.4082
2023-04-14 01:36:57 - training - INFO - Epoch [2/5][121/690] lr: 3.5e-05, eta: 5:59:40.162604, loss: 1.5477
2023-04-14 01:37:05 - training - INFO - Epoch [2/5][131/690] lr: 3.5e-05, eta: 5:34:25.864164, loss: 1.6652
2023-04-14 01:37:12 - training - INFO - Epoch [2/5][141/690] lr: 3.4e-05, eta: 5:12:45.497832, loss: 1.6030
2023-04-14 01:37:20 - training - INFO - Epoch [2/5][151/690] lr: 3.4e-05, eta: 4:53:57.978138, loss: 1.2118
2023-04-14 01:37:28 - training - INFO - Epoch [2/5][161/690] lr: 3.4e-05, eta: 4:37:31.355149, loss: 1.8527
2023-04-14 01:37:35 - training - INFO - Epoch [2/5][171/690] lr: 3.4e-05, eta: 4:22:56.065797, loss: 1.4367
2023-04-14 01:37:43 - training - INFO - Epoch [2/5][181/690] lr: 3.4e-05, eta: 4:10:04.157539, loss: 0.9745
2023-04-14 01:37:51 - training - INFO - Epoch [2/5][191/690] lr: 3.4e-05, eta: 3:58:23.086164, loss: 1.5692
2023-04-14 01:37:59 - training - INFO - Epoch [2/5][201/690] lr: 3.4e-05, eta: 3:47:57.880626, loss: 0.9068
2023-04-14 01:38:07 - training - INFO - Epoch [2/5][211/690] lr: 3.4e-05, eta: 3:38:28.323692, loss: 1.7468
2023-04-14 01:38:14 - training - INFO - Epoch [2/5][221/690] lr: 3.3e-05, eta: 3:29:51.075417, loss: 1.6721
2023-04-14 01:38:22 - training - INFO - Epoch [2/5][231/690] lr: 3.3e-05, eta: 3:21:58.907295, loss: 1.4421
2023-04-14 01:38:30 - training - INFO - Epoch [2/5][241/690] lr: 3.3e-05, eta: 3:14:43.737952, loss: 1.3999
2023-04-14 01:38:38 - training - INFO - Epoch [2/5][251/690] lr: 3.3e-05, eta: 3:08:06.702203, loss: 1.6114
2023-04-14 01:38:46 - training - INFO - Epoch [2/5][261/690] lr: 3.3e-05, eta: 3:01:56.868621, loss: 1.7897
2023-04-14 01:38:54 - training - INFO - Epoch [2/5][271/690] lr: 3.3e-05, eta: 2:56:16.005286, loss: 1.2142
2023-04-14 01:39:02 - training - INFO - Epoch [2/5][281/690] lr: 3.3e-05, eta: 2:50:58.991024, loss: 1.4060
2023-04-14 01:39:10 - training - INFO - Epoch [2/5][291/690] lr: 3.2e-05, eta: 2:46:00.927210, loss: 1.5926
2023-04-14 01:39:18 - training - INFO - Epoch [2/5][301/690] lr: 3.2e-05, eta: 2:41:18.539672, loss: 1.4194
2023-04-14 01:39:26 - training - INFO - Epoch [2/5][311/690] lr: 3.2e-05, eta: 2:36:56.237223, loss: 1.1978
2023-04-14 01:39:33 - training - INFO - Epoch [2/5][321/690] lr: 3.2e-05, eta: 2:32:48.470640, loss: 1.9056
2023-04-14 01:39:41 - training - INFO - Epoch [2/5][331/690] lr: 3.2e-05, eta: 2:28:54.085433, loss: 1.3161
2023-04-14 01:39:48 - training - INFO - Epoch [2/5][341/690] lr: 3.2e-05, eta: 2:25:14.859663, loss: 1.6195
2023-04-14 01:39:56 - training - INFO - Epoch [2/5][351/690] lr: 3.2e-05, eta: 2:21:48.233223, loss: 1.2492
2023-04-14 01:40:04 - training - INFO - Epoch [2/5][361/690] lr: 3.2e-05, eta: 2:18:31.822509, loss: 1.9725
2023-04-14 01:40:12 - training - INFO - Epoch [2/5][371/690] lr: 3.1e-05, eta: 2:15:26.333883, loss: 1.4566
2023-04-14 01:40:20 - training - INFO - Epoch [2/5][381/690] lr: 3.1e-05, eta: 2:12:29.725839, loss: 1.5573
2023-04-14 01:40:28 - training - INFO - Epoch [2/5][391/690] lr: 3.1e-05, eta: 2:09:45.806567, loss: 1.5852
2023-04-14 01:40:36 - training - INFO - Epoch [2/5][401/690] lr: 3.1e-05, eta: 2:07:06.469798, loss: 1.1054
2023-04-14 01:40:43 - training - INFO - Epoch [2/5][411/690] lr: 3.1e-05, eta: 2:04:33.223134, loss: 1.7826
2023-04-14 01:40:51 - training - INFO - Epoch [2/5][421/690] lr: 3.1e-05, eta: 2:02:08.438325, loss: 0.9772
2023-04-14 01:40:59 - training - INFO - Epoch [2/5][431/690] lr: 3.1e-05, eta: 1:59:49.060168, loss: 0.9873
2023-04-14 01:41:07 - training - INFO - Epoch [2/5][441/690] lr: 3.1e-05, eta: 1:57:35.629578, loss: 1.2640
2023-04-14 01:41:14 - training - INFO - Epoch [2/5][451/690] lr: 3.0e-05, eta: 1:55:27.111193, loss: 1.1084
2023-04-14 01:41:22 - training - INFO - Epoch [2/5][461/690] lr: 3.0e-05, eta: 1:53:23.923469, loss: 1.7779
2023-04-14 01:41:31 - training - INFO - Epoch [2/5][471/690] lr: 3.0e-05, eta: 1:51:30.914433, loss: 1.6078
2023-04-14 01:41:38 - training - INFO - Epoch [2/5][481/690] lr: 3.0e-05, eta: 1:49:38.576595, loss: 2.1622
2023-04-14 01:41:46 - training - INFO - Epoch [2/5][491/690] lr: 3.0e-05, eta: 1:47:48.669900, loss: 1.2047
2023-04-14 01:41:54 - training - INFO - Epoch [2/5][501/690] lr: 3.0e-05, eta: 1:46:03.473109, loss: 1.3818
2023-04-14 01:42:01 - training - INFO - Epoch [2/5][511/690] lr: 3.0e-05, eta: 1:44:22.056764, loss: 1.6945
2023-04-14 01:42:09 - training - INFO - Epoch [2/5][521/690] lr: 2.9e-05, eta: 1:42:43.711446, loss: 2.0388
2023-04-14 01:42:17 - training - INFO - Epoch [2/5][531/690] lr: 2.9e-05, eta: 1:41:09.809466, loss: 1.9919
2023-04-14 01:42:25 - training - INFO - Epoch [2/5][541/690] lr: 2.9e-05, eta: 1:39:39.978938, loss: 1.4041
2023-04-14 01:42:32 - training - INFO - Epoch [2/5][551/690] lr: 2.9e-05, eta: 1:38:10.933243, loss: 1.0510
2023-04-14 01:42:40 - training - INFO - Epoch [2/5][561/690] lr: 2.9e-05, eta: 1:36:45.907740, loss: 1.3232
2023-04-14 01:42:48 - training - INFO - Epoch [2/5][571/690] lr: 2.9e-05, eta: 1:35:22.536478, loss: 0.9749
2023-04-14 01:42:55 - training - INFO - Epoch [2/5][581/690] lr: 2.9e-05, eta: 1:34:02.568453, loss: 1.1292
2023-04-14 01:43:03 - training - INFO - Epoch [2/5][591/690] lr: 2.9e-05, eta: 1:32:44.731869, loss: 1.8501
2023-04-14 01:43:11 - training - INFO - Epoch [2/5][601/690] lr: 2.8e-05, eta: 1:31:29.481690, loss: 1.4500
2023-04-14 01:43:19 - training - INFO - Epoch [2/5][611/690] lr: 2.8e-05, eta: 1:30:17.391156, loss: 1.5199
2023-04-14 01:43:26 - training - INFO - Epoch [2/5][621/690] lr: 2.8e-05, eta: 1:29:05.785902, loss: 1.9299
2023-04-14 01:43:34 - training - INFO - Epoch [2/5][631/690] lr: 2.8e-05, eta: 1:27:56.928385, loss: 1.9470
2023-04-14 01:43:42 - training - INFO - Epoch [2/5][641/690] lr: 2.8e-05, eta: 1:26:50.751180, loss: 1.2020
2023-04-14 01:43:49 - training - INFO - Epoch [2/5][651/690] lr: 2.8e-05, eta: 1:25:45.258951, loss: 1.5925
2023-04-14 01:43:57 - training - INFO - Epoch [2/5][661/690] lr: 2.8e-05, eta: 1:24:42.001451, loss: 1.6459
2023-04-14 01:44:05 - training - INFO - Epoch [2/5][671/690] lr: 2.7e-05, eta: 1:23:40.483041, loss: 1.5876
2023-04-14 01:44:13 - training - INFO - Epoch [2/5][681/690] lr: 2.7e-05, eta: 1:22:40.311837, loss: 1.4755
2023-04-14 01:45:36 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.5308, Validation Metrics: {'exact_match': 34.458672875436555, 'f1': 41.700309608444925}, Test Metrics: {'exact_match': 38.095238095238095, 'f1': 45.304873858880846}
2023-04-14 01:45:36 - training - INFO - Epoch [3/5][1/690] lr: 2.7e-05, eta: 52 days, 1:04:02.396643, loss: 1.1827
2023-04-14 01:45:44 - training - INFO - Epoch [3/5][11/690] lr: 2.7e-05, eta: 4 days, 17:52:44.053189, loss: 1.4068
2023-04-14 01:45:52 - training - INFO - Epoch [3/5][21/690] lr: 2.7e-05, eta: 2 days, 11:49:34.224699, loss: 1.6327
2023-04-14 01:45:59 - training - INFO - Epoch [3/5][31/690] lr: 2.7e-05, eta: 1 day, 16:38:25.020602, loss: 0.9788
2023-04-14 01:46:07 - training - INFO - Epoch [3/5][41/690] lr: 2.7e-05, eta: 1 day, 6:48:50.506547, loss: 1.1571
2023-04-14 01:46:15 - training - INFO - Epoch [3/5][51/690] lr: 2.7e-05, eta: 1 day, 0:50:39.239802, loss: 1.1795
2023-04-14 01:46:22 - training - INFO - Epoch [3/5][61/690] lr: 2.6e-05, eta: 20:49:50.161891, loss: 1.1949
2023-04-14 01:46:30 - training - INFO - Epoch [3/5][71/690] lr: 2.6e-05, eta: 17:56:39.840265, loss: 1.1007
2023-04-14 01:46:38 - training - INFO - Epoch [3/5][81/690] lr: 2.6e-05, eta: 15:46:10.685469, loss: 1.2884
2023-04-14 01:46:45 - training - INFO - Epoch [3/5][91/690] lr: 2.6e-05, eta: 14:04:22.030166, loss: 1.1113
2023-04-14 01:46:53 - training - INFO - Epoch [3/5][101/690] lr: 2.6e-05, eta: 12:42:43.659677, loss: 1.0920
2023-04-14 01:47:01 - training - INFO - Epoch [3/5][111/690] lr: 2.6e-05, eta: 11:35:56.041467, loss: 1.1146
2023-04-14 01:47:08 - training - INFO - Epoch [3/5][121/690] lr: 2.6e-05, eta: 10:39:59.572243, loss: 0.8997
2023-04-14 01:47:16 - training - INFO - Epoch [3/5][131/690] lr: 2.6e-05, eta: 9:52:33.005197, loss: 1.5091
2023-04-14 01:47:24 - training - INFO - Epoch [3/5][141/690] lr: 2.5e-05, eta: 9:11:55.247670, loss: 1.6925
2023-04-14 01:47:31 - training - INFO - Epoch [3/5][151/690] lr: 2.5e-05, eta: 8:36:32.425809, loss: 1.6465
2023-04-14 01:47:39 - training - INFO - Epoch [3/5][161/690] lr: 2.5e-05, eta: 8:05:32.639822, loss: 1.6228
2023-04-14 01:47:46 - training - INFO - Epoch [3/5][171/690] lr: 2.5e-05, eta: 7:38:12.221349, loss: 1.6853
2023-04-14 01:47:54 - training - INFO - Epoch [3/5][181/690] lr: 2.5e-05, eta: 7:13:55.175747, loss: 1.1272
2023-04-14 01:48:02 - training - INFO - Epoch [3/5][191/690] lr: 2.5e-05, eta: 6:52:07.871076, loss: 0.9786
2023-04-14 01:48:10 - training - INFO - Epoch [3/5][201/690] lr: 2.5e-05, eta: 6:32:30.707898, loss: 1.0047
2023-04-14 01:48:17 - training - INFO - Epoch [3/5][211/690] lr: 2.4e-05, eta: 6:14:45.393881, loss: 0.7561
2023-04-14 01:48:25 - training - INFO - Epoch [3/5][221/690] lr: 2.4e-05, eta: 5:58:39.073135, loss: 1.6821
2023-04-14 01:48:33 - training - INFO - Epoch [3/5][231/690] lr: 2.4e-05, eta: 5:43:50.561343, loss: 1.1863
2023-04-14 01:48:41 - training - INFO - Epoch [3/5][241/690] lr: 2.4e-05, eta: 5:30:19.589459, loss: 1.0279
2023-04-14 01:48:49 - training - INFO - Epoch [3/5][251/690] lr: 2.4e-05, eta: 5:17:47.415570, loss: 1.2626
2023-04-14 01:48:57 - training - INFO - Epoch [3/5][261/690] lr: 2.4e-05, eta: 5:06:14.896818, loss: 1.2485
2023-04-14 01:49:04 - training - INFO - Epoch [3/5][271/690] lr: 2.4e-05, eta: 4:55:31.702219, loss: 2.0297
2023-04-14 01:49:12 - training - INFO - Epoch [3/5][281/690] lr: 2.4e-05, eta: 4:45:33.772089, loss: 1.6083
2023-04-14 01:49:20 - training - INFO - Epoch [3/5][291/690] lr: 2.3e-05, eta: 4:36:15.111891, loss: 1.4868
2023-04-14 01:49:27 - training - INFO - Epoch [3/5][301/690] lr: 2.3e-05, eta: 4:27:33.784642, loss: 1.5834
2023-04-14 01:49:35 - training - INFO - Epoch [3/5][311/690] lr: 2.3e-05, eta: 4:19:24.574550, loss: 1.3781
2023-04-14 01:49:42 - training - INFO - Epoch [3/5][321/690] lr: 2.3e-05, eta: 4:11:47.440929, loss: 1.0566
2023-04-14 01:49:50 - training - INFO - Epoch [3/5][331/690] lr: 2.3e-05, eta: 4:04:37.009682, loss: 0.9350
2023-04-14 01:49:58 - training - INFO - Epoch [3/5][341/690] lr: 2.3e-05, eta: 3:57:52.874925, loss: 1.7793
2023-04-14 01:50:06 - training - INFO - Epoch [3/5][351/690] lr: 2.3e-05, eta: 3:51:29.178774, loss: 1.0431
2023-04-14 01:50:13 - training - INFO - Epoch [3/5][361/690] lr: 2.2e-05, eta: 3:45:26.721733, loss: 1.0813
2023-04-14 01:50:21 - training - INFO - Epoch [3/5][371/690] lr: 2.2e-05, eta: 3:39:42.279729, loss: 1.7387
2023-04-14 01:50:29 - training - INFO - Epoch [3/5][381/690] lr: 2.2e-05, eta: 3:34:17.741226, loss: 1.3488
2023-04-14 01:50:37 - training - INFO - Epoch [3/5][391/690] lr: 2.2e-05, eta: 3:29:07.959879, loss: 0.8615
2023-04-14 01:50:44 - training - INFO - Epoch [3/5][401/690] lr: 2.2e-05, eta: 3:24:13.528532, loss: 1.1209
2023-04-14 01:50:52 - training - INFO - Epoch [3/5][411/690] lr: 2.2e-05, eta: 3:19:33.988212, loss: 1.5429
2023-04-14 01:51:00 - training - INFO - Epoch [3/5][421/690] lr: 2.2e-05, eta: 3:15:06.391359, loss: 1.2700
2023-04-14 01:51:07 - training - INFO - Epoch [3/5][431/690] lr: 2.2e-05, eta: 3:10:50.079787, loss: 1.4274
2023-04-14 01:51:15 - training - INFO - Epoch [3/5][441/690] lr: 2.1e-05, eta: 3:06:46.870050, loss: 1.6055
2023-04-14 01:51:23 - training - INFO - Epoch [3/5][451/690] lr: 2.1e-05, eta: 3:02:53.119074, loss: 1.0449
2023-04-14 01:51:31 - training - INFO - Epoch [3/5][461/690] lr: 2.1e-05, eta: 2:59:09.382546, loss: 1.6731
2023-04-14 01:51:39 - training - INFO - Epoch [3/5][471/690] lr: 2.1e-05, eta: 2:55:36.332751, loss: 1.3785
2023-04-14 01:51:46 - training - INFO - Epoch [3/5][481/690] lr: 2.1e-05, eta: 2:52:10.873020, loss: 1.5691
2023-04-14 01:51:54 - training - INFO - Epoch [3/5][491/690] lr: 2.1e-05, eta: 2:48:51.843843, loss: 1.0237
2023-04-14 01:52:02 - training - INFO - Epoch [3/5][501/690] lr: 2.1e-05, eta: 2:45:41.957802, loss: 1.4047
2023-04-14 01:52:09 - training - INFO - Epoch [3/5][511/690] lr: 2.1e-05, eta: 2:42:37.671035, loss: 1.1807
2023-04-14 01:52:17 - training - INFO - Epoch [3/5][521/690] lr: 2.0e-05, eta: 2:39:40.161484, loss: 1.6210
2023-04-14 01:52:25 - training - INFO - Epoch [3/5][531/690] lr: 2.0e-05, eta: 2:36:50.508639, loss: 1.4176
2023-04-14 01:52:32 - training - INFO - Epoch [3/5][541/690] lr: 2.0e-05, eta: 2:34:05.092900, loss: 1.1697
2023-04-14 01:52:40 - training - INFO - Epoch [3/5][551/690] lr: 2.0e-05, eta: 2:31:28.315717, loss: 1.3013
2023-04-14 01:52:48 - training - INFO - Epoch [3/5][561/690] lr: 2.0e-05, eta: 2:28:54.628293, loss: 1.1422
2023-04-14 01:52:55 - training - INFO - Epoch [3/5][571/690] lr: 2.0e-05, eta: 2:26:26.555413, loss: 1.3158
2023-04-14 01:53:03 - training - INFO - Epoch [3/5][581/690] lr: 2.0e-05, eta: 2:24:03.255553, loss: 0.6300
2023-04-14 01:53:11 - training - INFO - Epoch [3/5][591/690] lr: 1.9e-05, eta: 2:21:45.061842, loss: 1.0511
2023-04-14 01:53:18 - training - INFO - Epoch [3/5][601/690] lr: 1.9e-05, eta: 2:19:29.786502, loss: 1.3602
2023-04-14 01:53:26 - training - INFO - Epoch [3/5][611/690] lr: 1.9e-05, eta: 2:17:20.166271, loss: 1.0349
2023-04-14 01:53:34 - training - INFO - Epoch [3/5][621/690] lr: 1.9e-05, eta: 2:15:13.487130, loss: 1.1659
2023-04-14 01:53:41 - training - INFO - Epoch [3/5][631/690] lr: 1.9e-05, eta: 2:13:10.455500, loss: 1.2398
2023-04-14 01:53:49 - training - INFO - Epoch [3/5][641/690] lr: 1.9e-05, eta: 2:11:11.219687, loss: 1.0978
2023-04-14 01:53:57 - training - INFO - Epoch [3/5][651/690] lr: 1.9e-05, eta: 2:09:18.427743, loss: 1.1031
2023-04-14 01:54:05 - training - INFO - Epoch [3/5][661/690] lr: 1.9e-05, eta: 2:07:25.376929, loss: 1.4088
2023-04-14 01:54:12 - training - INFO - Epoch [3/5][671/690] lr: 1.8e-05, eta: 2:05:36.289509, loss: 1.4172
2023-04-14 01:54:20 - training - INFO - Epoch [3/5][681/690] lr: 1.8e-05, eta: 2:03:49.442982, loss: 1.0815
2023-04-14 01:55:43 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 1.2596, Validation Metrics: {'exact_match': 35.62281722933644, 'f1': 41.63369372559564}, Test Metrics: {'exact_match': 39.83739837398374, 'f1': 45.49619873982611}
2023-04-14 01:55:44 - training - INFO - Epoch [4/5][1/690] lr: 1.8e-05, eta: 76 days, 6:49:51.976334, loss: 1.2156
2023-04-14 01:55:51 - training - INFO - Epoch [4/5][11/690] lr: 1.8e-05, eta: 6 days, 22:37:28.080533, loss: 0.8047
2023-04-14 01:55:59 - training - INFO - Epoch [4/5][21/690] lr: 1.8e-05, eta: 3 days, 15:23:02.577336, loss: 0.9602
2023-04-14 01:56:07 - training - INFO - Epoch [4/5][31/690] lr: 1.8e-05, eta: 2 days, 11:15:20.596853, loss: 0.9999
2023-04-14 01:56:14 - training - INFO - Epoch [4/5][41/690] lr: 1.8e-05, eta: 1 day, 20:50:50.584309, loss: 1.2154
2023-04-14 01:56:23 - training - INFO - Epoch [4/5][51/690] lr: 1.7e-05, eta: 1 day, 12:05:59.966235, loss: 0.8406
2023-04-14 01:56:30 - training - INFO - Epoch [4/5][61/690] lr: 1.7e-05, eta: 1 day, 6:12:42.594092, loss: 0.9305
2023-04-14 01:56:38 - training - INFO - Epoch [4/5][71/690] lr: 1.7e-05, eta: 1 day, 1:58:46.266478, loss: 0.6930
2023-04-14 01:56:45 - training - INFO - Epoch [4/5][81/690] lr: 1.7e-05, eta: 22:47:33.797415, loss: 1.4027
2023-04-14 01:56:53 - training - INFO - Epoch [4/5][91/690] lr: 1.7e-05, eta: 20:18:31.056839, loss: 1.1595
2023-04-14 01:57:01 - training - INFO - Epoch [4/5][101/690] lr: 1.7e-05, eta: 18:19:01.408120, loss: 1.1820
2023-04-14 01:57:09 - training - INFO - Epoch [4/5][111/690] lr: 1.7e-05, eta: 16:40:50.101923, loss: 1.0982
2023-04-14 01:57:17 - training - INFO - Epoch [4/5][121/690] lr: 1.7e-05, eta: 15:18:55.237558, loss: 1.1324
2023-04-14 01:57:24 - training - INFO - Epoch [4/5][131/690] lr: 1.6e-05, eta: 14:09:31.097754, loss: 1.3807
2023-04-14 01:57:32 - training - INFO - Epoch [4/5][141/690] lr: 1.6e-05, eta: 13:09:52.993668, loss: 1.5180
2023-04-14 01:57:40 - training - INFO - Epoch [4/5][151/690] lr: 1.6e-05, eta: 12:18:09.147578, loss: 1.1477
2023-04-14 01:57:48 - training - INFO - Epoch [4/5][161/690] lr: 1.6e-05, eta: 11:32:54.653835, loss: 0.9710
2023-04-14 01:57:56 - training - INFO - Epoch [4/5][171/690] lr: 1.6e-05, eta: 10:52:54.799941, loss: 1.0689
2023-04-14 01:58:03 - training - INFO - Epoch [4/5][181/690] lr: 1.6e-05, eta: 10:17:13.991036, loss: 0.9378
2023-04-14 01:58:11 - training - INFO - Epoch [4/5][191/690] lr: 1.6e-05, eta: 9:45:18.322423, loss: 1.1117
2023-04-14 01:58:18 - training - INFO - Epoch [4/5][201/690] lr: 1.6e-05, eta: 9:16:30.691029, loss: 0.8319
2023-04-14 01:58:27 - training - INFO - Epoch [4/5][211/690] lr: 1.5e-05, eta: 8:50:35.379552, loss: 1.2308
2023-04-14 01:58:34 - training - INFO - Epoch [4/5][221/690] lr: 1.5e-05, eta: 8:26:52.607736, loss: 1.1574
2023-04-14 01:58:42 - training - INFO - Epoch [4/5][231/690] lr: 1.5e-05, eta: 8:05:11.345181, loss: 1.3397
2023-04-14 01:58:49 - training - INFO - Epoch [4/5][241/690] lr: 1.5e-05, eta: 7:45:18.043280, loss: 1.4340
2023-04-14 01:58:57 - training - INFO - Epoch [4/5][251/690] lr: 1.5e-05, eta: 7:27:01.996306, loss: 1.1676
2023-04-14 01:59:05 - training - INFO - Epoch [4/5][261/690] lr: 1.5e-05, eta: 7:10:05.454969, loss: 1.0563
2023-04-14 01:59:12 - training - INFO - Epoch [4/5][271/690] lr: 1.5e-05, eta: 6:54:23.909521, loss: 1.0110
2023-04-14 01:59:20 - training - INFO - Epoch [4/5][281/690] lr: 1.4e-05, eta: 6:39:49.456760, loss: 0.8376
2023-04-14 01:59:28 - training - INFO - Epoch [4/5][291/690] lr: 1.4e-05, eta: 6:26:17.627226, loss: 0.9073
2023-04-14 01:59:35 - training - INFO - Epoch [4/5][301/690] lr: 1.4e-05, eta: 6:13:37.822321, loss: 0.5518
2023-04-14 01:59:43 - training - INFO - Epoch [4/5][311/690] lr: 1.4e-05, eta: 6:01:46.348228, loss: 0.7521
2023-04-14 01:59:51 - training - INFO - Epoch [4/5][321/690] lr: 1.4e-05, eta: 5:50:36.251355, loss: 1.2654
2023-04-14 01:59:58 - training - INFO - Epoch [4/5][331/690] lr: 1.4e-05, eta: 5:40:06.244640, loss: 0.9708
2023-04-14 02:00:06 - training - INFO - Epoch [4/5][341/690] lr: 1.4e-05, eta: 5:30:13.852867, loss: 1.3971
2023-04-14 02:00:14 - training - INFO - Epoch [4/5][351/690] lr: 1.4e-05, eta: 5:20:55.134462, loss: 1.0241
2023-04-14 02:00:21 - training - INFO - Epoch [4/5][361/690] lr: 1.3e-05, eta: 5:12:07.176793, loss: 0.9869
2023-04-14 02:00:29 - training - INFO - Epoch [4/5][371/690] lr: 1.3e-05, eta: 5:03:47.464470, loss: 1.0730
2023-04-14 02:00:36 - training - INFO - Epoch [4/5][381/690] lr: 1.3e-05, eta: 4:55:52.348152, loss: 0.7168
2023-04-14 02:00:44 - training - INFO - Epoch [4/5][391/690] lr: 1.3e-05, eta: 4:48:21.517401, loss: 1.1180
2023-04-14 02:00:52 - training - INFO - Epoch [4/5][401/690] lr: 1.3e-05, eta: 4:41:12.690356, loss: 0.9475
2023-04-14 02:00:59 - training - INFO - Epoch [4/5][411/690] lr: 1.3e-05, eta: 4:34:24.314325, loss: 1.0515
2023-04-14 02:01:07 - training - INFO - Epoch [4/5][421/690] lr: 1.3e-05, eta: 4:27:55.030218, loss: 1.1713
2023-04-14 02:01:15 - training - INFO - Epoch [4/5][431/690] lr: 1.2e-05, eta: 4:21:43.512659, loss: 1.0307
2023-04-14 02:01:22 - training - INFO - Epoch [4/5][441/690] lr: 1.2e-05, eta: 4:15:48.385434, loss: 0.7978
2023-04-14 02:01:30 - training - INFO - Epoch [4/5][451/690] lr: 1.2e-05, eta: 4:10:09.800065, loss: 1.2612
2023-04-14 02:01:37 - training - INFO - Epoch [4/5][461/690] lr: 1.2e-05, eta: 4:04:44.404035, loss: 1.2483
2023-04-14 02:01:45 - training - INFO - Epoch [4/5][471/690] lr: 1.2e-05, eta: 3:59:32.957061, loss: 1.3135
2023-04-14 02:01:53 - training - INFO - Epoch [4/5][481/690] lr: 1.2e-05, eta: 3:54:35.743976, loss: 1.4210
2023-04-14 02:02:01 - training - INFO - Epoch [4/5][491/690] lr: 1.2e-05, eta: 3:49:49.061319, loss: 1.2574
2023-04-14 02:02:08 - training - INFO - Epoch [4/5][501/690] lr: 1.2e-05, eta: 3:45:13.541835, loss: 1.2991
2023-04-14 02:02:16 - training - INFO - Epoch [4/5][511/690] lr: 1.1e-05, eta: 3:40:48.303701, loss: 1.3160
2023-04-14 02:02:24 - training - INFO - Epoch [4/5][521/690] lr: 1.1e-05, eta: 3:36:32.285389, loss: 1.0445
2023-04-14 02:02:31 - training - INFO - Epoch [4/5][531/690] lr: 1.1e-05, eta: 3:32:25.848528, loss: 0.6680
2023-04-14 02:02:39 - training - INFO - Epoch [4/5][541/690] lr: 1.1e-05, eta: 3:28:28.595276, loss: 1.0396
2023-04-14 02:02:47 - training - INFO - Epoch [4/5][551/690] lr: 1.1e-05, eta: 3:24:39.323290, loss: 1.0573
2023-04-14 02:02:54 - training - INFO - Epoch [4/5][561/690] lr: 1.1e-05, eta: 3:20:58.755336, loss: 1.3024
2023-04-14 02:03:02 - training - INFO - Epoch [4/5][571/690] lr: 1.1e-05, eta: 3:17:27.640647, loss: 0.9475
2023-04-14 02:03:10 - training - INFO - Epoch [4/5][581/690] lr: 1.1e-05, eta: 3:14:00.993321, loss: 1.2265
2023-04-14 02:03:18 - training - INFO - Epoch [4/5][591/690] lr: 1.0e-05, eta: 3:10:41.069007, loss: 1.2249
2023-04-14 02:03:25 - training - INFO - Epoch [4/5][601/690] lr: 1.0e-05, eta: 3:07:26.957414, loss: 0.5822
2023-04-14 02:03:33 - training - INFO - Epoch [4/5][611/690] lr: 1.0e-05, eta: 3:04:19.176872, loss: 0.9942
2023-04-14 02:03:41 - training - INFO - Epoch [4/5][621/690] lr: 1.0e-05, eta: 3:01:18.282975, loss: 1.0864
2023-04-14 02:03:48 - training - INFO - Epoch [4/5][631/690] lr: 9.9e-06, eta: 2:58:22.232016, loss: 1.1718
2023-04-14 02:03:56 - training - INFO - Epoch [4/5][641/690] lr: 9.7e-06, eta: 2:55:31.837071, loss: 1.0958
2023-04-14 02:04:04 - training - INFO - Epoch [4/5][651/690] lr: 9.6e-06, eta: 2:52:46.454772, loss: 1.1167
2023-04-14 02:04:11 - training - INFO - Epoch [4/5][661/690] lr: 9.5e-06, eta: 2:50:04.920321, loss: 0.7141
2023-04-14 02:04:19 - training - INFO - Epoch [4/5][671/690] lr: 9.3e-06, eta: 2:47:28.210935, loss: 0.8633
2023-04-14 02:04:27 - training - INFO - Epoch [4/5][681/690] lr: 9.2e-06, eta: 2:44:56.298009, loss: 1.1205
2023-04-14 02:05:50 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 1.1005, Validation Metrics: {'exact_match': 37.485448195576254, 'f1': 42.39954335526059}, Test Metrics: {'exact_match': 39.721254355400696, 'f1': 44.76906494419788}
2023-04-14 02:05:51 - training - INFO - Epoch [5/5][1/690] lr: 9.1e-06, eta: 100 days, 12:39:02.756889, loss: 0.5981
2023-04-14 02:05:59 - training - INFO - Epoch [5/5][11/690] lr: 8.9e-06, eta: 9 days, 3:21:25.818937, loss: 0.4170
2023-04-14 02:06:06 - training - INFO - Epoch [5/5][21/690] lr: 8.8e-06, eta: 4 days, 18:55:20.800065, loss: 0.8975
2023-04-14 02:06:14 - training - INFO - Epoch [5/5][31/690] lr: 8.7e-06, eta: 3 days, 5:51:15.373027, loss: 0.6727
2023-04-14 02:06:22 - training - INFO - Epoch [5/5][41/690] lr: 8.5e-06, eta: 2 days, 10:52:10.094971, loss: 1.1532
2023-04-14 02:06:29 - training - INFO - Epoch [5/5][51/690] lr: 8.4e-06, eta: 1 day, 23:20:04.463295, loss: 0.9851
2023-04-14 02:06:37 - training - INFO - Epoch [5/5][61/690] lr: 8.3e-06, eta: 1 day, 15:34:32.299292, loss: 0.9209
2023-04-14 02:06:45 - training - INFO - Epoch [5/5][71/690] lr: 8.1e-06, eta: 1 day, 10:00:10.350442, loss: 0.9054
2023-04-14 02:06:52 - training - INFO - Epoch [5/5][81/690] lr: 8.0e-06, eta: 1 day, 5:48:17.492061, loss: 1.1770
2023-04-14 02:07:00 - training - INFO - Epoch [5/5][91/690] lr: 7.9e-06, eta: 1 day, 2:31:48.260860, loss: 1.4651
2023-04-14 02:07:08 - training - INFO - Epoch [5/5][101/690] lr: 7.8e-06, eta: 23:54:05.984262, loss: 0.9698
2023-04-14 02:07:15 - training - INFO - Epoch [5/5][111/690] lr: 7.6e-06, eta: 21:44:46.234068, loss: 1.1736
2023-04-14 02:07:23 - training - INFO - Epoch [5/5][121/690] lr: 7.5e-06, eta: 19:56:49.090001, loss: 0.6178
2023-04-14 02:07:30 - training - INFO - Epoch [5/5][131/690] lr: 7.4e-06, eta: 18:25:19.624071, loss: 1.2480
2023-04-14 02:07:38 - training - INFO - Epoch [5/5][141/690] lr: 7.2e-06, eta: 17:06:45.747597, loss: 1.2561
2023-04-14 02:07:46 - training - INFO - Epoch [5/5][151/690] lr: 7.1e-06, eta: 15:58:51.010276, loss: 1.2908
2023-04-14 02:07:54 - training - INFO - Epoch [5/5][161/690] lr: 7.0e-06, eta: 14:59:13.226327, loss: 0.8977
2023-04-14 02:08:01 - training - INFO - Epoch [5/5][171/690] lr: 6.8e-06, eta: 14:06:29.273703, loss: 0.9723
2023-04-14 02:08:09 - training - INFO - Epoch [5/5][181/690] lr: 6.7e-06, eta: 13:19:31.574686, loss: 0.5955
2023-04-14 02:08:16 - training - INFO - Epoch [5/5][191/690] lr: 6.6e-06, eta: 12:37:30.988441, loss: 1.2128
2023-04-14 02:08:24 - training - INFO - Epoch [5/5][201/690] lr: 6.4e-06, eta: 11:59:44.713806, loss: 1.0428
2023-04-14 02:08:32 - training - INFO - Epoch [5/5][211/690] lr: 6.3e-06, eta: 11:25:30.273072, loss: 0.9839
2023-04-14 02:08:40 - training - INFO - Epoch [5/5][221/690] lr: 6.2e-06, eta: 10:54:19.919202, loss: 1.1619
2023-04-14 02:08:47 - training - INFO - Epoch [5/5][231/690] lr: 6.0e-06, eta: 10:25:49.119960, loss: 1.4349
2023-04-14 02:08:55 - training - INFO - Epoch [5/5][241/690] lr: 5.9e-06, eta: 9:59:43.685076, loss: 1.0146
2023-04-14 02:09:03 - training - INFO - Epoch [5/5][251/690] lr: 5.8e-06, eta: 9:35:47.047073, loss: 1.1081
2023-04-14 02:09:11 - training - INFO - Epoch [5/5][261/690] lr: 5.6e-06, eta: 9:13:33.537048, loss: 1.0851
2023-04-14 02:09:18 - training - INFO - Epoch [5/5][271/690] lr: 5.5e-06, eta: 8:52:55.288015, loss: 0.7434
2023-04-14 02:09:26 - training - INFO - Epoch [5/5][281/690] lr: 5.4e-06, eta: 8:33:47.981296, loss: 0.6615
2023-04-14 02:09:34 - training - INFO - Epoch [5/5][291/690] lr: 5.3e-06, eta: 8:15:59.460588, loss: 1.2522
2023-04-14 02:09:42 - training - INFO - Epoch [5/5][301/690] lr: 5.1e-06, eta: 7:59:20.286201, loss: 1.1361
2023-04-14 02:09:49 - training - INFO - Epoch [5/5][311/690] lr: 5.0e-06, eta: 7:43:44.836804, loss: 1.1838
2023-04-14 02:09:57 - training - INFO - Epoch [5/5][321/690] lr: 4.9e-06, eta: 7:29:06.600681, loss: 0.9025
2023-04-14 02:10:05 - training - INFO - Epoch [5/5][331/690] lr: 4.7e-06, eta: 7:15:19.684982, loss: 0.6556
2023-04-14 02:10:12 - training - INFO - Epoch [5/5][341/690] lr: 4.6e-06, eta: 7:02:23.075680, loss: 1.0328
2023-04-14 02:10:20 - training - INFO - Epoch [5/5][351/690] lr: 4.5e-06, eta: 6:50:09.066030, loss: 0.8268
2023-04-14 02:10:27 - training - INFO - Epoch [5/5][361/690] lr: 4.3e-06, eta: 6:38:34.664231, loss: 1.0779
2023-04-14 02:10:35 - training - INFO - Epoch [5/5][371/690] lr: 4.2e-06, eta: 6:27:37.749930, loss: 0.6761
2023-04-14 02:10:43 - training - INFO - Epoch [5/5][381/690] lr: 4.1e-06, eta: 6:17:16.624824, loss: 1.0275
2023-04-14 02:10:50 - training - INFO - Epoch [5/5][391/690] lr: 3.9e-06, eta: 6:07:25.322831, loss: 1.0876
2023-04-14 02:10:58 - training - INFO - Epoch [5/5][401/690] lr: 3.8e-06, eta: 5:58:05.049861, loss: 1.0402
2023-04-14 02:11:06 - training - INFO - Epoch [5/5][411/690] lr: 3.7e-06, eta: 5:49:10.394955, loss: 0.9125
2023-04-14 02:11:14 - training - INFO - Epoch [5/5][421/690] lr: 3.5e-06, eta: 5:40:40.615845, loss: 1.2589
2023-04-14 02:11:21 - training - INFO - Epoch [5/5][431/690] lr: 3.4e-06, eta: 5:32:33.884265, loss: 1.1477
2023-04-14 02:11:29 - training - INFO - Epoch [5/5][441/690] lr: 3.3e-06, eta: 5:24:49.479558, loss: 0.9172
2023-04-14 02:11:37 - training - INFO - Epoch [5/5][451/690] lr: 3.1e-06, eta: 5:17:24.447734, loss: 0.8335
2023-04-14 02:11:45 - training - INFO - Epoch [5/5][461/690] lr: 3.0e-06, eta: 5:10:20.471674, loss: 1.5738
2023-04-14 02:11:52 - training - INFO - Epoch [5/5][471/690] lr: 2.9e-06, eta: 5:03:31.860306, loss: 1.0109
2023-04-14 02:12:00 - training - INFO - Epoch [5/5][481/690] lr: 2.8e-06, eta: 4:57:01.241391, loss: 0.9406
2023-04-14 02:12:07 - training - INFO - Epoch [5/5][491/690] lr: 2.6e-06, eta: 4:50:45.121826, loss: 1.1251
2023-04-14 02:12:15 - training - INFO - Epoch [5/5][501/690] lr: 2.5e-06, eta: 4:44:44.273607, loss: 1.1859
2023-04-14 02:12:23 - training - INFO - Epoch [5/5][511/690] lr: 2.4e-06, eta: 4:38:58.389713, loss: 1.1511
2023-04-14 02:12:30 - training - INFO - Epoch [5/5][521/690] lr: 2.2e-06, eta: 4:33:23.515949, loss: 1.2097
2023-04-14 02:12:38 - training - INFO - Epoch [5/5][531/690] lr: 2.1e-06, eta: 4:28:03.529455, loss: 0.8315
2023-04-14 02:12:46 - training - INFO - Epoch [5/5][541/690] lr: 2.0e-06, eta: 4:22:52.417642, loss: 0.6135
2023-04-14 02:12:54 - training - INFO - Epoch [5/5][551/690] lr: 1.8e-06, eta: 4:17:54.038684, loss: 1.0034
2023-04-14 02:13:02 - training - INFO - Epoch [5/5][561/690] lr: 1.7e-06, eta: 4:13:07.155210, loss: 1.1270
2023-04-14 02:13:09 - training - INFO - Epoch [5/5][571/690] lr: 1.6e-06, eta: 4:08:27.628982, loss: 0.9318
2023-04-14 02:13:17 - training - INFO - Epoch [5/5][581/690] lr: 1.4e-06, eta: 4:03:58.424106, loss: 0.9651
2023-04-14 02:13:25 - training - INFO - Epoch [5/5][591/690] lr: 1.3e-06, eta: 3:59:37.344918, loss: 1.3168
2023-04-14 02:13:32 - training - INFO - Epoch [5/5][601/690] lr: 1.2e-06, eta: 3:55:25.031459, loss: 0.8635
2023-04-14 02:13:40 - training - INFO - Epoch [5/5][611/690] lr: 1.0e-06, eta: 3:51:20.538165, loss: 1.1188
2023-04-14 02:13:48 - training - INFO - Epoch [5/5][621/690] lr: 9.1e-07, eta: 3:47:24.221736, loss: 0.8494
2023-04-14 02:13:55 - training - INFO - Epoch [5/5][631/690] lr: 7.8e-07, eta: 3:43:34.465210, loss: 1.0876
2023-04-14 02:14:03 - training - INFO - Epoch [5/5][641/690] lr: 6.4e-07, eta: 3:39:52.592096, loss: 1.3108
2023-04-14 02:14:11 - training - INFO - Epoch [5/5][651/690] lr: 5.1e-07, eta: 3:36:16.539066, loss: 0.8073
2023-04-14 02:14:18 - training - INFO - Epoch [5/5][661/690] lr: 3.8e-07, eta: 3:32:46.814840, loss: 0.9003
2023-04-14 02:14:26 - training - INFO - Epoch [5/5][671/690] lr: 2.5e-07, eta: 3:29:23.753398, loss: 1.0067
2023-04-14 02:14:34 - training - INFO - Epoch [5/5][681/690] lr: 1.2e-07, eta: 3:26:05.235324, loss: 1.0005
2023-04-14 02:15:57 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 0.9900, Validation Metrics: {'exact_match': 36.20488940628638, 'f1': 41.21097573165194}, Test Metrics: {'exact_match': 37.39837398373984, 'f1': 42.258871800408556}
2023-04-14 02:16:35 - training - INFO - Final Test - Train Loss: 0.9900, Test Metrics: {'exact_match': 37.39837398373984, 'f1': 42.258871800408556}
