2023-04-12 15:40:02 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-44a167365c0b341b/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'xlnet-base-cased'}, 'data': {'task_type': 'list', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 4.54e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/xlnet_list_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 603.50it/s]
Map:   0%|          | 0/6878 [00:00<?, ? examples/s]Map:  15%|█▍        | 1000/6878 [00:00<00:03, 1483.47 examples/s]Map:  29%|██▉       | 2000/6878 [00:01<00:03, 1549.26 examples/s]Map:  44%|████▎     | 3000/6878 [00:01<00:02, 1558.68 examples/s]Map:  58%|█████▊    | 4000/6878 [00:02<00:01, 1536.65 examples/s]Map:  73%|███████▎  | 5000/6878 [00:03<00:01, 1517.11 examples/s]Map:  87%|████████▋ | 6000/6878 [00:03<00:00, 1533.71 examples/s]Map: 100%|██████████| 6878/6878 [00:04<00:00, 1435.01 examples/s]                                                                 Map:   0%|          | 0/859 [00:00<?, ? examples/s]Map: 100%|██████████| 859/859 [00:00<00:00, 1225.24 examples/s]                                                               Map:   0%|          | 0/861 [00:00<?, ? examples/s]Map: 100%|██████████| 861/861 [00:00<00:00, 1241.98 examples/s]                                                               Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForQuestionAnsweringSimple: ['lm_loss.weight', 'lm_loss.bias']
- This IS expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForQuestionAnsweringSimple were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-12 15:41:30 - training - INFO - First Test - Val Metrics:{'exact_match': 0.3492433061699651, 'f1': 3.2844760057985867} Test Metrics: {'exact_match': 0.6968641114982579, 'f1': 3.20263173039842}
2023-04-12 15:41:31 - training - INFO - Epoch [1/5][1/690] lr: 4.5e-05, eta: 3 days, 0:54:12.337902, loss: 7.5106
2023-04-12 15:41:38 - training - INFO - Epoch [1/5][11/690] lr: 4.5e-05, eta: 7:16:15.755916, loss: 5.3669
2023-04-12 15:41:46 - training - INFO - Epoch [1/5][21/690] lr: 4.5e-05, eta: 4:09:06.969852, loss: 4.4134
2023-04-12 15:41:54 - training - INFO - Epoch [1/5][31/690] lr: 4.5e-05, eta: 3:02:08.252270, loss: 3.9593
2023-04-12 15:42:01 - training - INFO - Epoch [1/5][41/690] lr: 4.5e-05, eta: 2:27:56.091707, loss: 3.9408
2023-04-12 15:42:09 - training - INFO - Epoch [1/5][51/690] lr: 4.5e-05, eta: 2:06:56.910873, loss: 4.2040
2023-04-12 15:42:17 - training - INFO - Epoch [1/5][61/690] lr: 4.5e-05, eta: 1:52:50.191744, loss: 3.8874
2023-04-12 15:42:24 - training - INFO - Epoch [1/5][71/690] lr: 4.4e-05, eta: 1:42:43.133808, loss: 3.0451
2023-04-12 15:42:32 - training - INFO - Epoch [1/5][81/690] lr: 4.4e-05, eta: 1:34:59.519226, loss: 3.0287
2023-04-12 15:42:39 - training - INFO - Epoch [1/5][91/690] lr: 4.4e-05, eta: 1:29:03.157941, loss: 2.6370
2023-04-12 15:42:47 - training - INFO - Epoch [1/5][101/690] lr: 4.4e-05, eta: 1:24:10.027429, loss: 2.1087
2023-04-12 15:42:55 - training - INFO - Epoch [1/5][111/690] lr: 4.4e-05, eta: 1:20:07.952982, loss: 2.8889
2023-04-12 15:43:02 - training - INFO - Epoch [1/5][121/690] lr: 4.4e-05, eta: 1:16:42.488976, loss: 2.4317
2023-04-12 15:43:10 - training - INFO - Epoch [1/5][131/690] lr: 4.4e-05, eta: 1:13:53.569985, loss: 1.7475
2023-04-12 15:43:17 - training - INFO - Epoch [1/5][141/690] lr: 4.4e-05, eta: 1:11:29.006676, loss: 2.2823
2023-04-12 15:43:25 - training - INFO - Epoch [1/5][151/690] lr: 4.3e-05, eta: 1:09:21.962317, loss: 2.2573
2023-04-12 15:43:33 - training - INFO - Epoch [1/5][161/690] lr: 4.3e-05, eta: 1:07:27.117789, loss: 2.5253
2023-04-12 15:43:41 - training - INFO - Epoch [1/5][171/690] lr: 4.3e-05, eta: 1:05:47.460219, loss: 2.4049
2023-04-12 15:43:48 - training - INFO - Epoch [1/5][181/690] lr: 4.3e-05, eta: 1:04:17.831894, loss: 2.0082
2023-04-12 15:43:56 - training - INFO - Epoch [1/5][191/690] lr: 4.3e-05, eta: 1:02:56.995237, loss: 2.3017
2023-04-12 15:44:04 - training - INFO - Epoch [1/5][201/690] lr: 4.3e-05, eta: 1:01:40.705221, loss: 2.2575
2023-04-12 15:44:11 - training - INFO - Epoch [1/5][211/690] lr: 4.3e-05, eta: 1:00:29.597488, loss: 1.6411
2023-04-12 15:44:19 - training - INFO - Epoch [1/5][221/690] lr: 4.2e-05, eta: 0:59:24.919328, loss: 1.8925
2023-04-12 15:44:26 - training - INFO - Epoch [1/5][231/690] lr: 4.2e-05, eta: 0:58:25.365459, loss: 2.2691
2023-04-12 15:44:34 - training - INFO - Epoch [1/5][241/690] lr: 4.2e-05, eta: 0:57:29.999109, loss: 1.4471
2023-04-12 15:44:42 - training - INFO - Epoch [1/5][251/690] lr: 4.2e-05, eta: 0:56:41.736625, loss: 2.2281
2023-04-12 15:44:49 - training - INFO - Epoch [1/5][261/690] lr: 4.2e-05, eta: 0:55:53.338737, loss: 1.9854
2023-04-12 15:44:57 - training - INFO - Epoch [1/5][271/690] lr: 4.2e-05, eta: 0:55:09.075143, loss: 2.0446
2023-04-12 15:45:05 - training - INFO - Epoch [1/5][281/690] lr: 4.2e-05, eta: 0:54:28.836176, loss: 1.7989
2023-04-12 15:45:12 - training - INFO - Epoch [1/5][291/690] lr: 4.2e-05, eta: 0:53:51.233694, loss: 2.2800
2023-04-12 15:45:20 - training - INFO - Epoch [1/5][301/690] lr: 4.1e-05, eta: 0:53:12.944295, loss: 1.2035
2023-04-12 15:45:28 - training - INFO - Epoch [1/5][311/690] lr: 4.1e-05, eta: 0:52:39.105295, loss: 1.8805
2023-04-12 15:45:35 - training - INFO - Epoch [1/5][321/690] lr: 4.1e-05, eta: 0:52:05.561229, loss: 2.5341
2023-04-12 15:45:43 - training - INFO - Epoch [1/5][331/690] lr: 4.1e-05, eta: 0:51:33.127895, loss: 2.8988
2023-04-12 15:45:51 - training - INFO - Epoch [1/5][341/690] lr: 4.1e-05, eta: 0:51:02.859331, loss: 2.3513
2023-04-12 15:45:58 - training - INFO - Epoch [1/5][351/690] lr: 4.1e-05, eta: 0:50:32.755776, loss: 1.7061
2023-04-12 15:46:06 - training - INFO - Epoch [1/5][361/690] lr: 4.1e-05, eta: 0:50:03.654019, loss: 2.1606
2023-04-12 15:46:14 - training - INFO - Epoch [1/5][371/690] lr: 4.1e-05, eta: 0:49:37.805586, loss: 1.7737
2023-04-12 15:46:21 - training - INFO - Epoch [1/5][381/690] lr: 4.0e-05, eta: 0:49:10.800534, loss: 1.4453
2023-04-12 15:46:29 - training - INFO - Epoch [1/5][391/690] lr: 4.0e-05, eta: 0:48:44.887322, loss: 2.0265
2023-04-12 15:46:36 - training - INFO - Epoch [1/5][401/690] lr: 4.0e-05, eta: 0:48:20.690542, loss: 1.8161
2023-04-12 15:46:44 - training - INFO - Epoch [1/5][411/690] lr: 4.0e-05, eta: 0:47:58.215627, loss: 2.1816
2023-04-12 15:46:52 - training - INFO - Epoch [1/5][421/690] lr: 4.0e-05, eta: 0:47:35.286850, loss: 2.1290
2023-04-12 15:46:59 - training - INFO - Epoch [1/5][431/690] lr: 4.0e-05, eta: 0:47:12.676377, loss: 2.2580
2023-04-12 15:47:07 - training - INFO - Epoch [1/5][441/690] lr: 4.0e-05, eta: 0:46:50.869386, loss: 1.5355
2023-04-12 15:47:14 - training - INFO - Epoch [1/5][451/690] lr: 3.9e-05, eta: 0:46:30.179630, loss: 2.4548
2023-04-12 15:47:22 - training - INFO - Epoch [1/5][461/690] lr: 3.9e-05, eta: 0:46:10.531001, loss: 2.0235
2023-04-12 15:47:30 - training - INFO - Epoch [1/5][471/690] lr: 3.9e-05, eta: 0:45:50.266422, loss: 1.9094
2023-04-12 15:47:37 - training - INFO - Epoch [1/5][481/690] lr: 3.9e-05, eta: 0:45:30.716967, loss: 2.4456
2023-04-12 15:47:45 - training - INFO - Epoch [1/5][491/690] lr: 3.9e-05, eta: 0:45:11.509240, loss: 1.5525
2023-04-12 15:47:52 - training - INFO - Epoch [1/5][501/690] lr: 3.9e-05, eta: 0:44:52.938330, loss: 1.9641
2023-04-12 15:48:00 - training - INFO - Epoch [1/5][511/690] lr: 3.9e-05, eta: 0:44:35.277652, loss: 1.7904
2023-04-12 15:48:08 - training - INFO - Epoch [1/5][521/690] lr: 3.9e-05, eta: 0:44:18.354542, loss: 1.4106
2023-04-12 15:48:15 - training - INFO - Epoch [1/5][531/690] lr: 3.8e-05, eta: 0:44:01.262988, loss: 2.0066
2023-04-12 15:48:23 - training - INFO - Epoch [1/5][541/690] lr: 3.8e-05, eta: 0:43:43.807458, loss: 1.9937
2023-04-12 15:48:30 - training - INFO - Epoch [1/5][551/690] lr: 3.8e-05, eta: 0:43:27.325812, loss: 1.8799
2023-04-12 15:48:38 - training - INFO - Epoch [1/5][561/690] lr: 3.8e-05, eta: 0:43:10.826310, loss: 1.8149
2023-04-12 15:48:45 - training - INFO - Epoch [1/5][571/690] lr: 3.8e-05, eta: 0:42:55.190646, loss: 1.3786
2023-04-12 15:48:53 - training - INFO - Epoch [1/5][581/690] lr: 3.8e-05, eta: 0:42:40.077556, loss: 1.8137
2023-04-12 15:49:01 - training - INFO - Epoch [1/5][591/690] lr: 3.8e-05, eta: 0:42:25.041774, loss: 1.6943
2023-04-12 15:49:08 - training - INFO - Epoch [1/5][601/690] lr: 3.7e-05, eta: 0:42:10.182655, loss: 1.3299
2023-04-12 15:49:16 - training - INFO - Epoch [1/5][611/690] lr: 3.7e-05, eta: 0:41:56.767822, loss: 1.9131
2023-04-12 15:49:24 - training - INFO - Epoch [1/5][621/690] lr: 3.7e-05, eta: 0:41:41.925165, loss: 2.1443
2023-04-12 15:49:31 - training - INFO - Epoch [1/5][631/690] lr: 3.7e-05, eta: 0:41:27.302365, loss: 1.3624
2023-04-12 15:49:39 - training - INFO - Epoch [1/5][641/690] lr: 3.7e-05, eta: 0:41:12.838543, loss: 2.5605
2023-04-12 15:49:47 - training - INFO - Epoch [1/5][651/690] lr: 3.7e-05, eta: 0:40:59.148219, loss: 2.3178
2023-04-12 15:49:55 - training - INFO - Epoch [1/5][661/690] lr: 3.7e-05, eta: 0:40:46.848269, loss: 2.0970
2023-04-12 15:50:02 - training - INFO - Epoch [1/5][671/690] lr: 3.7e-05, eta: 0:40:32.908898, loss: 1.2660
2023-04-12 15:50:10 - training - INFO - Epoch [1/5][681/690] lr: 3.6e-05, eta: 0:40:19.020552, loss: 2.2339
2023-04-12 15:51:32 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 2.2952, Validation Metrics: {'exact_match': 15.59953434225844, 'f1': 20.428083179460188}, Test Metrics: {'exact_match': 20.557491289198605, 'f1': 25.372526121187544}
2023-04-12 15:51:33 - training - INFO - Epoch [2/5][1/690] lr: 3.6e-05, eta: 27 days, 1:31:11.333884, loss: 1.0621
2023-04-12 15:51:40 - training - INFO - Epoch [2/5][11/690] lr: 3.6e-05, eta: 2 days, 11:31:34.599584, loss: 1.3194
2023-04-12 15:51:48 - training - INFO - Epoch [2/5][21/690] lr: 3.6e-05, eta: 1 day, 7:25:56.046738, loss: 1.7037
2023-04-12 15:51:55 - training - INFO - Epoch [2/5][31/690] lr: 3.6e-05, eta: 21:27:43.447521, loss: 0.7989
2023-04-12 15:52:03 - training - INFO - Epoch [2/5][41/690] lr: 3.6e-05, eta: 16:21:17.524209, loss: 1.5391
2023-04-12 15:52:11 - training - INFO - Epoch [2/5][51/690] lr: 3.6e-05, eta: 13:15:07.721589, loss: 1.9123
2023-04-12 15:52:18 - training - INFO - Epoch [2/5][61/690] lr: 3.6e-05, eta: 11:09:53.384106, loss: 0.9602
2023-04-12 15:52:26 - training - INFO - Epoch [2/5][71/690] lr: 3.5e-05, eta: 9:39:47.359156, loss: 0.9224
2023-04-12 15:52:33 - training - INFO - Epoch [2/5][81/690] lr: 3.5e-05, eta: 8:32:05.933586, loss: 2.5015
2023-04-12 15:52:41 - training - INFO - Epoch [2/5][91/690] lr: 3.5e-05, eta: 7:39:09.416248, loss: 1.7877
2023-04-12 15:52:49 - training - INFO - Epoch [2/5][101/690] lr: 3.5e-05, eta: 6:56:40.003684, loss: 1.2863
2023-04-12 15:52:56 - training - INFO - Epoch [2/5][111/690] lr: 3.5e-05, eta: 6:21:54.331587, loss: 1.2644
2023-04-12 15:53:04 - training - INFO - Epoch [2/5][121/690] lr: 3.5e-05, eta: 5:52:47.569673, loss: 1.4062
2023-04-12 15:53:12 - training - INFO - Epoch [2/5][131/690] lr: 3.5e-05, eta: 5:28:07.481569, loss: 1.6897
2023-04-12 15:53:19 - training - INFO - Epoch [2/5][141/690] lr: 3.4e-05, eta: 5:06:56.424804, loss: 1.4855
2023-04-12 15:53:27 - training - INFO - Epoch [2/5][151/690] lr: 3.4e-05, eta: 4:48:33.059628, loss: 1.3818
2023-04-12 15:53:35 - training - INFO - Epoch [2/5][161/690] lr: 3.4e-05, eta: 4:32:24.981510, loss: 2.1706
2023-04-12 15:53:42 - training - INFO - Epoch [2/5][171/690] lr: 3.4e-05, eta: 4:18:07.477728, loss: 1.3166
2023-04-12 15:53:50 - training - INFO - Epoch [2/5][181/690] lr: 3.4e-05, eta: 4:05:23.137954, loss: 1.2012
2023-04-12 15:53:57 - training - INFO - Epoch [2/5][191/690] lr: 3.4e-05, eta: 3:53:57.513513, loss: 1.2607
2023-04-12 15:54:05 - training - INFO - Epoch [2/5][201/690] lr: 3.4e-05, eta: 3:43:38.717643, loss: 0.7255
2023-04-12 15:54:13 - training - INFO - Epoch [2/5][211/690] lr: 3.4e-05, eta: 3:34:21.790446, loss: 1.6115
2023-04-12 15:54:20 - training - INFO - Epoch [2/5][221/690] lr: 3.3e-05, eta: 3:25:52.016402, loss: 1.4519
2023-04-12 15:54:28 - training - INFO - Epoch [2/5][231/690] lr: 3.3e-05, eta: 3:18:07.393596, loss: 1.4359
2023-04-12 15:54:35 - training - INFO - Epoch [2/5][241/690] lr: 3.3e-05, eta: 3:10:58.366673, loss: 1.2700
2023-04-12 15:54:43 - training - INFO - Epoch [2/5][251/690] lr: 3.3e-05, eta: 3:04:24.371703, loss: 1.9796
2023-04-12 15:54:51 - training - INFO - Epoch [2/5][261/690] lr: 3.3e-05, eta: 2:58:21.451671, loss: 1.2075
2023-04-12 15:54:58 - training - INFO - Epoch [2/5][271/690] lr: 3.3e-05, eta: 2:52:42.843799, loss: 1.1890
2023-04-12 15:55:06 - training - INFO - Epoch [2/5][281/690] lr: 3.3e-05, eta: 2:47:27.149712, loss: 0.9871
2023-04-12 15:55:13 - training - INFO - Epoch [2/5][291/690] lr: 3.2e-05, eta: 2:42:33.264027, loss: 1.2019
2023-04-12 15:55:21 - training - INFO - Epoch [2/5][301/690] lr: 3.2e-05, eta: 2:38:00.436082, loss: 0.9803
2023-04-12 15:55:29 - training - INFO - Epoch [2/5][311/690] lr: 3.2e-05, eta: 2:33:44.299929, loss: 1.1899
2023-04-12 15:55:36 - training - INFO - Epoch [2/5][321/690] lr: 3.2e-05, eta: 2:29:42.486009, loss: 1.7316
2023-04-12 15:55:44 - training - INFO - Epoch [2/5][331/690] lr: 3.2e-05, eta: 2:25:55.843940, loss: 1.0886
2023-04-12 15:55:51 - training - INFO - Epoch [2/5][341/690] lr: 3.2e-05, eta: 2:22:20.612649, loss: 1.7508
2023-04-12 15:55:59 - training - INFO - Epoch [2/5][351/690] lr: 3.2e-05, eta: 2:18:57.772728, loss: 1.3935
2023-04-12 15:56:07 - training - INFO - Epoch [2/5][361/690] lr: 3.2e-05, eta: 2:15:45.195671, loss: 1.3020
2023-04-12 15:56:14 - training - INFO - Epoch [2/5][371/690] lr: 3.1e-05, eta: 2:12:42.407923, loss: 1.3330
2023-04-12 15:56:22 - training - INFO - Epoch [2/5][381/690] lr: 3.1e-05, eta: 2:09:49.330692, loss: 0.8423
2023-04-12 15:56:29 - training - INFO - Epoch [2/5][391/690] lr: 3.1e-05, eta: 2:07:04.456553, loss: 0.9301
2023-04-12 15:56:37 - training - INFO - Epoch [2/5][401/690] lr: 3.1e-05, eta: 2:04:27.754103, loss: 1.2524
2023-04-12 15:56:45 - training - INFO - Epoch [2/5][411/690] lr: 3.1e-05, eta: 2:01:58.848012, loss: 2.1323
2023-04-12 15:56:52 - training - INFO - Epoch [2/5][421/690] lr: 3.1e-05, eta: 1:59:37.582009, loss: 0.6929
2023-04-12 15:57:00 - training - INFO - Epoch [2/5][431/690] lr: 3.1e-05, eta: 1:57:24.277985, loss: 1.6002
2023-04-12 15:57:08 - training - INFO - Epoch [2/5][441/690] lr: 3.1e-05, eta: 1:55:13.601769, loss: 1.3693
2023-04-12 15:57:15 - training - INFO - Epoch [2/5][451/690] lr: 3.0e-05, eta: 1:53:07.576720, loss: 0.7811
2023-04-12 15:57:23 - training - INFO - Epoch [2/5][461/690] lr: 3.0e-05, eta: 1:51:06.913687, loss: 1.4175
2023-04-12 15:57:31 - training - INFO - Epoch [2/5][471/690] lr: 3.0e-05, eta: 1:49:12.256878, loss: 1.5135
2023-04-12 15:57:38 - training - INFO - Epoch [2/5][481/690] lr: 3.0e-05, eta: 1:47:20.636855, loss: 1.6988
2023-04-12 15:57:46 - training - INFO - Epoch [2/5][491/690] lr: 3.0e-05, eta: 1:45:33.171372, loss: 1.5055
2023-04-12 15:57:53 - training - INFO - Epoch [2/5][501/690] lr: 3.0e-05, eta: 1:43:50.638353, loss: 1.2266
2023-04-12 15:58:01 - training - INFO - Epoch [2/5][511/690] lr: 3.0e-05, eta: 1:42:11.659212, loss: 1.4618
2023-04-12 15:58:08 - training - INFO - Epoch [2/5][521/690] lr: 2.9e-05, eta: 1:40:36.560627, loss: 1.7704
2023-04-12 15:58:16 - training - INFO - Epoch [2/5][531/690] lr: 2.9e-05, eta: 1:39:04.543500, loss: 2.1177
2023-04-12 15:58:24 - training - INFO - Epoch [2/5][541/690] lr: 2.9e-05, eta: 1:37:35.098477, loss: 1.5348
2023-04-12 15:58:31 - training - INFO - Epoch [2/5][551/690] lr: 2.9e-05, eta: 1:36:08.859252, loss: 1.2778
2023-04-12 15:58:39 - training - INFO - Epoch [2/5][561/690] lr: 2.9e-05, eta: 1:34:45.809121, loss: 1.1240
2023-04-12 15:58:46 - training - INFO - Epoch [2/5][571/690] lr: 2.9e-05, eta: 1:33:25.533918, loss: 0.5521
2023-04-12 15:58:54 - training - INFO - Epoch [2/5][581/690] lr: 2.9e-05, eta: 1:32:09.199918, loss: 1.4162
2023-04-12 15:59:02 - training - INFO - Epoch [2/5][591/690] lr: 2.9e-05, eta: 1:30:53.356665, loss: 2.0093
2023-04-12 15:59:10 - training - INFO - Epoch [2/5][601/690] lr: 2.8e-05, eta: 1:29:40.421970, loss: 1.0920
2023-04-12 15:59:17 - training - INFO - Epoch [2/5][611/690] lr: 2.8e-05, eta: 1:28:29.185510, loss: 1.0992
2023-04-12 15:59:25 - training - INFO - Epoch [2/5][621/690] lr: 2.8e-05, eta: 1:27:19.808733, loss: 1.3789
2023-04-12 15:59:33 - training - INFO - Epoch [2/5][631/690] lr: 2.8e-05, eta: 1:26:12.729688, loss: 1.4848
2023-04-12 15:59:40 - training - INFO - Epoch [2/5][641/690] lr: 2.8e-05, eta: 1:25:07.301328, loss: 1.1018
2023-04-12 15:59:48 - training - INFO - Epoch [2/5][651/690] lr: 2.8e-05, eta: 1:24:04.850424, loss: 1.5428
2023-04-12 15:59:56 - training - INFO - Epoch [2/5][661/690] lr: 2.8e-05, eta: 1:23:04.146597, loss: 1.3128
2023-04-12 16:00:04 - training - INFO - Epoch [2/5][671/690] lr: 2.7e-05, eta: 1:22:03.704366, loss: 1.4671
2023-04-12 16:00:11 - training - INFO - Epoch [2/5][681/690] lr: 2.7e-05, eta: 1:21:04.642887, loss: 1.0819
2023-04-12 16:01:33 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.4315, Validation Metrics: {'exact_match': 21.30384167636787, 'f1': 26.70257704502749}, Test Metrics: {'exact_match': 24.041811846689896, 'f1': 29.47299648954561}
2023-04-12 16:01:34 - training - INFO - Epoch [3/5][1/690] lr: 2.7e-05, eta: 51 days, 1:52:43.607709, loss: 1.2308
2023-04-12 16:01:42 - training - INFO - Epoch [3/5][11/690] lr: 2.7e-05, eta: 4 days, 15:47:18.809316, loss: 1.7736
2023-04-12 16:01:50 - training - INFO - Epoch [3/5][21/690] lr: 2.7e-05, eta: 2 days, 10:44:07.820817, loss: 1.5529
2023-04-12 16:01:57 - training - INFO - Epoch [3/5][31/690] lr: 2.7e-05, eta: 1 day, 15:54:13.640806, loss: 1.0547
2023-04-12 16:02:05 - training - INFO - Epoch [3/5][41/690] lr: 2.7e-05, eta: 1 day, 6:15:43.349312, loss: 1.0282
2023-04-12 16:02:13 - training - INFO - Epoch [3/5][51/690] lr: 2.7e-05, eta: 1 day, 0:24:00.764880, loss: 1.3830
2023-04-12 16:02:20 - training - INFO - Epoch [3/5][61/690] lr: 2.6e-05, eta: 20:27:21.163663, loss: 1.3562
2023-04-12 16:02:28 - training - INFO - Epoch [3/5][71/690] lr: 2.6e-05, eta: 17:37:17.883261, loss: 1.1172
2023-04-12 16:02:35 - training - INFO - Epoch [3/5][81/690] lr: 2.6e-05, eta: 15:29:17.111712, loss: 2.1935
2023-04-12 16:02:43 - training - INFO - Epoch [3/5][91/690] lr: 2.6e-05, eta: 13:49:30.366821, loss: 0.9268
2023-04-12 16:02:51 - training - INFO - Epoch [3/5][101/690] lr: 2.6e-05, eta: 12:29:20.767068, loss: 0.8095
2023-04-12 16:02:58 - training - INFO - Epoch [3/5][111/690] lr: 2.6e-05, eta: 11:23:39.875442, loss: 0.7639
2023-04-12 16:03:06 - training - INFO - Epoch [3/5][121/690] lr: 2.6e-05, eta: 10:28:45.183423, loss: 0.9763
2023-04-12 16:03:14 - training - INFO - Epoch [3/5][131/690] lr: 2.6e-05, eta: 9:42:13.045868, loss: 1.9053
2023-04-12 16:03:21 - training - INFO - Epoch [3/5][141/690] lr: 2.5e-05, eta: 9:02:12.896760, loss: 1.7887
2023-04-12 16:03:29 - training - INFO - Epoch [3/5][151/690] lr: 2.5e-05, eta: 8:27:32.343220, loss: 1.4483
2023-04-12 16:03:36 - training - INFO - Epoch [3/5][161/690] lr: 2.5e-05, eta: 7:57:10.735133, loss: 1.0234
2023-04-12 16:03:44 - training - INFO - Epoch [3/5][171/690] lr: 2.5e-05, eta: 7:30:19.832214, loss: 1.4667
2023-04-12 16:03:51 - training - INFO - Epoch [3/5][181/690] lr: 2.5e-05, eta: 7:06:27.224677, loss: 0.9741
2023-04-12 16:03:59 - training - INFO - Epoch [3/5][191/690] lr: 2.5e-05, eta: 6:45:01.799193, loss: 1.1257
2023-04-12 16:04:06 - training - INFO - Epoch [3/5][201/690] lr: 2.5e-05, eta: 6:25:43.478238, loss: 0.6960
2023-04-12 16:04:14 - training - INFO - Epoch [3/5][211/690] lr: 2.4e-05, eta: 6:08:15.042557, loss: 1.2542
2023-04-12 16:04:22 - training - INFO - Epoch [3/5][221/690] lr: 2.4e-05, eta: 5:52:21.215555, loss: 1.7450
2023-04-12 16:04:29 - training - INFO - Epoch [3/5][231/690] lr: 2.4e-05, eta: 5:37:49.193184, loss: 1.0630
2023-04-12 16:04:37 - training - INFO - Epoch [3/5][241/690] lr: 2.4e-05, eta: 5:24:30.421378, loss: 0.9205
2023-04-12 16:04:45 - training - INFO - Epoch [3/5][251/690] lr: 2.4e-05, eta: 5:12:13.622313, loss: 1.2009
2023-04-12 16:04:52 - training - INFO - Epoch [3/5][261/690] lr: 2.4e-05, eta: 5:00:51.372768, loss: 1.2891
2023-04-12 16:05:00 - training - INFO - Epoch [3/5][271/690] lr: 2.4e-05, eta: 4:50:19.982195, loss: 1.5959
2023-04-12 16:05:07 - training - INFO - Epoch [3/5][281/690] lr: 2.4e-05, eta: 4:40:33.008637, loss: 1.2892
2023-04-12 16:05:15 - training - INFO - Epoch [3/5][291/690] lr: 2.3e-05, eta: 4:31:25.235733, loss: 1.4886
2023-04-12 16:05:23 - training - INFO - Epoch [3/5][301/690] lr: 2.3e-05, eta: 4:22:54.890308, loss: 1.4518
2023-04-12 16:05:30 - training - INFO - Epoch [3/5][311/690] lr: 2.3e-05, eta: 4:14:55.417856, loss: 1.0884
2023-04-12 16:05:38 - training - INFO - Epoch [3/5][321/690] lr: 2.3e-05, eta: 4:07:25.800207, loss: 0.8197
2023-04-12 16:05:45 - training - INFO - Epoch [3/5][331/690] lr: 2.3e-05, eta: 4:00:23.806143, loss: 0.9125
2023-04-12 16:05:53 - training - INFO - Epoch [3/5][341/690] lr: 2.3e-05, eta: 3:53:46.119813, loss: 1.3582
2023-04-12 16:06:01 - training - INFO - Epoch [3/5][351/690] lr: 2.3e-05, eta: 3:47:29.334768, loss: 1.4138
2023-04-12 16:06:08 - training - INFO - Epoch [3/5][361/690] lr: 2.2e-05, eta: 3:41:33.372495, loss: 0.7494
2023-04-12 16:06:16 - training - INFO - Epoch [3/5][371/690] lr: 2.2e-05, eta: 3:35:55.936281, loss: 1.9074
2023-04-12 16:06:23 - training - INFO - Epoch [3/5][381/690] lr: 2.2e-05, eta: 3:30:35.637696, loss: 1.2263
2023-04-12 16:06:31 - training - INFO - Epoch [3/5][391/690] lr: 2.2e-05, eta: 3:25:33.413855, loss: 0.5624
2023-04-12 16:06:39 - training - INFO - Epoch [3/5][401/690] lr: 2.2e-05, eta: 3:20:48.949779, loss: 1.0808
2023-04-12 16:06:47 - training - INFO - Epoch [3/5][411/690] lr: 2.2e-05, eta: 3:16:12.894543, loss: 1.6354
2023-04-12 16:06:54 - training - INFO - Epoch [3/5][421/690] lr: 2.2e-05, eta: 3:11:49.812288, loss: 1.2381
2023-04-12 16:07:02 - training - INFO - Epoch [3/5][431/690] lr: 2.2e-05, eta: 3:07:37.860057, loss: 0.9195
2023-04-12 16:07:10 - training - INFO - Epoch [3/5][441/690] lr: 2.1e-05, eta: 3:03:39.048270, loss: 1.3039
2023-04-12 16:07:18 - training - INFO - Epoch [3/5][451/690] lr: 2.1e-05, eta: 2:59:51.073776, loss: 0.9182
2023-04-12 16:07:25 - training - INFO - Epoch [3/5][461/690] lr: 2.1e-05, eta: 2:56:11.662584, loss: 1.5905
2023-04-12 16:07:33 - training - INFO - Epoch [3/5][471/690] lr: 2.1e-05, eta: 2:52:40.893483, loss: 1.0514
2023-04-12 16:07:40 - training - INFO - Epoch [3/5][481/690] lr: 2.1e-05, eta: 2:49:17.931739, loss: 1.4204
2023-04-12 16:07:48 - training - INFO - Epoch [3/5][491/690] lr: 2.1e-05, eta: 2:46:02.636387, loss: 0.8441
2023-04-12 16:07:55 - training - INFO - Epoch [3/5][501/690] lr: 2.1e-05, eta: 2:42:55.295067, loss: 1.1525
2023-04-12 16:08:03 - training - INFO - Epoch [3/5][511/690] lr: 2.1e-05, eta: 2:39:56.084815, loss: 0.7579
2023-04-12 16:08:11 - training - INFO - Epoch [3/5][521/690] lr: 2.0e-05, eta: 2:37:02.639864, loss: 1.0977
2023-04-12 16:08:18 - training - INFO - Epoch [3/5][531/690] lr: 2.0e-05, eta: 2:34:15.019347, loss: 1.0124
2023-04-12 16:08:26 - training - INFO - Epoch [3/5][541/690] lr: 2.0e-05, eta: 2:31:33.676541, loss: 0.9259
2023-04-12 16:08:33 - training - INFO - Epoch [3/5][551/690] lr: 2.0e-05, eta: 2:28:57.434363, loss: 1.0115
2023-04-12 16:08:41 - training - INFO - Epoch [3/5][561/690] lr: 2.0e-05, eta: 2:26:26.838609, loss: 1.1608
2023-04-12 16:08:49 - training - INFO - Epoch [3/5][571/690] lr: 2.0e-05, eta: 2:24:01.813688, loss: 1.3090
2023-04-12 16:08:56 - training - INFO - Epoch [3/5][581/690] lr: 2.0e-05, eta: 2:21:41.590071, loss: 0.5529
2023-04-12 16:09:04 - training - INFO - Epoch [3/5][591/690] lr: 1.9e-05, eta: 2:19:25.105215, loss: 1.0618
2023-04-12 16:09:12 - training - INFO - Epoch [3/5][601/690] lr: 1.9e-05, eta: 2:17:13.419117, loss: 1.2164
2023-04-12 16:09:19 - training - INFO - Epoch [3/5][611/690] lr: 1.9e-05, eta: 2:15:05.589154, loss: 1.0059
2023-04-12 16:09:27 - training - INFO - Epoch [3/5][621/690] lr: 1.9e-05, eta: 2:13:01.590663, loss: 0.7634
2023-04-12 16:09:34 - training - INFO - Epoch [3/5][631/690] lr: 1.9e-05, eta: 2:11:01.308653, loss: 1.1389
2023-04-12 16:09:42 - training - INFO - Epoch [3/5][641/690] lr: 1.9e-05, eta: 2:09:05.168621, loss: 1.0357
2023-04-12 16:09:50 - training - INFO - Epoch [3/5][651/690] lr: 1.9e-05, eta: 2:07:11.498691, loss: 0.9952
2023-04-12 16:09:57 - training - INFO - Epoch [3/5][661/690] lr: 1.9e-05, eta: 2:05:21.154869, loss: 1.0219
2023-04-12 16:10:05 - training - INFO - Epoch [3/5][671/690] lr: 1.8e-05, eta: 2:03:33.429919, loss: 1.3322
2023-04-12 16:10:12 - training - INFO - Epoch [3/5][681/690] lr: 1.8e-05, eta: 2:01:49.196388, loss: 0.7882
2023-04-12 16:11:34 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 1.1669, Validation Metrics: {'exact_match': 23.28288707799767, 'f1': 26.930065041170767}, Test Metrics: {'exact_match': 25.551684088269454, 'f1': 29.800454433053943}
2023-04-12 16:11:35 - training - INFO - Epoch [4/5][1/690] lr: 1.8e-05, eta: 75 days, 1:01:39.946290, loss: 1.0538
2023-04-12 16:11:42 - training - INFO - Epoch [4/5][11/690] lr: 1.8e-05, eta: 6 days, 19:55:32.633852, loss: 0.7056
2023-04-12 16:11:50 - training - INFO - Epoch [4/5][21/690] lr: 1.8e-05, eta: 3 days, 13:57:25.691877, loss: 1.0283
2023-04-12 16:11:58 - training - INFO - Epoch [4/5][31/690] lr: 1.8e-05, eta: 2 days, 10:17:40.076517, loss: 1.1193
2023-04-12 16:12:05 - training - INFO - Epoch [4/5][41/690] lr: 1.8e-05, eta: 1 day, 20:07:27.083283, loss: 0.9569
2023-04-12 16:12:13 - training - INFO - Epoch [4/5][51/690] lr: 1.7e-05, eta: 1 day, 11:30:38.830482, loss: 0.8516
2023-04-12 16:12:21 - training - INFO - Epoch [4/5][61/690] lr: 1.7e-05, eta: 1 day, 5:43:12.319441, loss: 1.3076
2023-04-12 16:12:28 - training - INFO - Epoch [4/5][71/690] lr: 1.7e-05, eta: 1 day, 1:33:40.634763, loss: 0.8551
2023-04-12 16:12:36 - training - INFO - Epoch [4/5][81/690] lr: 1.7e-05, eta: 22:25:42.100848, loss: 1.4116
2023-04-12 16:12:44 - training - INFO - Epoch [4/5][91/690] lr: 1.7e-05, eta: 19:59:03.686774, loss: 1.1944
2023-04-12 16:12:51 - training - INFO - Epoch [4/5][101/690] lr: 1.7e-05, eta: 18:01:22.745683, loss: 1.1154
2023-04-12 16:12:59 - training - INFO - Epoch [4/5][111/690] lr: 1.7e-05, eta: 16:24:51.247971, loss: 1.2470
2023-04-12 16:13:08 - training - INFO - Epoch [4/5][121/690] lr: 1.7e-05, eta: 15:04:43.852466, loss: 1.0763
2023-04-12 16:13:16 - training - INFO - Epoch [4/5][131/690] lr: 1.6e-05, eta: 13:56:47.223266, loss: 1.1650
2023-04-12 16:13:24 - training - INFO - Epoch [4/5][141/690] lr: 1.6e-05, eta: 12:58:09.596229, loss: 1.5042
2023-04-12 16:13:32 - training - INFO - Epoch [4/5][151/690] lr: 1.6e-05, eta: 12:07:15.589286, loss: 1.0306
2023-04-12 16:13:39 - training - INFO - Epoch [4/5][161/690] lr: 1.6e-05, eta: 11:22:34.239898, loss: 0.6651
2023-04-12 16:13:47 - training - INFO - Epoch [4/5][171/690] lr: 1.6e-05, eta: 10:43:06.744081, loss: 0.8219
2023-04-12 16:13:55 - training - INFO - Epoch [4/5][181/690] lr: 1.6e-05, eta: 10:07:59.980530, loss: 0.8314
2023-04-12 16:14:02 - training - INFO - Epoch [4/5][191/690] lr: 1.6e-05, eta: 9:36:34.053611, loss: 1.1198
2023-04-12 16:14:10 - training - INFO - Epoch [4/5][201/690] lr: 1.6e-05, eta: 9:08:16.566864, loss: 0.7642
2023-04-12 16:14:18 - training - INFO - Epoch [4/5][211/690] lr: 1.5e-05, eta: 8:42:39.207684, loss: 1.4513
2023-04-12 16:14:25 - training - INFO - Epoch [4/5][221/690] lr: 1.5e-05, eta: 8:19:21.099895, loss: 1.0944
2023-04-12 16:14:33 - training - INFO - Epoch [4/5][231/690] lr: 1.5e-05, eta: 7:58:04.698921, loss: 0.8344
2023-04-12 16:14:41 - training - INFO - Epoch [4/5][241/690] lr: 1.5e-05, eta: 7:38:30.323785, loss: 1.1945
2023-04-12 16:14:49 - training - INFO - Epoch [4/5][251/690] lr: 1.5e-05, eta: 7:20:30.681830, loss: 0.9262
2023-04-12 16:14:56 - training - INFO - Epoch [4/5][261/690] lr: 1.5e-05, eta: 7:03:49.832226, loss: 1.2058
2023-04-12 16:15:04 - training - INFO - Epoch [4/5][271/690] lr: 1.5e-05, eta: 6:48:23.519007, loss: 0.6216
2023-04-12 16:15:11 - training - INFO - Epoch [4/5][281/690] lr: 1.4e-05, eta: 6:34:05.100544, loss: 1.2021
2023-04-12 16:15:19 - training - INFO - Epoch [4/5][291/690] lr: 1.4e-05, eta: 6:20:41.958204, loss: 1.5022
2023-04-12 16:15:26 - training - INFO - Epoch [4/5][301/690] lr: 1.4e-05, eta: 6:08:11.778010, loss: 0.6208
2023-04-12 16:15:34 - training - INFO - Epoch [4/5][311/690] lr: 1.4e-05, eta: 5:56:28.898019, loss: 0.9214
2023-04-12 16:15:42 - training - INFO - Epoch [4/5][321/690] lr: 1.4e-05, eta: 5:45:33.342252, loss: 1.1068
2023-04-12 16:15:49 - training - INFO - Epoch [4/5][331/690] lr: 1.4e-05, eta: 5:35:13.473467, loss: 1.1475
2023-04-12 16:15:57 - training - INFO - Epoch [4/5][341/690] lr: 1.4e-05, eta: 5:25:31.770188, loss: 1.5629
2023-04-12 16:16:04 - training - INFO - Epoch [4/5][351/690] lr: 1.4e-05, eta: 5:16:20.346132, loss: 1.0171
2023-04-12 16:16:12 - training - INFO - Epoch [4/5][361/690] lr: 1.3e-05, eta: 5:07:39.073216, loss: 0.8981
2023-04-12 16:16:20 - training - INFO - Epoch [4/5][371/690] lr: 1.3e-05, eta: 4:59:26.433008, loss: 1.2506
2023-04-12 16:16:27 - training - INFO - Epoch [4/5][381/690] lr: 1.3e-05, eta: 4:51:39.391965, loss: 0.7192
2023-04-12 16:16:35 - training - INFO - Epoch [4/5][391/690] lr: 1.3e-05, eta: 4:44:16.026533, loss: 0.9057
2023-04-12 16:16:43 - training - INFO - Epoch [4/5][401/690] lr: 1.3e-05, eta: 4:37:15.984290, loss: 1.0510
2023-04-12 16:16:50 - training - INFO - Epoch [4/5][411/690] lr: 1.3e-05, eta: 4:30:34.647978, loss: 0.9938
2023-04-12 16:16:58 - training - INFO - Epoch [4/5][421/690] lr: 1.3e-05, eta: 4:24:11.181060, loss: 0.8292
2023-04-12 16:17:06 - training - INFO - Epoch [4/5][431/690] lr: 1.2e-05, eta: 4:18:06.334856, loss: 0.7721
2023-04-12 16:17:13 - training - INFO - Epoch [4/5][441/690] lr: 1.2e-05, eta: 4:12:17.120535, loss: 0.6313
2023-04-12 16:17:21 - training - INFO - Epoch [4/5][451/690] lr: 1.2e-05, eta: 4:06:43.240941, loss: 1.0734
2023-04-12 16:17:29 - training - INFO - Epoch [4/5][461/690] lr: 1.2e-05, eta: 4:01:25.022790, loss: 1.0525
2023-04-12 16:17:36 - training - INFO - Epoch [4/5][471/690] lr: 1.2e-05, eta: 3:56:18.782862, loss: 0.9039
2023-04-12 16:17:44 - training - INFO - Epoch [4/5][481/690] lr: 1.2e-05, eta: 3:51:24.305825, loss: 1.1659
2023-04-12 16:17:52 - training - INFO - Epoch [4/5][491/690] lr: 1.2e-05, eta: 3:46:41.481432, loss: 1.5386
2023-04-12 16:17:59 - training - INFO - Epoch [4/5][501/690] lr: 1.2e-05, eta: 3:42:09.686430, loss: 1.5366
2023-04-12 16:18:08 - training - INFO - Epoch [4/5][511/690] lr: 1.1e-05, eta: 3:37:52.298747, loss: 1.0875
2023-04-12 16:18:15 - training - INFO - Epoch [4/5][521/690] lr: 1.1e-05, eta: 3:33:40.821729, loss: 0.8446
2023-04-12 16:18:23 - training - INFO - Epoch [4/5][531/690] lr: 1.1e-05, eta: 3:29:37.533150, loss: 0.7488
2023-04-12 16:18:30 - training - INFO - Epoch [4/5][541/690] lr: 1.1e-05, eta: 3:25:43.340804, loss: 1.1685
2023-04-12 16:18:38 - training - INFO - Epoch [4/5][551/690] lr: 1.1e-05, eta: 3:21:58.182375, loss: 0.8813
2023-04-12 16:18:46 - training - INFO - Epoch [4/5][561/690] lr: 1.1e-05, eta: 3:18:20.507472, loss: 1.1380
2023-04-12 16:18:53 - training - INFO - Epoch [4/5][571/690] lr: 1.1e-05, eta: 3:14:49.416565, loss: 1.0702
2023-04-12 16:19:01 - training - INFO - Epoch [4/5][581/690] lr: 1.1e-05, eta: 3:11:25.938216, loss: 1.0708
2023-04-12 16:19:09 - training - INFO - Epoch [4/5][591/690] lr: 1.0e-05, eta: 3:08:10.202436, loss: 1.1551
2023-04-12 16:19:16 - training - INFO - Epoch [4/5][601/690] lr: 1.0e-05, eta: 3:05:00.974654, loss: 0.5848
2023-04-12 16:19:24 - training - INFO - Epoch [4/5][611/690] lr: 1.0e-05, eta: 3:01:56.687462, loss: 0.4798
2023-04-12 16:19:32 - training - INFO - Epoch [4/5][621/690] lr: 1.0e-05, eta: 2:58:57.939114, loss: 0.7531
2023-04-12 16:19:39 - training - INFO - Epoch [4/5][631/690] lr: 9.9e-06, eta: 2:56:04.619712, loss: 1.0283
2023-04-12 16:19:47 - training - INFO - Epoch [4/5][641/690] lr: 9.7e-06, eta: 2:53:15.766302, loss: 1.0381
2023-04-12 16:19:55 - training - INFO - Epoch [4/5][651/690] lr: 9.6e-06, eta: 2:50:32.668170, loss: 0.9708
2023-04-12 16:20:02 - training - INFO - Epoch [4/5][661/690] lr: 9.5e-06, eta: 2:47:54.805104, loss: 0.9623
2023-04-12 16:20:10 - training - INFO - Epoch [4/5][671/690] lr: 9.3e-06, eta: 2:45:20.335250, loss: 0.9005
2023-04-12 16:20:18 - training - INFO - Epoch [4/5][681/690] lr: 9.2e-06, eta: 2:42:50.629713, loss: 0.7199
2023-04-12 16:21:41 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 1.0091, Validation Metrics: {'exact_match': 23.166472642607683, 'f1': 27.507812724901246}, Test Metrics: {'exact_match': 25.78397212543554, 'f1': 30.6329658728165}
2023-04-12 16:21:42 - training - INFO - Epoch [5/5][1/690] lr: 9.1e-06, eta: 99 days, 6:38:38.918025, loss: 0.5893
2023-04-12 16:21:49 - training - INFO - Epoch [5/5][11/690] lr: 8.9e-06, eta: 9 days, 0:38:31.544722, loss: 0.5700
2023-04-12 16:21:57 - training - INFO - Epoch [5/5][21/690] lr: 8.8e-06, eta: 4 days, 17:29:58.690167, loss: 0.9894
2023-04-12 16:22:05 - training - INFO - Epoch [5/5][31/690] lr: 8.7e-06, eta: 3 days, 4:53:42.996749, loss: 0.5087
2023-04-12 16:22:12 - training - INFO - Epoch [5/5][41/690] lr: 8.5e-06, eta: 2 days, 10:08:49.818859, loss: 0.5586
2023-04-12 16:22:20 - training - INFO - Epoch [5/5][51/690] lr: 8.4e-06, eta: 1 day, 22:44:50.064360, loss: 0.9826
2023-04-12 16:22:27 - training - INFO - Epoch [5/5][61/690] lr: 8.3e-06, eta: 1 day, 15:05:08.873810, loss: 0.9656
2023-04-12 16:22:35 - training - INFO - Epoch [5/5][71/690] lr: 8.1e-06, eta: 1 day, 9:35:13.980566, loss: 0.7606
2023-04-12 16:22:43 - training - INFO - Epoch [5/5][81/690] lr: 8.0e-06, eta: 1 day, 5:26:39.884652, loss: 1.0841
2023-04-12 16:22:51 - training - INFO - Epoch [5/5][91/690] lr: 7.9e-06, eta: 1 day, 2:12:37.806719, loss: 1.3977
2023-04-12 16:22:59 - training - INFO - Epoch [5/5][101/690] lr: 7.8e-06, eta: 23:36:53.417233, loss: 0.8416
2023-04-12 16:23:06 - training - INFO - Epoch [5/5][111/690] lr: 7.6e-06, eta: 21:29:12.222276, loss: 1.0089
2023-04-12 16:23:14 - training - INFO - Epoch [5/5][121/690] lr: 7.5e-06, eta: 19:42:35.917236, loss: 0.9156
2023-04-12 16:23:21 - training - INFO - Epoch [5/5][131/690] lr: 7.4e-06, eta: 18:12:17.833621, loss: 0.8083
2023-04-12 16:23:29 - training - INFO - Epoch [5/5][141/690] lr: 7.2e-06, eta: 16:54:44.984526, loss: 1.2863
2023-04-12 16:23:37 - training - INFO - Epoch [5/5][151/690] lr: 7.1e-06, eta: 15:47:26.688809, loss: 0.8235
2023-04-12 16:23:44 - training - INFO - Epoch [5/5][161/690] lr: 7.0e-06, eta: 14:48:28.736910, loss: 1.0770
2023-04-12 16:23:52 - training - INFO - Epoch [5/5][171/690] lr: 6.8e-06, eta: 13:56:25.045815, loss: 0.6189
2023-04-12 16:24:00 - training - INFO - Epoch [5/5][181/690] lr: 6.7e-06, eta: 13:10:08.616927, loss: 0.6635
2023-04-12 16:24:07 - training - INFO - Epoch [5/5][191/690] lr: 6.6e-06, eta: 12:28:38.689453, loss: 1.0658
2023-04-12 16:24:15 - training - INFO - Epoch [5/5][201/690] lr: 6.4e-06, eta: 11:51:16.739154, loss: 0.9932
2023-04-12 16:24:22 - training - INFO - Epoch [5/5][211/690] lr: 6.3e-06, eta: 11:17:25.038482, loss: 0.6015
2023-04-12 16:24:30 - training - INFO - Epoch [5/5][221/690] lr: 6.2e-06, eta: 10:46:36.967785, loss: 0.9288
2023-04-12 16:24:38 - training - INFO - Epoch [5/5][231/690] lr: 6.0e-06, eta: 10:18:29.246829, loss: 1.5005
2023-04-12 16:24:45 - training - INFO - Epoch [5/5][241/690] lr: 5.9e-06, eta: 9:52:40.671487, loss: 0.9578
2023-04-12 16:24:53 - training - INFO - Epoch [5/5][251/690] lr: 5.8e-06, eta: 9:28:53.170050, loss: 1.1600
2023-04-12 16:25:00 - training - INFO - Epoch [5/5][261/690] lr: 5.6e-06, eta: 9:06:54.953505, loss: 1.1288
2023-04-12 16:25:08 - training - INFO - Epoch [5/5][271/690] lr: 5.5e-06, eta: 8:46:38.319016, loss: 0.7699
2023-04-12 16:25:16 - training - INFO - Epoch [5/5][281/690] lr: 5.4e-06, eta: 8:27:46.499804, loss: 0.6222
2023-04-12 16:25:24 - training - INFO - Epoch [5/5][291/690] lr: 5.3e-06, eta: 8:10:10.236297, loss: 0.7958
2023-04-12 16:25:32 - training - INFO - Epoch [5/5][301/690] lr: 5.1e-06, eta: 7:53:45.276687, loss: 1.2288
2023-04-12 16:25:39 - training - INFO - Epoch [5/5][311/690] lr: 5.0e-06, eta: 7:38:21.629669, loss: 0.9655
2023-04-12 16:25:47 - training - INFO - Epoch [5/5][321/690] lr: 4.9e-06, eta: 7:23:57.033066, loss: 0.9372
2023-04-12 16:25:55 - training - INFO - Epoch [5/5][331/690] lr: 4.7e-06, eta: 7:10:21.758102, loss: 0.4000
2023-04-12 16:26:03 - training - INFO - Epoch [5/5][341/690] lr: 4.6e-06, eta: 6:57:32.536521, loss: 1.0066
2023-04-12 16:26:10 - training - INFO - Epoch [5/5][351/690] lr: 4.5e-06, eta: 6:45:28.123086, loss: 0.8711
2023-04-12 16:26:18 - training - INFO - Epoch [5/5][361/690] lr: 4.3e-06, eta: 6:34:05.834739, loss: 0.7645
2023-04-12 16:26:26 - training - INFO - Epoch [5/5][371/690] lr: 4.2e-06, eta: 6:23:16.761574, loss: 0.6807
2023-04-12 16:26:33 - training - INFO - Epoch [5/5][381/690] lr: 4.1e-06, eta: 6:13:00.771501, loss: 1.2044
2023-04-12 16:26:41 - training - INFO - Epoch [5/5][391/690] lr: 3.9e-06, eta: 6:03:18.339171, loss: 0.9136
2023-04-12 16:26:49 - training - INFO - Epoch [5/5][401/690] lr: 3.8e-06, eta: 5:54:04.370948, loss: 0.8636
2023-04-12 16:26:57 - training - INFO - Epoch [5/5][411/690] lr: 3.7e-06, eta: 5:45:18.145458, loss: 0.7009
2023-04-12 16:27:04 - training - INFO - Epoch [5/5][421/690] lr: 3.5e-06, eta: 5:36:53.713455, loss: 1.1156
2023-04-12 16:27:12 - training - INFO - Epoch [5/5][431/690] lr: 3.4e-06, eta: 5:28:53.201403, loss: 1.1927
2023-04-12 16:27:19 - training - INFO - Epoch [5/5][441/690] lr: 3.3e-06, eta: 5:21:13.686114, loss: 0.7588
2023-04-12 16:27:27 - training - INFO - Epoch [5/5][451/690] lr: 3.1e-06, eta: 5:13:54.262819, loss: 0.7798
2023-04-12 16:27:35 - training - INFO - Epoch [5/5][461/690] lr: 3.0e-06, eta: 5:06:54.227685, loss: 1.2439
2023-04-12 16:27:42 - training - INFO - Epoch [5/5][471/690] lr: 2.9e-06, eta: 5:00:10.563318, loss: 1.0003
2023-04-12 16:27:50 - training - INFO - Epoch [5/5][481/690] lr: 2.8e-06, eta: 4:53:46.130587, loss: 0.9566
2023-04-12 16:27:58 - training - INFO - Epoch [5/5][491/690] lr: 2.6e-06, eta: 4:47:34.710176, loss: 0.9145
2023-04-12 16:28:05 - training - INFO - Epoch [5/5][501/690] lr: 2.5e-06, eta: 4:41:37.280466, loss: 1.0941
2023-04-12 16:28:13 - training - INFO - Epoch [5/5][511/690] lr: 2.4e-06, eta: 4:35:54.493544, loss: 0.6195
2023-04-12 16:28:21 - training - INFO - Epoch [5/5][521/690] lr: 2.2e-06, eta: 4:30:24.715144, loss: 1.3294
2023-04-12 16:28:28 - training - INFO - Epoch [5/5][531/690] lr: 2.1e-06, eta: 4:25:06.156420, loss: 0.8403
2023-04-12 16:28:36 - training - INFO - Epoch [5/5][541/690] lr: 2.0e-06, eta: 4:19:59.579407, loss: 0.7124
2023-04-12 16:28:44 - training - INFO - Epoch [5/5][551/690] lr: 1.8e-06, eta: 4:15:05.719845, loss: 0.8492
2023-04-12 16:28:51 - training - INFO - Epoch [5/5][561/690] lr: 1.7e-06, eta: 4:10:19.700103, loss: 1.0211
2023-04-12 16:28:59 - training - INFO - Epoch [5/5][571/690] lr: 1.6e-06, eta: 4:05:43.644021, loss: 0.8670
2023-04-12 16:29:06 - training - INFO - Epoch [5/5][581/690] lr: 1.4e-06, eta: 4:01:16.804729, loss: 0.7645
2023-04-12 16:29:14 - training - INFO - Epoch [5/5][591/690] lr: 1.3e-06, eta: 3:56:59.908365, loss: 0.9639
2023-04-12 16:29:22 - training - INFO - Epoch [5/5][601/690] lr: 1.2e-06, eta: 3:52:50.837881, loss: 0.7777
2023-04-12 16:29:29 - training - INFO - Epoch [5/5][611/690] lr: 1.0e-06, eta: 3:48:48.716962, loss: 1.1593
2023-04-12 16:29:37 - training - INFO - Epoch [5/5][621/690] lr: 9.1e-07, eta: 3:44:55.835028, loss: 0.9832
2023-04-12 16:29:45 - training - INFO - Epoch [5/5][631/690] lr: 7.8e-07, eta: 3:41:09.658818, loss: 0.9591
2023-04-12 16:29:53 - training - INFO - Epoch [5/5][641/690] lr: 6.4e-07, eta: 3:37:29.378040, loss: 1.3987
2023-04-12 16:30:00 - training - INFO - Epoch [5/5][651/690] lr: 5.1e-07, eta: 3:33:56.295171, loss: 0.8641
2023-04-12 16:30:08 - training - INFO - Epoch [5/5][661/690] lr: 3.8e-07, eta: 3:30:28.608734, loss: 1.0362
2023-04-12 16:30:15 - training - INFO - Epoch [5/5][671/690] lr: 2.5e-07, eta: 3:27:07.890867, loss: 0.9618
2023-04-12 16:30:23 - training - INFO - Epoch [5/5][681/690] lr: 1.2e-07, eta: 3:23:51.999351, loss: 0.8243
2023-04-12 16:31:45 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 0.8854, Validation Metrics: {'exact_match': 23.39930151338766, 'f1': 28.123191473336437}, Test Metrics: {'exact_match': 25.551684088269454, 'f1': 30.509477371456185}
2023-04-12 16:32:22 - training - INFO - Final Test - Train Loss: 0.8854, Test Metrics: {'exact_match': 25.551684088269454, 'f1': 30.509477371456185}
