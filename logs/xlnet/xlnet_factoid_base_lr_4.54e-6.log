2023-04-12 11:42:24 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1380cc367820a3f3/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'xlnet-base-cased'}, 'data': {'task_type': 'factoid', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 4.54e-06, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/xlnet_factoid_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 597.37it/s]
Map:   0%|          | 0/4429 [00:00<?, ? examples/s]Map:  23%|██▎       | 1000/4429 [00:00<00:03, 1049.86 examples/s]Map:  45%|████▌     | 2000/4429 [00:01<00:02, 1191.64 examples/s]Map:  68%|██████▊   | 3000/4429 [00:02<00:01, 1247.73 examples/s]Map:  90%|█████████ | 4000/4429 [00:03<00:00, 1264.70 examples/s]Map: 100%|██████████| 4429/4429 [00:03<00:00, 1273.70 examples/s]                                                                 Map:   0%|          | 0/553 [00:00<?, ? examples/s]Map: 100%|██████████| 553/553 [00:00<00:00, 1011.46 examples/s]                                                               Map:   0%|          | 0/555 [00:00<?, ? examples/s]Map: 100%|██████████| 555/555 [00:00<00:00, 1034.82 examples/s]                                                               Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForQuestionAnsweringSimple: ['lm_loss.bias', 'lm_loss.weight']
- This IS expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForQuestionAnsweringSimple were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-12 11:43:39 - training - INFO - First Test - Val Metrics:{'exact_match': 0.7233273056057866, 'f1': 4.9508364099333395} Test Metrics: {'exact_match': 0.36036036036036034, 'f1': 4.856686845541333}
2023-04-12 11:43:40 - training - INFO - Epoch [1/5][1/438] lr: 4.5e-06, eta: 1 day, 13:58:28.353947, loss: 6.9248
2023-04-12 11:43:49 - training - INFO - Epoch [1/5][11/438] lr: 4.5e-06, eta: 3:53:46.830941, loss: 6.5198
2023-04-12 11:43:57 - training - INFO - Epoch [1/5][21/438] lr: 4.5e-06, eta: 2:17:13.317945, loss: 6.0563
2023-04-12 11:44:06 - training - INFO - Epoch [1/5][31/438] lr: 4.5e-06, eta: 1:42:43.536949, loss: 5.8525
2023-04-12 11:44:14 - training - INFO - Epoch [1/5][41/438] lr: 4.5e-06, eta: 1:24:30.260342, loss: 5.6144
2023-04-12 11:44:24 - training - INFO - Epoch [1/5][51/438] lr: 4.4e-06, eta: 1:14:08.007720, loss: 5.8427
2023-04-12 11:44:32 - training - INFO - Epoch [1/5][61/438] lr: 4.4e-06, eta: 1:06:27.895899, loss: 5.3576
2023-04-12 11:44:40 - training - INFO - Epoch [1/5][71/438] lr: 4.4e-06, eta: 1:00:56.669302, loss: 4.9989
2023-04-12 11:44:49 - training - INFO - Epoch [1/5][81/438] lr: 4.4e-06, eta: 0:56:47.635731, loss: 4.8825
2023-04-12 11:44:57 - training - INFO - Epoch [1/5][91/438] lr: 4.4e-06, eta: 0:53:25.941234, loss: 4.3809
2023-04-12 11:45:05 - training - INFO - Epoch [1/5][101/438] lr: 4.3e-06, eta: 0:50:48.465166, loss: 4.7781
2023-04-12 11:45:14 - training - INFO - Epoch [1/5][111/438] lr: 4.3e-06, eta: 0:48:46.961730, loss: 4.4260
2023-04-12 11:45:23 - training - INFO - Epoch [1/5][121/438] lr: 4.3e-06, eta: 0:47:02.521524, loss: 3.8031
2023-04-12 11:45:31 - training - INFO - Epoch [1/5][131/438] lr: 4.3e-06, eta: 0:45:31.333506, loss: 4.6669
2023-04-12 11:45:39 - training - INFO - Epoch [1/5][141/438] lr: 4.2e-06, eta: 0:44:01.212225, loss: 4.2098
2023-04-12 11:45:48 - training - INFO - Epoch [1/5][151/438] lr: 4.2e-06, eta: 0:42:47.227418, loss: 4.2788
2023-04-12 11:45:56 - training - INFO - Epoch [1/5][161/438] lr: 4.2e-06, eta: 0:41:43.613535, loss: 4.1192
2023-04-12 11:46:05 - training - INFO - Epoch [1/5][171/438] lr: 4.2e-06, eta: 0:40:44.922183, loss: 3.9763
2023-04-12 11:46:14 - training - INFO - Epoch [1/5][181/438] lr: 4.2e-06, eta: 0:39:58.567199, loss: 3.7597
2023-04-12 11:46:22 - training - INFO - Epoch [1/5][191/438] lr: 4.1e-06, eta: 0:39:11.389717, loss: 3.8822
2023-04-12 11:46:31 - training - INFO - Epoch [1/5][201/438] lr: 4.1e-06, eta: 0:38:32.363664, loss: 4.0967
2023-04-12 11:46:40 - training - INFO - Epoch [1/5][211/438] lr: 4.1e-06, eta: 0:37:54.371687, loss: 3.6150
2023-04-12 11:46:49 - training - INFO - Epoch [1/5][221/438] lr: 4.1e-06, eta: 0:37:17.815756, loss: 4.0198
2023-04-12 11:46:58 - training - INFO - Epoch [1/5][231/438] lr: 4.1e-06, eta: 0:36:45.179694, loss: 3.6088
2023-04-12 11:47:07 - training - INFO - Epoch [1/5][241/438] lr: 4.0e-06, eta: 0:36:16.652945, loss: 3.5876
2023-04-12 11:47:14 - training - INFO - Epoch [1/5][251/438] lr: 4.0e-06, eta: 0:35:37.125081, loss: 4.4038
2023-04-12 11:47:22 - training - INFO - Epoch [1/5][261/438] lr: 4.0e-06, eta: 0:35:00.397437, loss: 3.4100
2023-04-12 11:47:30 - training - INFO - Epoch [1/5][271/438] lr: 4.0e-06, eta: 0:34:27.334862, loss: 3.4539
2023-04-12 11:47:37 - training - INFO - Epoch [1/5][281/438] lr: 4.0e-06, eta: 0:33:54.329668, loss: 4.3411
2023-04-12 11:47:45 - training - INFO - Epoch [1/5][291/438] lr: 3.9e-06, eta: 0:33:23.010129, loss: 4.1238
2023-04-12 11:47:52 - training - INFO - Epoch [1/5][301/438] lr: 3.9e-06, eta: 0:32:54.909831, loss: 3.3489
2023-04-12 11:48:00 - training - INFO - Epoch [1/5][311/438] lr: 3.9e-06, eta: 0:32:27.850318, loss: 3.5463
2023-04-12 11:48:08 - training - INFO - Epoch [1/5][321/438] lr: 3.9e-06, eta: 0:32:00.618042, loss: 3.6135
2023-04-12 11:48:16 - training - INFO - Epoch [1/5][331/438] lr: 3.9e-06, eta: 0:31:38.585546, loss: 3.4310
2023-04-12 11:48:24 - training - INFO - Epoch [1/5][341/438] lr: 3.8e-06, eta: 0:31:15.490623, loss: 3.5634
2023-04-12 11:48:31 - training - INFO - Epoch [1/5][351/438] lr: 3.8e-06, eta: 0:30:52.307004, loss: 2.5998
2023-04-12 11:48:39 - training - INFO - Epoch [1/5][361/438] lr: 3.8e-06, eta: 0:30:29.985831, loss: 3.4484
2023-04-12 11:48:47 - training - INFO - Epoch [1/5][371/438] lr: 3.8e-06, eta: 0:30:08.593501, loss: 3.3837
2023-04-12 11:48:54 - training - INFO - Epoch [1/5][381/438] lr: 3.8e-06, eta: 0:29:46.798143, loss: 3.4291
2023-04-12 11:49:02 - training - INFO - Epoch [1/5][391/438] lr: 3.7e-06, eta: 0:29:28.355834, loss: 3.5206
2023-04-12 11:49:10 - training - INFO - Epoch [1/5][401/438] lr: 3.7e-06, eta: 0:29:09.702826, loss: 3.7158
2023-04-12 11:49:18 - training - INFO - Epoch [1/5][411/438] lr: 3.7e-06, eta: 0:28:50.694813, loss: 2.7396
2023-04-12 11:49:25 - training - INFO - Epoch [1/5][421/438] lr: 3.7e-06, eta: 0:28:31.978054, loss: 2.9463
2023-04-12 11:49:33 - training - INFO - Epoch [1/5][431/438] lr: 3.6e-06, eta: 0:28:15.114879, loss: 2.8063
2023-04-12 11:50:27 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 4.2021, Validation Metrics: {'exact_match': 23.688969258589513, 'f1': 28.472890724628936}, Test Metrics: {'exact_match': 24.864864864864863, 'f1': 30.04781576590363}
2023-04-12 11:50:28 - training - INFO - Epoch [2/5][1/438] lr: 3.6e-06, eta: 11 days, 21:39:29.298682, loss: 2.7147
2023-04-12 11:50:35 - training - INFO - Epoch [2/5][11/438] lr: 3.6e-06, eta: 1 day, 2:16:17.799738, loss: 3.7813
2023-04-12 11:50:43 - training - INFO - Epoch [2/5][21/438] lr: 3.6e-06, eta: 13:55:08.331267, loss: 3.6071
2023-04-12 11:50:50 - training - INFO - Epoch [2/5][31/438] lr: 3.6e-06, eta: 9:31:48.755360, loss: 3.1364
2023-04-12 11:50:58 - training - INFO - Epoch [2/5][41/438] lr: 3.5e-06, eta: 7:17:00.552869, loss: 2.7360
2023-04-12 11:51:05 - training - INFO - Epoch [2/5][51/438] lr: 3.5e-06, eta: 5:54:55.460478, loss: 3.0078
2023-04-12 11:51:13 - training - INFO - Epoch [2/5][61/438] lr: 3.5e-06, eta: 4:59:44.278281, loss: 2.6519
2023-04-12 11:51:21 - training - INFO - Epoch [2/5][71/438] lr: 3.5e-06, eta: 4:20:02.459756, loss: 2.7904
2023-04-12 11:51:28 - training - INFO - Epoch [2/5][81/438] lr: 3.5e-06, eta: 3:50:05.583597, loss: 2.1038
2023-04-12 11:51:36 - training - INFO - Epoch [2/5][91/438] lr: 3.4e-06, eta: 3:26:48.055887, loss: 2.1967
2023-04-12 11:51:43 - training - INFO - Epoch [2/5][101/438] lr: 3.4e-06, eta: 3:08:01.905625, loss: 2.6607
2023-04-12 11:51:51 - training - INFO - Epoch [2/5][111/438] lr: 3.4e-06, eta: 2:52:38.411679, loss: 2.9836
2023-04-12 11:51:58 - training - INFO - Epoch [2/5][121/438] lr: 3.4e-06, eta: 2:39:46.241837, loss: 2.3950
2023-04-12 11:52:06 - training - INFO - Epoch [2/5][131/438] lr: 3.4e-06, eta: 2:28:50.473933, loss: 3.3728
2023-04-12 11:52:13 - training - INFO - Epoch [2/5][141/438] lr: 3.3e-06, eta: 2:19:25.956354, loss: 3.6825
2023-04-12 11:52:21 - training - INFO - Epoch [2/5][151/438] lr: 3.3e-06, eta: 2:11:14.605766, loss: 2.7188
2023-04-12 11:52:29 - training - INFO - Epoch [2/5][161/438] lr: 3.3e-06, eta: 2:04:06.647103, loss: 3.3806
2023-04-12 11:52:36 - training - INFO - Epoch [2/5][171/438] lr: 3.3e-06, eta: 1:57:45.781236, loss: 2.3998
2023-04-12 11:52:44 - training - INFO - Epoch [2/5][181/438] lr: 3.3e-06, eta: 1:52:08.315783, loss: 2.3959
2023-04-12 11:52:52 - training - INFO - Epoch [2/5][191/438] lr: 3.2e-06, eta: 1:47:05.491647, loss: 2.5976
2023-04-12 11:52:59 - training - INFO - Epoch [2/5][201/438] lr: 3.2e-06, eta: 1:42:29.986011, loss: 2.3540
2023-04-12 11:53:07 - training - INFO - Epoch [2/5][211/438] lr: 3.2e-06, eta: 1:38:22.250739, loss: 2.9604
2023-04-12 11:53:14 - training - INFO - Epoch [2/5][221/438] lr: 3.2e-06, eta: 1:34:33.297421, loss: 3.1232
2023-04-12 11:53:22 - training - INFO - Epoch [2/5][231/438] lr: 3.2e-06, eta: 1:31:06.409272, loss: 2.6834
2023-04-12 11:53:30 - training - INFO - Epoch [2/5][241/438] lr: 3.1e-06, eta: 1:27:54.602088, loss: 2.8478
2023-04-12 11:53:38 - training - INFO - Epoch [2/5][251/438] lr: 3.1e-06, eta: 1:24:57.929606, loss: 2.5189
2023-04-12 11:53:46 - training - INFO - Epoch [2/5][261/438] lr: 3.1e-06, eta: 1:22:15.728442, loss: 2.3166
2023-04-12 11:53:53 - training - INFO - Epoch [2/5][271/438] lr: 3.1e-06, eta: 1:19:43.677443, loss: 2.2853
2023-04-12 11:54:01 - training - INFO - Epoch [2/5][281/438] lr: 3.0e-06, eta: 1:17:20.546102, loss: 1.8050
2023-04-12 11:54:08 - training - INFO - Epoch [2/5][291/438] lr: 3.0e-06, eta: 1:15:07.215732, loss: 1.7462
2023-04-12 11:54:16 - training - INFO - Epoch [2/5][301/438] lr: 3.0e-06, eta: 1:13:02.555560, loss: 2.3580
2023-04-12 11:54:24 - training - INFO - Epoch [2/5][311/438] lr: 3.0e-06, eta: 1:11:06.090995, loss: 2.1284
2023-04-12 11:54:32 - training - INFO - Epoch [2/5][321/438] lr: 3.0e-06, eta: 1:09:16.915791, loss: 3.4420
2023-04-12 11:54:40 - training - INFO - Epoch [2/5][331/438] lr: 2.9e-06, eta: 1:07:36.423514, loss: 2.7282
2023-04-12 11:54:48 - training - INFO - Epoch [2/5][341/438] lr: 2.9e-06, eta: 1:05:59.036273, loss: 3.6219
2023-04-12 11:54:56 - training - INFO - Epoch [2/5][351/438] lr: 2.9e-06, eta: 1:04:26.311761, loss: 2.2831
2023-04-12 11:55:03 - training - INFO - Epoch [2/5][361/438] lr: 2.9e-06, eta: 1:02:57.739143, loss: 1.8740
2023-04-12 11:55:11 - training - INFO - Epoch [2/5][371/438] lr: 2.9e-06, eta: 1:01:33.796006, loss: 2.6421
2023-04-12 11:55:19 - training - INFO - Epoch [2/5][381/438] lr: 2.8e-06, eta: 1:00:15.357051, loss: 2.3015
2023-04-12 11:55:27 - training - INFO - Epoch [2/5][391/438] lr: 2.8e-06, eta: 0:58:58.237220, loss: 1.7494
2023-04-12 11:55:34 - training - INFO - Epoch [2/5][401/438] lr: 2.8e-06, eta: 0:57:44.394922, loss: 2.3135
2023-04-12 11:55:42 - training - INFO - Epoch [2/5][411/438] lr: 2.8e-06, eta: 0:56:34.766076, loss: 2.8086
2023-04-12 11:55:50 - training - INFO - Epoch [2/5][421/438] lr: 2.8e-06, eta: 0:55:27.425316, loss: 2.1263
2023-04-12 11:55:57 - training - INFO - Epoch [2/5][431/438] lr: 2.7e-06, eta: 0:54:23.701370, loss: 2.5216
2023-04-12 11:56:51 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 2.5631, Validation Metrics: {'exact_match': 30.379746835443036, 'f1': 35.389479058350034}, Test Metrics: {'exact_match': 29.36936936936937, 'f1': 34.34791758423356}
2023-04-12 11:56:52 - training - INFO - Epoch [3/5][1/438] lr: 2.7e-06, eta: 21 days, 15:23:06.699935, loss: 2.5918
2023-04-12 11:57:00 - training - INFO - Epoch [3/5][11/438] lr: 2.7e-06, eta: 1 day, 23:25:12.903011, loss: 1.9214
2023-04-12 11:57:07 - training - INFO - Epoch [3/5][21/438] lr: 2.7e-06, eta: 1 day, 0:56:40.877268, loss: 1.9451
2023-04-12 11:57:15 - training - INFO - Epoch [3/5][31/438] lr: 2.7e-06, eta: 16:58:09.131695, loss: 2.8080
2023-04-12 11:57:23 - training - INFO - Epoch [3/5][41/438] lr: 2.6e-06, eta: 12:52:59.212985, loss: 1.5413
2023-04-12 11:57:30 - training - INFO - Epoch [3/5][51/438] lr: 2.6e-06, eta: 10:23:50.936391, loss: 2.2404
2023-04-12 11:57:38 - training - INFO - Epoch [3/5][61/438] lr: 2.6e-06, eta: 8:43:36.320246, loss: 1.6586
2023-04-12 11:57:46 - training - INFO - Epoch [3/5][71/438] lr: 2.6e-06, eta: 7:31:37.280392, loss: 1.9464
2023-04-12 11:57:53 - training - INFO - Epoch [3/5][81/438] lr: 2.6e-06, eta: 6:37:17.670579, loss: 1.5352
2023-04-12 11:58:01 - training - INFO - Epoch [3/5][91/438] lr: 2.5e-06, eta: 5:54:57.744885, loss: 2.0843
2023-04-12 11:58:09 - training - INFO - Epoch [3/5][101/438] lr: 2.5e-06, eta: 5:20:57.818342, loss: 2.2179
2023-04-12 11:58:16 - training - INFO - Epoch [3/5][111/438] lr: 2.5e-06, eta: 4:53:00.599883, loss: 3.1935
2023-04-12 11:58:24 - training - INFO - Epoch [3/5][121/438] lr: 2.5e-06, eta: 4:29:45.174576, loss: 2.4693
2023-04-12 11:58:32 - training - INFO - Epoch [3/5][131/438] lr: 2.5e-06, eta: 4:09:56.327054, loss: 2.0999
2023-04-12 11:58:40 - training - INFO - Epoch [3/5][141/438] lr: 2.4e-06, eta: 3:52:57.470694, loss: 1.6076
2023-04-12 11:58:47 - training - INFO - Epoch [3/5][151/438] lr: 2.4e-06, eta: 3:38:10.573705, loss: 1.9691
2023-04-12 11:58:55 - training - INFO - Epoch [3/5][161/438] lr: 2.4e-06, eta: 3:25:12.087653, loss: 2.3924
2023-04-12 11:59:02 - training - INFO - Epoch [3/5][171/438] lr: 2.4e-06, eta: 3:13:45.074922, loss: 1.4312
2023-04-12 11:59:10 - training - INFO - Epoch [3/5][181/438] lr: 2.3e-06, eta: 3:03:35.061722, loss: 2.4295
2023-04-12 11:59:18 - training - INFO - Epoch [3/5][191/438] lr: 2.3e-06, eta: 2:54:26.975894, loss: 1.9540
2023-04-12 11:59:25 - training - INFO - Epoch [3/5][201/438] lr: 2.3e-06, eta: 2:46:10.475112, loss: 1.6373
2023-04-12 11:59:33 - training - INFO - Epoch [3/5][211/438] lr: 2.3e-06, eta: 2:38:41.982248, loss: 2.5558
2023-04-12 11:59:41 - training - INFO - Epoch [3/5][221/438] lr: 2.3e-06, eta: 2:31:55.692245, loss: 2.5173
2023-04-12 11:59:49 - training - INFO - Epoch [3/5][231/438] lr: 2.2e-06, eta: 2:25:42.045336, loss: 1.6938
2023-04-12 11:59:56 - training - INFO - Epoch [3/5][241/438] lr: 2.2e-06, eta: 2:19:56.903986, loss: 2.4981
2023-04-12 12:00:04 - training - INFO - Epoch [3/5][251/438] lr: 2.2e-06, eta: 2:14:40.778622, loss: 2.5053
2023-04-12 12:00:12 - training - INFO - Epoch [3/5][261/438] lr: 2.2e-06, eta: 2:09:49.494900, loss: 2.7661
2023-04-12 12:00:19 - training - INFO - Epoch [3/5][271/438] lr: 2.2e-06, eta: 2:05:17.642201, loss: 2.0539
2023-04-12 12:00:27 - training - INFO - Epoch [3/5][281/438] lr: 2.1e-06, eta: 2:01:06.717313, loss: 1.4642
2023-04-12 12:00:36 - training - INFO - Epoch [3/5][291/438] lr: 2.1e-06, eta: 1:57:14.439114, loss: 1.8529
2023-04-12 12:00:43 - training - INFO - Epoch [3/5][301/438] lr: 2.1e-06, eta: 1:53:33.515327, loss: 1.9499
2023-04-12 12:00:51 - training - INFO - Epoch [3/5][311/438] lr: 2.1e-06, eta: 1:50:06.932284, loss: 1.5664
2023-04-12 12:00:59 - training - INFO - Epoch [3/5][321/438] lr: 2.1e-06, eta: 1:46:53.692173, loss: 2.3175
2023-04-12 12:01:07 - training - INFO - Epoch [3/5][331/438] lr: 2.0e-06, eta: 1:43:51.265755, loss: 2.2089
2023-04-12 12:01:15 - training - INFO - Epoch [3/5][341/438] lr: 2.0e-06, eta: 1:40:58.487021, loss: 1.4013
2023-04-12 12:01:23 - training - INFO - Epoch [3/5][351/438] lr: 2.0e-06, eta: 1:38:13.402842, loss: 2.4017
2023-04-12 12:01:30 - training - INFO - Epoch [3/5][361/438] lr: 2.0e-06, eta: 1:35:37.530933, loss: 2.2468
2023-04-12 12:01:38 - training - INFO - Epoch [3/5][371/438] lr: 2.0e-06, eta: 1:33:08.984821, loss: 1.9432
2023-04-12 12:01:45 - training - INFO - Epoch [3/5][381/438] lr: 1.9e-06, eta: 1:30:49.010103, loss: 2.0597
2023-04-12 12:01:53 - training - INFO - Epoch [3/5][391/438] lr: 1.9e-06, eta: 1:28:35.289420, loss: 2.0379
2023-04-12 12:02:01 - training - INFO - Epoch [3/5][401/438] lr: 1.9e-06, eta: 1:26:28.727939, loss: 2.2708
2023-04-12 12:02:09 - training - INFO - Epoch [3/5][411/438] lr: 1.9e-06, eta: 1:24:29.691018, loss: 1.2433
2023-04-12 12:02:17 - training - INFO - Epoch [3/5][421/438] lr: 1.9e-06, eta: 1:22:33.399897, loss: 1.4221
2023-04-12 12:02:24 - training - INFO - Epoch [3/5][431/438] lr: 1.8e-06, eta: 1:20:42.147056, loss: 1.3276
2023-04-12 12:03:18 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 2.0827, Validation Metrics: {'exact_match': 37.43218806509946, 'f1': 43.28839360009219}, Test Metrics: {'exact_match': 41.08108108108108, 'f1': 46.66154050026383}
2023-04-12 12:03:19 - training - INFO - Epoch [4/5][1/438] lr: 1.8e-06, eta: 31 days, 10:44:49.223310, loss: 1.9901
2023-04-12 12:03:26 - training - INFO - Epoch [4/5][11/438] lr: 1.8e-06, eta: 2 days, 20:42:50.262467, loss: 2.0743
2023-04-12 12:03:34 - training - INFO - Epoch [4/5][21/438] lr: 1.8e-06, eta: 1 day, 12:02:41.407557, loss: 1.6517
2023-04-12 12:03:42 - training - INFO - Epoch [4/5][31/438] lr: 1.8e-06, eta: 1 day, 0:27:07.199231, loss: 1.8889
2023-04-12 12:03:49 - training - INFO - Epoch [4/5][41/438] lr: 1.7e-06, eta: 18:30:41.787996, loss: 1.5785
2023-04-12 12:03:57 - training - INFO - Epoch [4/5][51/438] lr: 1.7e-06, eta: 14:54:14.954070, loss: 1.3294
2023-04-12 12:04:05 - training - INFO - Epoch [4/5][61/438] lr: 1.7e-06, eta: 12:28:32.864524, loss: 2.2067
2023-04-12 12:04:12 - training - INFO - Epoch [4/5][71/438] lr: 1.7e-06, eta: 10:43:50.967726, loss: 2.0557
2023-04-12 12:04:20 - training - INFO - Epoch [4/5][81/438] lr: 1.6e-06, eta: 9:24:58.684605, loss: 2.1473
2023-04-12 12:04:27 - training - INFO - Epoch [4/5][91/438] lr: 1.6e-06, eta: 8:23:28.092241, loss: 2.0084
2023-04-12 12:04:35 - training - INFO - Epoch [4/5][101/438] lr: 1.6e-06, eta: 7:34:04.430917, loss: 1.9311
2023-04-12 12:04:43 - training - INFO - Epoch [4/5][111/438] lr: 1.6e-06, eta: 6:53:40.781832, loss: 2.3277
2023-04-12 12:04:51 - training - INFO - Epoch [4/5][121/438] lr: 1.6e-06, eta: 6:19:53.746786, loss: 2.3009
2023-04-12 12:04:59 - training - INFO - Epoch [4/5][131/438] lr: 1.5e-06, eta: 5:51:13.893826, loss: 2.1607
2023-04-12 12:05:06 - training - INFO - Epoch [4/5][141/438] lr: 1.5e-06, eta: 5:26:34.515285, loss: 2.3382
2023-04-12 12:05:14 - training - INFO - Epoch [4/5][151/438] lr: 1.5e-06, eta: 5:05:09.616456, loss: 1.7848
2023-04-12 12:05:21 - training - INFO - Epoch [4/5][161/438] lr: 1.5e-06, eta: 4:46:26.695225, loss: 2.2664
2023-04-12 12:05:29 - training - INFO - Epoch [4/5][171/438] lr: 1.5e-06, eta: 4:29:54.392943, loss: 1.4301
2023-04-12 12:05:37 - training - INFO - Epoch [4/5][181/438] lr: 1.4e-06, eta: 4:15:07.137538, loss: 1.0647
2023-04-12 12:05:44 - training - INFO - Epoch [4/5][191/438] lr: 1.4e-06, eta: 4:01:52.590075, loss: 2.1371
2023-04-12 12:05:52 - training - INFO - Epoch [4/5][201/438] lr: 1.4e-06, eta: 3:49:56.821818, loss: 1.9047
2023-04-12 12:06:00 - training - INFO - Epoch [4/5][211/438] lr: 1.4e-06, eta: 3:39:09.815783, loss: 2.2688
2023-04-12 12:06:07 - training - INFO - Epoch [4/5][221/438] lr: 1.4e-06, eta: 3:29:18.545846, loss: 1.8888
2023-04-12 12:06:15 - training - INFO - Epoch [4/5][231/438] lr: 1.3e-06, eta: 3:20:18.903816, loss: 2.6077
2023-04-12 12:06:23 - training - INFO - Epoch [4/5][241/438] lr: 1.3e-06, eta: 3:12:04.475980, loss: 1.7122
2023-04-12 12:06:30 - training - INFO - Epoch [4/5][251/438] lr: 1.3e-06, eta: 3:04:27.641368, loss: 2.8189
2023-04-12 12:06:39 - training - INFO - Epoch [4/5][261/438] lr: 1.3e-06, eta: 2:57:28.569966, loss: 1.5908
2023-04-12 12:06:47 - training - INFO - Epoch [4/5][271/438] lr: 1.3e-06, eta: 2:50:59.559295, loss: 2.5302
2023-04-12 12:06:54 - training - INFO - Epoch [4/5][281/438] lr: 1.2e-06, eta: 2:44:53.808662, loss: 1.7636
2023-04-12 12:07:02 - training - INFO - Epoch [4/5][291/438] lr: 1.2e-06, eta: 2:39:14.326659, loss: 1.7967
2023-04-12 12:07:09 - training - INFO - Epoch [4/5][301/438] lr: 1.2e-06, eta: 2:33:55.387115, loss: 1.6160
2023-04-12 12:07:17 - training - INFO - Epoch [4/5][311/438] lr: 1.2e-06, eta: 2:28:57.206077, loss: 1.7846
2023-04-12 12:07:24 - training - INFO - Epoch [4/5][321/438] lr: 1.2e-06, eta: 2:24:16.417413, loss: 2.3275
2023-04-12 12:07:32 - training - INFO - Epoch [4/5][331/438] lr: 1.1e-06, eta: 2:19:53.230703, loss: 2.5040
2023-04-12 12:07:40 - training - INFO - Epoch [4/5][341/438] lr: 1.1e-06, eta: 2:15:45.597543, loss: 1.4412
2023-04-12 12:07:47 - training - INFO - Epoch [4/5][351/438] lr: 1.1e-06, eta: 2:11:50.180811, loss: 1.5138
2023-04-12 12:07:55 - training - INFO - Epoch [4/5][361/438] lr: 1.1e-06, eta: 2:08:08.817873, loss: 1.9709
2023-04-12 12:08:03 - training - INFO - Epoch [4/5][371/438] lr: 1.0e-06, eta: 2:04:38.220049, loss: 1.8498
2023-04-12 12:08:11 - training - INFO - Epoch [4/5][381/438] lr: 1.0e-06, eta: 2:01:19.274898, loss: 1.8607
2023-04-12 12:08:19 - training - INFO - Epoch [4/5][391/438] lr: 1.0e-06, eta: 1:58:09.648517, loss: 1.8069
2023-04-12 12:08:26 - training - INFO - Epoch [4/5][401/438] lr: 9.8e-07, eta: 1:55:08.075013, loss: 2.3084
2023-04-12 12:08:34 - training - INFO - Epoch [4/5][411/438] lr: 9.6e-07, eta: 1:52:16.594449, loss: 1.7241
2023-04-12 12:08:42 - training - INFO - Epoch [4/5][421/438] lr: 9.4e-07, eta: 1:49:31.865073, loss: 2.3759
2023-04-12 12:08:49 - training - INFO - Epoch [4/5][431/438] lr: 9.2e-07, eta: 1:46:54.686020, loss: 2.4423
2023-04-12 12:09:43 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 1.8757, Validation Metrics: {'exact_match': 35.98553345388788, 'f1': 42.479173381568515}, Test Metrics: {'exact_match': 38.01801801801802, 'f1': 43.37901899272426}
2023-04-12 12:09:44 - training - INFO - Epoch [5/5][1/438] lr: 9.1e-07, eta: 41 days, 4:58:07.739437, loss: 1.8880
2023-04-12 12:09:52 - training - INFO - Epoch [5/5][11/438] lr: 8.9e-07, eta: 3 days, 17:54:57.361788, loss: 1.8046
2023-04-12 12:09:59 - training - INFO - Epoch [5/5][21/438] lr: 8.6e-07, eta: 1 day, 23:05:52.718973, loss: 2.4787
2023-04-12 12:10:07 - training - INFO - Epoch [5/5][31/438] lr: 8.4e-07, eta: 1 day, 7:54:10.664888, loss: 1.1219
2023-04-12 12:10:15 - training - INFO - Epoch [5/5][41/438] lr: 8.2e-07, eta: 1 day, 0:07:28.411643, loss: 1.6546
2023-04-12 12:10:22 - training - INFO - Epoch [5/5][51/438] lr: 8.0e-07, eta: 19:23:26.553765, loss: 2.0850
2023-04-12 12:10:30 - training - INFO - Epoch [5/5][61/438] lr: 7.8e-07, eta: 16:12:47.533501, loss: 1.4514
2023-04-12 12:10:38 - training - INFO - Epoch [5/5][71/438] lr: 7.6e-07, eta: 13:55:37.909042, loss: 1.7681
2023-04-12 12:10:45 - training - INFO - Epoch [5/5][81/438] lr: 7.4e-07, eta: 12:12:18.536925, loss: 1.8811
2023-04-12 12:10:53 - training - INFO - Epoch [5/5][91/438] lr: 7.2e-07, eta: 10:51:36.381206, loss: 1.6053
2023-04-12 12:11:00 - training - INFO - Epoch [5/5][101/438] lr: 7.0e-07, eta: 9:46:54.555015, loss: 1.5075
2023-04-12 12:11:08 - training - INFO - Epoch [5/5][111/438] lr: 6.8e-07, eta: 8:53:53.225763, loss: 2.4686
2023-04-12 12:11:15 - training - INFO - Epoch [5/5][121/438] lr: 6.6e-07, eta: 8:09:32.020560, loss: 1.9338
2023-04-12 12:11:23 - training - INFO - Epoch [5/5][131/438] lr: 6.4e-07, eta: 7:31:58.226279, loss: 2.0427
2023-04-12 12:11:31 - training - INFO - Epoch [5/5][141/438] lr: 6.2e-07, eta: 6:59:42.238686, loss: 1.3928
2023-04-12 12:11:38 - training - INFO - Epoch [5/5][151/438] lr: 5.9e-07, eta: 6:31:40.953275, loss: 1.3625
2023-04-12 12:11:46 - training - INFO - Epoch [5/5][161/438] lr: 5.7e-07, eta: 6:07:10.886058, loss: 1.4694
2023-04-12 12:11:53 - training - INFO - Epoch [5/5][171/438] lr: 5.5e-07, eta: 5:45:30.137013, loss: 1.3944
2023-04-12 12:12:01 - training - INFO - Epoch [5/5][181/438] lr: 5.3e-07, eta: 5:26:13.160642, loss: 1.1365
2023-04-12 12:12:09 - training - INFO - Epoch [5/5][191/438] lr: 5.1e-07, eta: 5:08:54.618055, loss: 2.2787
2023-04-12 12:12:16 - training - INFO - Epoch [5/5][201/438] lr: 4.9e-07, eta: 4:53:18.594429, loss: 1.3210
2023-04-12 12:12:24 - training - INFO - Epoch [5/5][211/438] lr: 4.7e-07, eta: 4:39:11.552245, loss: 1.6620
2023-04-12 12:12:31 - training - INFO - Epoch [5/5][221/438] lr: 4.5e-07, eta: 4:26:20.041704, loss: 1.3777
2023-04-12 12:12:39 - training - INFO - Epoch [5/5][231/438] lr: 4.3e-07, eta: 4:14:35.839266, loss: 1.3922
2023-04-12 12:12:47 - training - INFO - Epoch [5/5][241/438] lr: 4.1e-07, eta: 4:03:49.889793, loss: 1.3380
2023-04-12 12:12:54 - training - INFO - Epoch [5/5][251/438] lr: 3.9e-07, eta: 3:53:54.010823, loss: 1.6740
2023-04-12 12:13:02 - training - INFO - Epoch [5/5][261/438] lr: 3.7e-07, eta: 3:44:42.610470, loss: 1.8318
2023-04-12 12:13:10 - training - INFO - Epoch [5/5][271/438] lr: 3.5e-07, eta: 3:36:11.923789, loss: 1.5507
2023-04-12 12:13:17 - training - INFO - Epoch [5/5][281/438] lr: 3.3e-07, eta: 3:28:15.871112, loss: 1.3542
2023-04-12 12:13:25 - training - INFO - Epoch [5/5][291/438] lr: 3.0e-07, eta: 3:20:51.976914, loss: 1.5471
2023-04-12 12:13:32 - training - INFO - Epoch [5/5][301/438] lr: 2.8e-07, eta: 3:13:57.305396, loss: 1.3020
2023-04-12 12:13:40 - training - INFO - Epoch [5/5][311/438] lr: 2.6e-07, eta: 3:07:30.576386, loss: 1.5159
2023-04-12 12:13:47 - training - INFO - Epoch [5/5][321/438] lr: 2.4e-07, eta: 3:01:26.437191, loss: 1.2192
2023-04-12 12:13:55 - training - INFO - Epoch [5/5][331/438] lr: 2.2e-07, eta: 2:55:43.197665, loss: 2.0263
2023-04-12 12:14:03 - training - INFO - Epoch [5/5][341/438] lr: 2.0e-07, eta: 2:50:20.029472, loss: 1.9805
2023-04-12 12:14:10 - training - INFO - Epoch [5/5][351/438] lr: 1.8e-07, eta: 2:45:16.160172, loss: 1.8814
2023-04-12 12:14:18 - training - INFO - Epoch [5/5][361/438] lr: 1.6e-07, eta: 2:40:27.188415, loss: 1.3680
2023-04-12 12:14:25 - training - INFO - Epoch [5/5][371/438] lr: 1.4e-07, eta: 2:35:53.219783, loss: 1.9734
2023-04-12 12:14:33 - training - INFO - Epoch [5/5][381/438] lr: 1.2e-07, eta: 2:31:34.988097, loss: 1.4828
2023-04-12 12:14:41 - training - INFO - Epoch [5/5][391/438] lr: 9.7e-08, eta: 2:27:28.613571, loss: 1.8786
2023-04-12 12:14:49 - training - INFO - Epoch [5/5][401/438] lr: 7.7e-08, eta: 2:23:34.160230, loss: 2.1168
2023-04-12 12:14:56 - training - INFO - Epoch [5/5][411/438] lr: 5.6e-08, eta: 2:19:50.171391, loss: 2.4811
2023-04-12 12:15:04 - training - INFO - Epoch [5/5][421/438] lr: 3.5e-08, eta: 2:16:17.492616, loss: 1.2533
2023-04-12 12:15:12 - training - INFO - Epoch [5/5][431/438] lr: 1.5e-08, eta: 2:12:53.830199, loss: 2.0750
2023-04-12 12:16:05 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 1.7560, Validation Metrics: {'exact_match': 37.070524412296564, 'f1': 43.20272032563232}, Test Metrics: {'exact_match': 39.63963963963964, 'f1': 45.13674823200361}
2023-04-12 12:16:29 - training - INFO - Final Test - Train Loss: 1.7560, Test Metrics: {'exact_match': 39.63963963963964, 'f1': 45.13674823200361}
