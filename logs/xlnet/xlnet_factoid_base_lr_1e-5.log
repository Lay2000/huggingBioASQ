2023-04-12 11:08:22 - datasets.builder - WARNING - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1380cc367820a3f3/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
{'model': {'model_checkpoint': 'xlnet-base-cased'}, 'data': {'task_type': 'factoid', 'max_length': 384, 'stride': 128}, 'hyperparameters': {'batch_size': 16, 'train_epochs': 5, 'lr': 1e-05, 'optimizer': 'AdamW', 'scheduler': 'linear', 'num_warmup_steps': 0}, 'others': {'n_best': 20, 'max_answer_length': 30, 'output_dir': 'models/xlnet_factoid_base'}}
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 599.04it/s]
Map:   0%|          | 0/4429 [00:00<?, ? examples/s]Map:  23%|██▎       | 1000/4429 [00:00<00:02, 1243.09 examples/s]Map:  45%|████▌     | 2000/4429 [00:01<00:01, 1428.96 examples/s]Map:  68%|██████▊   | 3000/4429 [00:02<00:00, 1497.44 examples/s]Map:  90%|█████████ | 4000/4429 [00:02<00:00, 1526.73 examples/s]Map: 100%|██████████| 4429/4429 [00:02<00:00, 1533.76 examples/s]                                                                 Map:   0%|          | 0/553 [00:00<?, ? examples/s]Map: 100%|██████████| 553/553 [00:00<00:00, 1255.53 examples/s]                                                               Map:   0%|          | 0/555 [00:00<?, ? examples/s]Map: 100%|██████████| 555/555 [00:00<00:00, 1236.77 examples/s]                                                               Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForQuestionAnsweringSimple: ['lm_loss.weight', 'lm_loss.bias']
- This IS expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForQuestionAnsweringSimple from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForQuestionAnsweringSimple were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-04-12 11:09:21 - training - INFO - First Test - Val Metrics:{'exact_match': 0.7233273056057866, 'f1': 4.9508364099333395} Test Metrics: {'exact_match': 0.36036036036036034, 'f1': 4.856686845541333}
2023-04-12 11:09:22 - training - INFO - Epoch [1/5][1/438] lr: 1.0e-05, eta: 1 day, 6:06:10.564698, loss: 6.9248
2023-04-12 11:09:29 - training - INFO - Epoch [1/5][11/438] lr: 9.9e-06, eta: 3:08:23.797832, loss: 6.0182
2023-04-12 11:09:37 - training - INFO - Epoch [1/5][21/438] lr: 9.9e-06, eta: 1:52:02.926119, loss: 5.6573
2023-04-12 11:09:45 - training - INFO - Epoch [1/5][31/438] lr: 9.9e-06, eta: 1:24:26.270538, loss: 5.1761
2023-04-12 11:09:53 - training - INFO - Epoch [1/5][41/438] lr: 9.8e-06, eta: 1:10:04.651738, loss: 4.8274
2023-04-12 11:10:00 - training - INFO - Epoch [1/5][51/438] lr: 9.8e-06, eta: 1:01:19.920627, loss: 4.7181
2023-04-12 11:10:08 - training - INFO - Epoch [1/5][61/438] lr: 9.7e-06, eta: 0:55:26.051540, loss: 4.9679
2023-04-12 11:10:15 - training - INFO - Epoch [1/5][71/438] lr: 9.7e-06, eta: 0:51:12.448288, loss: 4.1282
2023-04-12 11:10:23 - training - INFO - Epoch [1/5][81/438] lr: 9.6e-06, eta: 0:47:59.736159, loss: 4.7165
2023-04-12 11:10:31 - training - INFO - Epoch [1/5][91/438] lr: 9.6e-06, eta: 0:45:32.138162, loss: 3.6279
2023-04-12 11:10:39 - training - INFO - Epoch [1/5][101/438] lr: 9.5e-06, eta: 0:43:30.349641, loss: 3.9393
2023-04-12 11:10:46 - training - INFO - Epoch [1/5][111/438] lr: 9.5e-06, eta: 0:41:46.473585, loss: 3.8003
2023-04-12 11:10:54 - training - INFO - Epoch [1/5][121/438] lr: 9.4e-06, eta: 0:40:21.313458, loss: 3.3834
2023-04-12 11:11:01 - training - INFO - Epoch [1/5][131/438] lr: 9.4e-06, eta: 0:39:03.757641, loss: 4.1499
2023-04-12 11:11:09 - training - INFO - Epoch [1/5][141/438] lr: 9.4e-06, eta: 0:38:00.063681, loss: 3.2761
2023-04-12 11:11:17 - training - INFO - Epoch [1/5][151/438] lr: 9.3e-06, eta: 0:37:00.691212, loss: 3.3922
2023-04-12 11:11:24 - training - INFO - Epoch [1/5][161/438] lr: 9.3e-06, eta: 0:36:07.611135, loss: 3.0411
2023-04-12 11:11:32 - training - INFO - Epoch [1/5][171/438] lr: 9.2e-06, eta: 0:35:20.688954, loss: 2.8687
2023-04-12 11:11:39 - training - INFO - Epoch [1/5][181/438] lr: 9.2e-06, eta: 0:34:36.805759, loss: 2.9122
2023-04-12 11:11:47 - training - INFO - Epoch [1/5][191/438] lr: 9.1e-06, eta: 0:34:00.771104, loss: 2.8089
2023-04-12 11:11:55 - training - INFO - Epoch [1/5][201/438] lr: 9.1e-06, eta: 0:33:25.813017, loss: 3.2880
2023-04-12 11:12:03 - training - INFO - Epoch [1/5][211/438] lr: 9.0e-06, eta: 0:32:51.717280, loss: 3.0987
2023-04-12 11:12:10 - training - INFO - Epoch [1/5][221/438] lr: 9.0e-06, eta: 0:32:21.349333, loss: 3.3505
2023-04-12 11:12:18 - training - INFO - Epoch [1/5][231/438] lr: 8.9e-06, eta: 0:31:53.629560, loss: 2.6698
2023-04-12 11:12:26 - training - INFO - Epoch [1/5][241/438] lr: 8.9e-06, eta: 0:31:29.187139, loss: 2.5435
2023-04-12 11:12:34 - training - INFO - Epoch [1/5][251/438] lr: 8.9e-06, eta: 0:31:03.051309, loss: 3.5878
2023-04-12 11:12:41 - training - INFO - Epoch [1/5][261/438] lr: 8.8e-06, eta: 0:30:38.855901, loss: 2.5508
2023-04-12 11:12:49 - training - INFO - Epoch [1/5][271/438] lr: 8.8e-06, eta: 0:30:15.184019, loss: 2.2113
2023-04-12 11:12:56 - training - INFO - Epoch [1/5][281/438] lr: 8.7e-06, eta: 0:29:52.449823, loss: 2.8033
2023-04-12 11:13:04 - training - INFO - Epoch [1/5][291/438] lr: 8.7e-06, eta: 0:29:31.449867, loss: 3.0251
2023-04-12 11:13:12 - training - INFO - Epoch [1/5][301/438] lr: 8.6e-06, eta: 0:29:12.481970, loss: 2.4765
2023-04-12 11:13:19 - training - INFO - Epoch [1/5][311/438] lr: 8.6e-06, eta: 0:28:52.776220, loss: 2.4691
2023-04-12 11:13:27 - training - INFO - Epoch [1/5][321/438] lr: 8.5e-06, eta: 0:28:33.783288, loss: 2.7457
2023-04-12 11:13:34 - training - INFO - Epoch [1/5][331/438] lr: 8.5e-06, eta: 0:28:15.536271, loss: 2.8102
2023-04-12 11:13:42 - training - INFO - Epoch [1/5][341/438] lr: 8.4e-06, eta: 0:27:58.080289, loss: 2.2623
2023-04-12 11:13:49 - training - INFO - Epoch [1/5][351/438] lr: 8.4e-06, eta: 0:27:41.352600, loss: 2.0512
2023-04-12 11:13:57 - training - INFO - Epoch [1/5][361/438] lr: 8.4e-06, eta: 0:27:24.726421, loss: 2.6013
2023-04-12 11:14:04 - training - INFO - Epoch [1/5][371/438] lr: 8.3e-06, eta: 0:27:08.239651, loss: 2.2567
2023-04-12 11:14:12 - training - INFO - Epoch [1/5][381/438] lr: 8.3e-06, eta: 0:26:52.414161, loss: 2.6873
2023-04-12 11:14:19 - training - INFO - Epoch [1/5][391/438] lr: 8.2e-06, eta: 0:26:37.058652, loss: 2.2762
2023-04-12 11:14:27 - training - INFO - Epoch [1/5][401/438] lr: 8.2e-06, eta: 0:26:21.975131, loss: 2.6342
2023-04-12 11:14:35 - training - INFO - Epoch [1/5][411/438] lr: 8.1e-06, eta: 0:26:07.903860, loss: 1.7950
2023-04-12 11:14:43 - training - INFO - Epoch [1/5][421/438] lr: 8.1e-06, eta: 0:25:56.385659, loss: 2.0641
2023-04-12 11:14:51 - training - INFO - Epoch [1/5][431/438] lr: 8.0e-06, eta: 0:25:43.666738, loss: 1.9455
2023-04-12 11:15:44 - training - INFO - Epoch [1/5][Evaluation] - Train Loss: 3.3700, Validation Metrics: {'exact_match': 33.45388788426763, 'f1': 38.8813975380167}, Test Metrics: {'exact_match': 33.873873873873876, 'f1': 39.41404927734111}
2023-04-12 11:15:44 - training - INFO - Epoch [2/5][1/438] lr: 8.0e-06, eta: 10 days, 22:37:12.171167, loss: 1.6081
2023-04-12 11:15:52 - training - INFO - Epoch [2/5][11/438] lr: 7.9e-06, eta: 1 day, 0:10:48.049517, loss: 2.9089
2023-04-12 11:15:59 - training - INFO - Epoch [2/5][21/438] lr: 7.9e-06, eta: 12:49:27.776658, loss: 2.4271
2023-04-12 11:16:07 - training - INFO - Epoch [2/5][31/438] lr: 7.9e-06, eta: 8:47:33.634432, loss: 2.2910
2023-04-12 11:16:15 - training - INFO - Epoch [2/5][41/438] lr: 7.8e-06, eta: 6:43:45.453504, loss: 1.9785
2023-04-12 11:16:22 - training - INFO - Epoch [2/5][51/438] lr: 7.8e-06, eta: 5:28:24.048756, loss: 1.9738
2023-04-12 11:16:30 - training - INFO - Epoch [2/5][61/438] lr: 7.7e-06, eta: 4:37:48.886276, loss: 1.7726
2023-04-12 11:16:38 - training - INFO - Epoch [2/5][71/438] lr: 7.7e-06, eta: 4:01:21.048933, loss: 1.8409
2023-04-12 11:16:45 - training - INFO - Epoch [2/5][81/438] lr: 7.6e-06, eta: 3:33:51.761283, loss: 1.6377
2023-04-12 11:16:53 - training - INFO - Epoch [2/5][91/438] lr: 7.6e-06, eta: 3:12:31.195810, loss: 1.6009
2023-04-12 11:17:01 - training - INFO - Epoch [2/5][101/438] lr: 7.5e-06, eta: 2:55:15.355431, loss: 1.6230
2023-04-12 11:17:08 - training - INFO - Epoch [2/5][111/438] lr: 7.5e-06, eta: 2:41:03.651459, loss: 2.1430
2023-04-12 11:17:16 - training - INFO - Epoch [2/5][121/438] lr: 7.4e-06, eta: 2:29:13.692674, loss: 1.7810
2023-04-12 11:17:23 - training - INFO - Epoch [2/5][131/438] lr: 7.4e-06, eta: 2:19:07.346602, loss: 2.7929
2023-04-12 11:17:31 - training - INFO - Epoch [2/5][141/438] lr: 7.4e-06, eta: 2:10:26.905434, loss: 2.9704
2023-04-12 11:17:39 - training - INFO - Epoch [2/5][151/438] lr: 7.3e-06, eta: 2:02:56.117163, loss: 2.1217
2023-04-12 11:17:46 - training - INFO - Epoch [2/5][161/438] lr: 7.3e-06, eta: 1:56:19.183764, loss: 2.8286
2023-04-12 11:17:54 - training - INFO - Epoch [2/5][171/438] lr: 7.2e-06, eta: 1:50:30.167853, loss: 1.4314
2023-04-12 11:18:01 - training - INFO - Epoch [2/5][181/438] lr: 7.2e-06, eta: 1:45:15.651111, loss: 1.2252
2023-04-12 11:18:09 - training - INFO - Epoch [2/5][191/438] lr: 7.1e-06, eta: 1:40:37.857561, loss: 1.4098
2023-04-12 11:18:17 - training - INFO - Epoch [2/5][201/438] lr: 7.1e-06, eta: 1:36:27.333630, loss: 1.5636
2023-04-12 11:18:25 - training - INFO - Epoch [2/5][211/438] lr: 7.0e-06, eta: 1:32:38.027437, loss: 2.3173
2023-04-12 11:18:33 - training - INFO - Epoch [2/5][221/438] lr: 7.0e-06, eta: 1:29:11.962528, loss: 2.1227
2023-04-12 11:18:41 - training - INFO - Epoch [2/5][231/438] lr: 6.9e-06, eta: 1:26:02.102130, loss: 2.1246
2023-04-12 11:18:49 - training - INFO - Epoch [2/5][241/438] lr: 6.9e-06, eta: 1:23:04.906626, loss: 1.8987
2023-04-12 11:18:57 - training - INFO - Epoch [2/5][251/438] lr: 6.9e-06, eta: 1:20:23.119014, loss: 1.6084
2023-04-12 11:19:04 - training - INFO - Epoch [2/5][261/438] lr: 6.8e-06, eta: 1:17:50.865168, loss: 1.9492
2023-04-12 11:19:12 - training - INFO - Epoch [2/5][271/438] lr: 6.8e-06, eta: 1:15:31.918076, loss: 1.7670
2023-04-12 11:19:20 - training - INFO - Epoch [2/5][281/438] lr: 6.7e-06, eta: 1:13:20.928422, loss: 0.9198
2023-04-12 11:19:28 - training - INFO - Epoch [2/5][291/438] lr: 6.7e-06, eta: 1:11:19.109751, loss: 1.1781
2023-04-12 11:19:36 - training - INFO - Epoch [2/5][301/438] lr: 6.6e-06, eta: 1:09:23.365445, loss: 1.7002
2023-04-12 11:19:44 - training - INFO - Epoch [2/5][311/438] lr: 6.6e-06, eta: 1:07:37.685468, loss: 1.4837
2023-04-12 11:19:52 - training - INFO - Epoch [2/5][321/438] lr: 6.5e-06, eta: 1:05:54.854463, loss: 2.8741
2023-04-12 11:19:59 - training - INFO - Epoch [2/5][331/438] lr: 6.5e-06, eta: 1:04:17.798659, loss: 2.2954
2023-04-12 11:20:07 - training - INFO - Epoch [2/5][341/438] lr: 6.4e-06, eta: 1:02:45.771397, loss: 2.4198
2023-04-12 11:20:15 - training - INFO - Epoch [2/5][351/438] lr: 6.4e-06, eta: 1:01:19.629354, loss: 1.9328
2023-04-12 11:20:22 - training - INFO - Epoch [2/5][361/438] lr: 6.4e-06, eta: 0:59:56.540113, loss: 1.3668
2023-04-12 11:20:30 - training - INFO - Epoch [2/5][371/438] lr: 6.3e-06, eta: 0:58:38.393474, loss: 1.9792
2023-04-12 11:20:38 - training - INFO - Epoch [2/5][381/438] lr: 6.3e-06, eta: 0:57:23.274117, loss: 1.4085
2023-04-12 11:20:45 - training - INFO - Epoch [2/5][391/438] lr: 6.2e-06, eta: 0:56:13.035050, loss: 1.3737
2023-04-12 11:20:53 - training - INFO - Epoch [2/5][401/438] lr: 6.2e-06, eta: 0:55:05.161399, loss: 2.0043
2023-04-12 11:21:01 - training - INFO - Epoch [2/5][411/438] lr: 6.1e-06, eta: 0:53:59.694204, loss: 1.6906
2023-04-12 11:21:08 - training - INFO - Epoch [2/5][421/438] lr: 6.1e-06, eta: 0:52:56.503081, loss: 1.4718
2023-04-12 11:21:16 - training - INFO - Epoch [2/5][431/438] lr: 6.0e-06, eta: 0:51:56.872363, loss: 1.8968
2023-04-12 11:22:10 - training - INFO - Epoch [2/5][Evaluation] - Train Loss: 1.8210, Validation Metrics: {'exact_match': 41.22965641952984, 'f1': 46.60881554203221}, Test Metrics: {'exact_match': 44.5045045045045, 'f1': 48.803008164198474}
2023-04-12 11:22:11 - training - INFO - Epoch [3/5][1/438] lr: 6.0e-06, eta: 20 days, 17:38:12.734746, loss: 2.0843
2023-04-12 11:22:18 - training - INFO - Epoch [3/5][11/438] lr: 5.9e-06, eta: 1 day, 21:26:49.288925, loss: 1.0582
2023-04-12 11:22:26 - training - INFO - Epoch [3/5][21/438] lr: 5.9e-06, eta: 23:54:58.131819, loss: 1.3603
2023-04-12 11:22:34 - training - INFO - Epoch [3/5][31/438] lr: 5.9e-06, eta: 16:16:33.006004, loss: 2.0293
2023-04-12 11:22:41 - training - INFO - Epoch [3/5][41/438] lr: 5.8e-06, eta: 12:21:42.925383, loss: 1.1351
2023-04-12 11:22:49 - training - INFO - Epoch [3/5][51/438] lr: 5.8e-06, eta: 9:58:46.986240, loss: 1.7294
2023-04-12 11:22:57 - training - INFO - Epoch [3/5][61/438] lr: 5.7e-06, eta: 8:22:43.565550, loss: 0.8060
2023-04-12 11:23:04 - training - INFO - Epoch [3/5][71/438] lr: 5.7e-06, eta: 7:13:39.629038, loss: 1.6380
2023-04-12 11:23:12 - training - INFO - Epoch [3/5][81/438] lr: 5.6e-06, eta: 6:21:38.111079, loss: 1.3071
2023-04-12 11:23:19 - training - INFO - Epoch [3/5][91/438] lr: 5.6e-06, eta: 5:40:59.393790, loss: 1.5553
2023-04-12 11:23:27 - training - INFO - Epoch [3/5][101/438] lr: 5.5e-06, eta: 5:08:22.049477, loss: 1.5647
2023-04-12 11:23:34 - training - INFO - Epoch [3/5][111/438] lr: 5.5e-06, eta: 4:41:35.916576, loss: 2.2888
2023-04-12 11:23:42 - training - INFO - Epoch [3/5][121/438] lr: 5.4e-06, eta: 4:19:15.118558, loss: 1.6181
2023-04-12 11:23:50 - training - INFO - Epoch [3/5][131/438] lr: 5.4e-06, eta: 4:00:16.712377, loss: 1.4544
2023-04-12 11:23:57 - training - INFO - Epoch [3/5][141/438] lr: 5.4e-06, eta: 3:44:03.370158, loss: 1.1549
2023-04-12 11:24:05 - training - INFO - Epoch [3/5][151/438] lr: 5.3e-06, eta: 3:29:52.978184, loss: 1.4352
2023-04-12 11:24:13 - training - INFO - Epoch [3/5][161/438] lr: 5.3e-06, eta: 3:17:29.546668, loss: 1.6355
2023-04-12 11:24:20 - training - INFO - Epoch [3/5][171/438] lr: 5.2e-06, eta: 3:06:29.814864, loss: 0.9370
2023-04-12 11:24:28 - training - INFO - Epoch [3/5][181/438] lr: 5.2e-06, eta: 2:56:42.678310, loss: 1.7262
2023-04-12 11:24:35 - training - INFO - Epoch [3/5][191/438] lr: 5.1e-06, eta: 2:47:56.529215, loss: 0.9999
2023-04-12 11:24:43 - training - INFO - Epoch [3/5][201/438] lr: 5.1e-06, eta: 2:40:07.442832, loss: 1.1494
2023-04-12 11:24:51 - training - INFO - Epoch [3/5][211/438] lr: 5.0e-06, eta: 2:32:57.309713, loss: 1.8277
2023-04-12 11:24:58 - training - INFO - Epoch [3/5][221/438] lr: 5.0e-06, eta: 2:26:25.427937, loss: 2.2291
2023-04-12 11:25:06 - training - INFO - Epoch [3/5][231/438] lr: 4.9e-06, eta: 2:20:28.822785, loss: 1.2135
2023-04-12 11:25:14 - training - INFO - Epoch [3/5][241/438] lr: 4.9e-06, eta: 2:15:01.714293, loss: 2.3835
2023-04-12 11:25:22 - training - INFO - Epoch [3/5][251/438] lr: 4.9e-06, eta: 2:09:58.308980, loss: 1.6919
2023-04-12 11:25:29 - training - INFO - Epoch [3/5][261/438] lr: 4.8e-06, eta: 2:05:17.150964, loss: 2.0676
2023-04-12 11:25:37 - training - INFO - Epoch [3/5][271/438] lr: 4.8e-06, eta: 2:00:57.134113, loss: 1.4408
2023-04-12 11:25:45 - training - INFO - Epoch [3/5][281/438] lr: 4.7e-06, eta: 1:56:53.935169, loss: 0.9356
2023-04-12 11:25:53 - training - INFO - Epoch [3/5][291/438] lr: 4.7e-06, eta: 1:53:08.093238, loss: 1.3036
2023-04-12 11:26:00 - training - INFO - Epoch [3/5][301/438] lr: 4.6e-06, eta: 1:49:36.086917, loss: 1.4388
2023-04-12 11:26:08 - training - INFO - Epoch [3/5][311/438] lr: 4.6e-06, eta: 1:46:18.658211, loss: 0.9805
2023-04-12 11:26:16 - training - INFO - Epoch [3/5][321/438] lr: 4.5e-06, eta: 1:43:11.890467, loss: 2.0411
2023-04-12 11:26:23 - training - INFO - Epoch [3/5][331/438] lr: 4.5e-06, eta: 1:40:15.573421, loss: 1.7130
2023-04-12 11:26:31 - training - INFO - Epoch [3/5][341/438] lr: 4.4e-06, eta: 1:37:30.300715, loss: 1.0743
2023-04-12 11:26:39 - training - INFO - Epoch [3/5][351/438] lr: 4.4e-06, eta: 1:34:54.086505, loss: 1.6395
2023-04-12 11:26:47 - training - INFO - Epoch [3/5][361/438] lr: 4.4e-06, eta: 1:32:24.721411, loss: 1.5830
2023-04-12 11:26:54 - training - INFO - Epoch [3/5][371/438] lr: 4.3e-06, eta: 1:30:02.511855, loss: 1.0364
2023-04-12 11:27:02 - training - INFO - Epoch [3/5][381/438] lr: 4.3e-06, eta: 1:27:48.898827, loss: 1.7708
2023-04-12 11:27:10 - training - INFO - Epoch [3/5][391/438] lr: 4.2e-06, eta: 1:25:40.491384, loss: 1.2169
2023-04-12 11:27:17 - training - INFO - Epoch [3/5][401/438] lr: 4.2e-06, eta: 1:23:38.311377, loss: 1.6339
2023-04-12 11:27:25 - training - INFO - Epoch [3/5][411/438] lr: 4.1e-06, eta: 1:21:41.685816, loss: 0.8725
2023-04-12 11:27:32 - training - INFO - Epoch [3/5][421/438] lr: 4.1e-06, eta: 1:19:50.154808, loss: 0.8761
2023-04-12 11:27:40 - training - INFO - Epoch [3/5][431/438] lr: 4.0e-06, eta: 1:18:03.775491, loss: 0.8327
2023-04-12 11:28:33 - training - INFO - Epoch [3/5][Evaluation] - Train Loss: 1.4409, Validation Metrics: {'exact_match': 49.54792043399638, 'f1': 54.79891340221201}, Test Metrics: {'exact_match': 54.95495495495496, 'f1': 59.20396798871576}
2023-04-12 11:28:34 - training - INFO - Epoch [4/5][1/438] lr: 4.0e-06, eta: 30 days, 10:23:54.229466, loss: 1.4991
2023-04-12 11:28:41 - training - INFO - Epoch [4/5][11/438] lr: 3.9e-06, eta: 2 days, 18:30:55.862092, loss: 1.6400
2023-04-12 11:28:49 - training - INFO - Epoch [4/5][21/438] lr: 3.9e-06, eta: 1 day, 10:53:56.301414, loss: 0.9322
2023-04-12 11:28:56 - training - INFO - Epoch [4/5][31/438] lr: 3.9e-06, eta: 23:40:50.736026, loss: 1.1098
2023-04-12 11:29:04 - training - INFO - Epoch [4/5][41/438] lr: 3.8e-06, eta: 17:55:59.168457, loss: 0.7443
2023-04-12 11:29:12 - training - INFO - Epoch [4/5][51/438] lr: 3.8e-06, eta: 14:26:17.565243, loss: 0.5251
2023-04-12 11:29:19 - training - INFO - Epoch [4/5][61/438] lr: 3.7e-06, eta: 12:05:17.109156, loss: 1.7726
2023-04-12 11:29:27 - training - INFO - Epoch [4/5][71/438] lr: 3.7e-06, eta: 10:23:57.786373, loss: 1.4988
2023-04-12 11:29:34 - training - INFO - Epoch [4/5][81/438] lr: 3.6e-06, eta: 9:07:41.522694, loss: 1.7022
2023-04-12 11:29:42 - training - INFO - Epoch [4/5][91/438] lr: 3.6e-06, eta: 8:08:06.786567, loss: 1.2816
2023-04-12 11:29:50 - training - INFO - Epoch [4/5][101/438] lr: 3.5e-06, eta: 7:20:17.159760, loss: 0.9905
2023-04-12 11:29:57 - training - INFO - Epoch [4/5][111/438] lr: 3.5e-06, eta: 6:41:03.555978, loss: 1.4603
2023-04-12 11:30:05 - training - INFO - Epoch [4/5][121/438] lr: 3.4e-06, eta: 6:08:21.602147, loss: 1.6002
2023-04-12 11:30:12 - training - INFO - Epoch [4/5][131/438] lr: 3.4e-06, eta: 5:40:34.990244, loss: 1.3558
2023-04-12 11:30:20 - training - INFO - Epoch [4/5][141/438] lr: 3.4e-06, eta: 5:16:41.833839, loss: 1.8640
2023-04-12 11:30:28 - training - INFO - Epoch [4/5][151/438] lr: 3.3e-06, eta: 4:56:02.248945, loss: 1.1593
2023-04-12 11:30:35 - training - INFO - Epoch [4/5][161/438] lr: 3.3e-06, eta: 4:37:53.128948, loss: 1.5315
2023-04-12 11:30:43 - training - INFO - Epoch [4/5][171/438] lr: 3.2e-06, eta: 4:21:54.278781, loss: 0.8530
2023-04-12 11:30:51 - training - INFO - Epoch [4/5][181/438] lr: 3.2e-06, eta: 4:07:38.377163, loss: 0.5934
2023-04-12 11:30:59 - training - INFO - Epoch [4/5][191/438] lr: 3.1e-06, eta: 3:54:50.355298, loss: 1.2812
2023-04-12 11:31:06 - training - INFO - Epoch [4/5][201/438] lr: 3.1e-06, eta: 3:43:16.497777, loss: 1.3260
2023-04-12 11:31:14 - training - INFO - Epoch [4/5][211/438] lr: 3.0e-06, eta: 3:32:50.200045, loss: 1.6599
2023-04-12 11:31:22 - training - INFO - Epoch [4/5][221/438] lr: 3.0e-06, eta: 3:23:19.870837, loss: 1.2879
2023-04-12 11:31:29 - training - INFO - Epoch [4/5][231/438] lr: 2.9e-06, eta: 3:14:37.291437, loss: 1.8448
2023-04-12 11:31:37 - training - INFO - Epoch [4/5][241/438] lr: 2.9e-06, eta: 3:06:37.437678, loss: 1.1438
2023-04-12 11:31:45 - training - INFO - Epoch [4/5][251/438] lr: 2.9e-06, eta: 2:59:15.654329, loss: 2.0482
2023-04-12 11:31:52 - training - INFO - Epoch [4/5][261/438] lr: 2.8e-06, eta: 2:52:26.340033, loss: 1.1832
2023-04-12 11:32:00 - training - INFO - Epoch [4/5][271/438] lr: 2.8e-06, eta: 2:46:07.038449, loss: 1.3957
2023-04-12 11:32:08 - training - INFO - Epoch [4/5][281/438] lr: 2.7e-06, eta: 2:40:14.336789, loss: 1.0252
2023-04-12 11:32:15 - training - INFO - Epoch [4/5][291/438] lr: 2.7e-06, eta: 2:34:45.525108, loss: 1.4831
2023-04-12 11:32:23 - training - INFO - Epoch [4/5][301/438] lr: 2.6e-06, eta: 2:29:37.984419, loss: 0.9032
2023-04-12 11:32:31 - training - INFO - Epoch [4/5][311/438] lr: 2.6e-06, eta: 2:24:49.591457, loss: 1.2686
2023-04-12 11:32:38 - training - INFO - Epoch [4/5][321/438] lr: 2.5e-06, eta: 2:20:18.611460, loss: 1.9066
2023-04-12 11:32:46 - training - INFO - Epoch [4/5][331/438] lr: 2.5e-06, eta: 2:16:03.426700, loss: 1.9514
2023-04-12 11:32:53 - training - INFO - Epoch [4/5][341/438] lr: 2.4e-06, eta: 2:12:02.454676, loss: 0.9750
2023-04-12 11:33:01 - training - INFO - Epoch [4/5][351/438] lr: 2.4e-06, eta: 2:08:14.951607, loss: 0.9301
2023-04-12 11:33:09 - training - INFO - Epoch [4/5][361/438] lr: 2.4e-06, eta: 2:04:40.705108, loss: 1.2818
2023-04-12 11:33:16 - training - INFO - Epoch [4/5][371/438] lr: 2.3e-06, eta: 2:01:15.945430, loss: 1.1653
2023-04-12 11:33:24 - training - INFO - Epoch [4/5][381/438] lr: 2.3e-06, eta: 1:58:02.401428, loss: 1.3101
2023-04-12 11:33:32 - training - INFO - Epoch [4/5][391/438] lr: 2.2e-06, eta: 1:54:58.085600, loss: 0.9848
2023-04-12 11:33:39 - training - INFO - Epoch [4/5][401/438] lr: 2.2e-06, eta: 1:52:03.716774, loss: 1.5828
2023-04-12 11:33:47 - training - INFO - Epoch [4/5][411/438] lr: 2.1e-06, eta: 1:49:15.769773, loss: 0.9504
2023-04-12 11:33:55 - training - INFO - Epoch [4/5][421/438] lr: 2.1e-06, eta: 1:46:36.067160, loss: 1.5089
2023-04-12 11:34:02 - training - INFO - Epoch [4/5][431/438] lr: 2.0e-06, eta: 1:44:03.391082, loss: 1.7731
2023-04-12 11:34:55 - training - INFO - Epoch [4/5][Evaluation] - Train Loss: 1.2603, Validation Metrics: {'exact_match': 48.64376130198915, 'f1': 53.81137736728298}, Test Metrics: {'exact_match': 54.95495495495496, 'f1': 58.45943739891632}
2023-04-12 11:34:56 - training - INFO - Epoch [5/5][1/438] lr: 2.0e-06, eta: 40 days, 2:58:23.734021, loss: 1.6210
2023-04-12 11:35:04 - training - INFO - Epoch [5/5][11/438] lr: 1.9e-06, eta: 3 days, 15:34:02.693616, loss: 1.0706
2023-04-12 11:35:11 - training - INFO - Epoch [5/5][21/438] lr: 1.9e-06, eta: 1 day, 21:52:33.193119, loss: 1.4284
2023-04-12 11:35:19 - training - INFO - Epoch [5/5][31/438] lr: 1.9e-06, eta: 1 day, 7:04:57.420604, loss: 0.6103
2023-04-12 11:35:27 - training - INFO - Epoch [5/5][41/438] lr: 1.8e-06, eta: 23:30:08.627138, loss: 1.2956
2023-04-12 11:35:34 - training - INFO - Epoch [5/5][51/438] lr: 1.8e-06, eta: 18:53:35.759436, loss: 1.4853
2023-04-12 11:35:42 - training - INFO - Epoch [5/5][61/438] lr: 1.7e-06, eta: 15:47:46.375601, loss: 1.0273
2023-04-12 11:35:49 - training - INFO - Epoch [5/5][71/438] lr: 1.7e-06, eta: 13:34:12.824540, loss: 1.0779
2023-04-12 11:35:57 - training - INFO - Epoch [5/5][81/438] lr: 1.6e-06, eta: 11:53:43.059408, loss: 1.2369
2023-04-12 11:36:05 - training - INFO - Epoch [5/5][91/438] lr: 1.6e-06, eta: 10:35:12.036265, loss: 0.7355
2023-04-12 11:36:12 - training - INFO - Epoch [5/5][101/438] lr: 1.5e-06, eta: 9:32:11.634987, loss: 1.0467
2023-04-12 11:36:20 - training - INFO - Epoch [5/5][111/438] lr: 1.5e-06, eta: 8:40:30.209934, loss: 1.5845
2023-04-12 11:36:27 - training - INFO - Epoch [5/5][121/438] lr: 1.4e-06, eta: 7:57:20.587680, loss: 1.3988
2023-04-12 11:36:35 - training - INFO - Epoch [5/5][131/438] lr: 1.4e-06, eta: 7:20:43.792593, loss: 1.8450
2023-04-12 11:36:42 - training - INFO - Epoch [5/5][141/438] lr: 1.4e-06, eta: 6:49:18.789456, loss: 0.8981
2023-04-12 11:36:50 - training - INFO - Epoch [5/5][151/438] lr: 1.3e-06, eta: 6:22:02.464507, loss: 0.8472
2023-04-12 11:36:58 - training - INFO - Epoch [5/5][161/438] lr: 1.3e-06, eta: 5:58:09.027405, loss: 0.8983
2023-04-12 11:37:05 - training - INFO - Epoch [5/5][171/438] lr: 1.2e-06, eta: 5:37:03.101505, loss: 0.7869
2023-04-12 11:37:13 - training - INFO - Epoch [5/5][181/438] lr: 1.2e-06, eta: 5:18:15.512856, loss: 1.0406
2023-04-12 11:37:20 - training - INFO - Epoch [5/5][191/438] lr: 1.1e-06, eta: 5:01:25.442755, loss: 1.4314
2023-04-12 11:37:28 - training - INFO - Epoch [5/5][201/438] lr: 1.1e-06, eta: 4:46:15.096549, loss: 0.8401
2023-04-12 11:37:36 - training - INFO - Epoch [5/5][211/438] lr: 1.0e-06, eta: 4:32:32.087137, loss: 1.2213
2023-04-12 11:37:44 - training - INFO - Epoch [5/5][221/438] lr: 9.9e-07, eta: 4:20:02.216201, loss: 1.0356
2023-04-12 11:37:51 - training - INFO - Epoch [5/5][231/438] lr: 9.5e-07, eta: 4:08:37.232562, loss: 0.8936
2023-04-12 11:37:59 - training - INFO - Epoch [5/5][241/438] lr: 9.0e-07, eta: 3:58:06.797578, loss: 1.0287
2023-04-12 11:38:07 - training - INFO - Epoch [5/5][251/438] lr: 8.5e-07, eta: 3:48:27.490979, loss: 1.2876
2023-04-12 11:38:15 - training - INFO - Epoch [5/5][261/438] lr: 8.1e-07, eta: 3:39:32.529507, loss: 1.2494
2023-04-12 11:38:22 - training - INFO - Epoch [5/5][271/438] lr: 7.6e-07, eta: 3:31:14.605443, loss: 1.0444
2023-04-12 11:38:30 - training - INFO - Epoch [5/5][281/438] lr: 7.2e-07, eta: 3:23:30.700874, loss: 1.0460
2023-04-12 11:38:37 - training - INFO - Epoch [5/5][291/438] lr: 6.7e-07, eta: 3:16:17.983497, loss: 1.0286
2023-04-12 11:38:45 - training - INFO - Epoch [5/5][301/438] lr: 6.3e-07, eta: 3:09:33.825787, loss: 0.6855
2023-04-12 11:38:52 - training - INFO - Epoch [5/5][311/438] lr: 5.8e-07, eta: 3:03:16.105295, loss: 1.2985
2023-04-12 11:39:00 - training - INFO - Epoch [5/5][321/438] lr: 5.3e-07, eta: 2:57:21.699117, loss: 0.6025
2023-04-12 11:39:08 - training - INFO - Epoch [5/5][331/438] lr: 4.9e-07, eta: 2:51:48.162436, loss: 1.0653
2023-04-12 11:39:15 - training - INFO - Epoch [5/5][341/438] lr: 4.4e-07, eta: 2:46:33.434522, loss: 1.2154
2023-04-12 11:39:23 - training - INFO - Epoch [5/5][351/438] lr: 4.0e-07, eta: 2:41:35.813031, loss: 1.2686
2023-04-12 11:39:31 - training - INFO - Epoch [5/5][361/438] lr: 3.5e-07, eta: 2:36:55.777963, loss: 0.8704
2023-04-12 11:39:38 - training - INFO - Epoch [5/5][371/438] lr: 3.1e-07, eta: 2:32:29.140716, loss: 1.2995
2023-04-12 11:39:46 - training - INFO - Epoch [5/5][381/438] lr: 2.6e-07, eta: 2:28:16.135581, loss: 0.8777
2023-04-12 11:39:53 - training - INFO - Epoch [5/5][391/438] lr: 2.1e-07, eta: 2:24:15.129322, loss: 1.1511
2023-04-12 11:40:01 - training - INFO - Epoch [5/5][401/438] lr: 1.7e-07, eta: 2:20:26.789315, loss: 1.1528
2023-04-12 11:40:09 - training - INFO - Epoch [5/5][411/438] lr: 1.2e-07, eta: 2:16:50.266458, loss: 1.3961
2023-04-12 11:40:17 - training - INFO - Epoch [5/5][421/438] lr: 7.8e-08, eta: 2:13:23.907722, loss: 0.7454
2023-04-12 11:40:26 - training - INFO - Epoch [5/5][431/438] lr: 3.2e-08, eta: 2:10:11.226480, loss: 1.3280
2023-04-12 11:41:30 - training - INFO - Epoch [5/5][Evaluation] - Train Loss: 1.1516, Validation Metrics: {'exact_match': 48.824593128390596, 'f1': 53.45961153757238}, Test Metrics: {'exact_match': 55.4954954954955, 'f1': 59.053985698018444}
2023-04-12 11:42:00 - training - INFO - Final Test - Train Loss: 1.1516, Test Metrics: {'exact_match': 55.4954954954955, 'f1': 59.053985698018444}
